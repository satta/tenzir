"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[11477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/tenzir-v4.18","metadata":{"permalink":"/blog/tenzir-v4.18","source":"@site/blog/tenzir-v4.18/index.md","title":"Tenzir v4.18","description":"Monitoring Tenzir nodes is easier than before with [Tenzir","date":"2024-07-11T00:00:00.000Z","formattedDate":"July 11, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"health-metrics","permalink":"/blog/tags/health-metrics"},{"label":"tql2","permalink":"/blog/tags/tql-2"}],"readingTime":2.7,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.18","authors":["dominiklohmann"],"date":"2024-07-11T00:00:00.000Z","tags":["release","health-metrics","tql2"],"comments":true},"nextItem":{"title":"Tenzir v4.17","permalink":"/blog/tenzir-v4.17"}},"content":"Monitoring Tenzir nodes is easier than before with [Tenzir\\nv4.18][github-release] and its new health metrics.\\n\\n![Tenzir v4.18](tenzir-v4.18.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.18.0\\n\\n\x3c!-- truncate --\x3e\\n\\n## Monitor Node Health With Metrics\\n\\nThe `metrics` operator now additionally takes a positional argument for the\\nmetrics name. Now, `metrics cpu` is equivalent to `metrics | where #schema ==\\n\\"tenzir.metrics.cpu\\"`. That\'s a lot easier to write!\\n\\nTenzir nodes now collect more metrics than before. In particular, the `import`,\\n`export`, `publish`, `subscribe`, `enrich`, and `lookup` operators now emit\\nmetrics, and nodes additionally collect `api` metrics for every API call and\\n`platform` metrics that record the connection status to the Tenzir Platform from\\nthe node\'s perspective. These are best explained on examples:\\n\\n```text {0} title=\\"Show imported events per schema and day for the last month\\"\\nmetrics import\\n| where timestamp > 30d ago\\n| summarize events=sum(events) by timestamp, schema resolution 1d\\n| sort timestamp, schema\\n```\\n\\n```text {0} title=\\"Calculate the rate of context hits for the context \'iocs\'\\"\\nmetrics enrich\\n| where context == \\"iocs\\"\\n| summarize events=sum(events), hits=sum(hits)\\n| python \'self.rate = self.hits / self.events\'\\n```\\n\\n```text {0} title=\\"Show the most commonly used APIs in the last hour\\"\\nmetrics api\\n| where timestamp > 1 day ago\\n| top path\\n```\\n\\nThese are just three examples to get you started with monitoring your node.\\nWe\'re planning to make more data available from more operators in the future,\\nand have built a new framework that makes emitting custom metrics from operators\\na breeze.\\n\\n:::tip Want to learn more?\\nTake a look at the [`metrics` operator\'s documentation](/operators/metrics),\\nwhich details all the available metrics and their schema.\\n:::\\n\\n## Play With TQL2\\n\\nAt this point it\'s an open secret that we\'re working working on a major revamp\\nto the Tenzir Query Language. We still have quite a way to go before\\nmaking the next version the new default, but we\'re excited to announce that as\\nof Tenzir v4.18, it is now possible to try it out without being a Tenzir developer.\\n\\nTo use TQL2 on [app.tenzir.com](https://app.tenzir.com), for pipelines\\nconfigured in the `tenzir.yaml` configuration file, or through the API, start\\nthe pipeline with a `// experimental-tql2` comment. For example:\\n\\n```\\n// experimental-tql2\\nexport live=true\\nwhere @name == \\"zeek.http\\"\\nif method == \\"GET\\" {\\n  where request_body.len > 0\\n  publish \\"weird\\"\\n} else {\\n  where uri.ends_with(\\".exe\\") and status_code < 400\\n  publish \\"suspicious\\"\\n}\\n```\\n\\nUse `tenzir --tql2 <pipeline>` to use TQL2 with the `tenzir` binary on the\\ncommand-line.\\n\\n:::warning Experimental\\nMany things in TQL2 are not implemented, not documented, or not yet working\\ncorrectly. We are still making breaking changes to it, so we would like to ask\\nyou not to use it in production.\\n\\nGot feedback? Head over to the `#developers` channel in our [community\\nDiscord](/discord).\\n:::\\n\\n## Other Changes\\n\\nAs usual, the [changelog][changelog] contains a full list of features, changes,\\nand bug fixes in this release.\\n\\nEvery second Tuesday at 8 AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST, we hold\\noffice hours in [our Discord server][discord]. Whether you want to participate\\nin the TQL2 discussion, have ideas for further metrics, feedback of any kind, a\\nwild idea that you\'d like to bring up, or just want to hang out\u2014come join us!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4180"},{"id":"/tenzir-v4.17","metadata":{"permalink":"/blog/tenzir-v4.17","source":"@site/blog/tenzir-v4.17/index.md","title":"Tenzir v4.17","description":"The new Tenzir v4.17 brings an integration with Azure Log","date":"2024-06-21T00:00:00.000Z","formattedDate":"June 21, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"azure-log-analytics","permalink":"/blog/tags/azure-log-analytics"},{"label":"lookup-table","permalink":"/blog/tags/lookup-table"}],"readingTime":3.13,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.17","authors":["dominiklohmann"],"date":"2024-06-21T00:00:00.000Z","tags":["release","azure-log-analytics","lookup-table"],"comments":true},"prevItem":{"title":"Tenzir v4.18","permalink":"/blog/tenzir-v4.18"},"nextItem":{"title":"An Intern\'s Reflection","permalink":"/blog/an-interns-reflection"}},"content":"The new [Tenzir v4.17][github-release] brings an integration with Azure Log\\nAnalytics and adds support for expiring entries in lookup tables.\\n\\n![Tenzir v4.17](tenzir-v4.17.excalidraw.svg)\\n\\n[github-release]: https://github.com/tenzir/tenzir/releases/tag/v4.17.1\\n\\n\x3c!-- truncate --\x3e\\n\\n## Send Events to Azure Log Analytics\\n\\nThe shining star of Tenzir v4.17 is the new [`azure-log-analytics` sink\\noperator][azure-log-analytics-operator], which sends events to [Log Analytics in\\nAzure Monitor][log-analytics-overview].\\n\\n:::tip Want to Learn More?\\nWe wrote an [integration guide][azure-log-analytics-integration] showing how to\\nsend your events to Azure Log Analytics using Tenzir. Come check it out!\\n:::\\n\\n[azure-log-analytics-operator]: /next/operators/azure-log-analytics\\n[log-analytics-overview]: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-overview\\n[azure-log-analytics-integration]: /next/integrations/azure-log-analytics\\n\\n## Lookup Table Timeouts\\n\\nThe `context update` operator gained two new options when used together with\\n[lookup table contexts][lookup-table-docs]: `--create-timeout <duration>` and\\n`--update-timeout <duration>`.\\n\\nBoth new options cause individual events to expire in the lookup table. Create\\ntimeouts specify the time after which entries in the lookup table expire, and\\nupdate timeouts specify the time after which entries in the lookup table expire\\nwhen they\'re not accessed.\\n\\nThe following example adds lookup table entries that expire after a week at the\\nlatest, or when they were not accessed for a day, whichever comes first:\\n\\n```\\n\u2026\\n| context update my-lookup-table --create-timeout 1w --update-timeout 1d\\n```\\n\\n[lookup-table-docs]: /next/contexts/lookup-table\\n\\n## Print Individual Fields in Events\\n\\nThe [`print <field> <format>` operator][print-operator-docs] is the counterpart\\nto the [`parse` operator][parse-operator-docs]. Given a field of type record\\nwithin an event, it replaces it with a string containing the formatted\\nrepresentation. This is best explained on an example:\\n\\n```json {0} title=\\"Input\\"\\n{\\n  \\"flow_id\\": 852833247340038,\\n  \\"flow\\": {\\n    \\"pkts_toserver\\": 1,\\n    \\"pkts_toclient\\": 0,\\n    \\"bytes_toserver\\": 54,\\n    \\"bytes_toclient\\": 0\\n  }\\n}\\n```\\n\\n```text {0} title=\\"Render the field flow as CSV\\"\\nfrom input.json\\n| print flow csv --no-header\\n```\\n\\n```json {0} title=\\"Output\\"\\n{\\n  \\"flow_id\\": 852833247340038,\\n  \\"flow\\": \\"1,0,54,0\\"\\n}\\n```\\n\\nThe `print` operator is especially useful when working with third-party APIs\\nthat often do not support deeply nested data structures in their data model.\\n\\n[print-operator-docs]: /next/operators/print\\n[parse-operator-docs]: /next/operators/parse\\n\\n## Changes to Built-in Type Aliases\\n\\nWe removed the built-in `timestamp` and `port` type aliases for `time` and\\n`uint64`, respectively.\\n\\nThese types were relics of Tenzir\'s past, when onboarding data required\\nspecifying a schema explicitly. Back then, we started using type aliases to\\nfurther categorize parts of the onboarded data. With Tenzir today, automatic\\nschema inference is the modus operandi. This caused data that was imported with\\na schema to sometimes use a `timestamp` type, but all automatically inferred\\ndata used the underlying `time` type. This caused issues down the line, because\\noperators like `summarize` by design do not group fields together with distinct\\ntypes. To users, this showed as duplicate values that were supposed to be\\ngrouped by in summarized results.\\n\\n:::warning Required Configuration Changes\\nIf you have custom schemas installed in `/opt/tenzir/etc/tenzir/schemas` or\\n`~/.config/tenzir/schemas`, you will need to adapt them in one of two ways:\\n1. Replace all `timestamp` types with `time` and all `port` types with `uint64`\\n   (recommended).\\n2. Add the aliases back to your own schemas by defining `type timestamp = time`\\n   and `type port = uint64`, respectively.\\n:::\\n\\n## Edit Pipelines in the Tenzir Platform\\n\\nYou can now change pipelines on [app.tenzir.com][tenzir-app] more quickly.\\nSimply click on any pipeline on the overview page to open a detailed view. In\\nthis view, you can directly edit the definition or options. The new action menu\\nallows you to quickly start, pause, stop, duplicate, or delete a pipeline.\\n\\n## Other Changes\\n\\nFor a full list of changes in this release, please check our\\n[changelog][changelog], and play with the new changes at\\n[app.tenzir.com][tenzir-app].\\n\\nEvery second Tuesday at 8 AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST, we hold\\noffice hours in [our Discord server][discord]. Join us next week for an\\nexclusive sneak peek with our designer into upcoming changes to\\n[app.tenzir.com][tenzir-app]!\\n\\n[discord]: /discord\\n[changelog]: /changelog#v4170\\n[tenzir-app]: https://app.tenzir.com"},{"id":"/an-interns-reflection","metadata":{"permalink":"/blog/an-interns-reflection","source":"@site/blog/an-interns-reflection/index.md","title":"An Intern\'s Reflection","description":"I spent the past twelve weeks interning at Tenzir and am excited to share my","date":"2024-06-14T00:00:00.000Z","formattedDate":"June 14, 2024","tags":[{"label":"internship","permalink":"/blog/tags/internship"}],"readingTime":6.075,"hasTruncateMarker":true,"authors":[{"name":"Bala Vinaithirthan","title":"Software Engineering Intern","url":"https://github.com/balavinaithirthan","email":"balabv1@gmail.com","imageURL":"https://github.com/balavinaithirthan.png","key":"balavinaithirthan"}],"frontMatter":{"title":"An Intern\'s Reflection","authors":["balavinaithirthan"],"date":"2024-06-14T00:00:00.000Z","tags":["internship"],"comments":true},"prevItem":{"title":"Tenzir v4.17","permalink":"/blog/tenzir-v4.17"},"nextItem":{"title":"Tenzir v4.16","permalink":"/blog/tenzir-v4.16"}},"content":"I spent the past twelve weeks interning at Tenzir and am excited to share my\\nexperiences.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Journey\\n\\nMy first days at Tenzir felt like the first time I surfed. The summer after my\\nfirst year, I spent the mornings at Half Moon Bay, trying desperately to stay on\\nmy surfboard. Just as I would climb a giant wave, another would come behind and\\ncrash against my chest. A never-ending cycle for the weeks of summer.\\n\\nNow, a few years later, I sat in front of a monitor at Tenzir\u2019s headquarters,\\nleft-clicking on my mouse for what felt like hours. Each left click revealed yet\\nanother function, namespace, or variable; an endless cycle of waves. As a\\nstudent who learned from the ground up and got a broad understanding, I found\\nmyself trying to learn the meaning of every function and the intricate specifics\\nof the architecture.\\n\\nTwelve weeks later, I look back at this first week with a smile\u2014both at how much\\nI have grown with Dominik\u2019s mentorship and how little I knew about software\\nengineering and the complexities of these large codebases. In my first project,\\nI tackled problems head-on without understanding their scope. It took many\\nfrustrating sessions of left-clicking loops before I overcame my initial fear of\\nasking for help.\\n\\nTwo days into the internship, Dominik walked me through the setup for the `read`\\nand `write` operators, how to segment my tasks, and how to celebrate small wins.\\nWriting my first feature taught me to tackle issues systematically, just as one\\nmust focus only on the top of the wave. I had done more in that one hour than\\ncombined in the past two days. This began the journey of how I learned to ride\\neach wave as its own and respect the vastness of the codebase.\\n\\nThis journey of learning the basics continued as I spent the rest of the first\\nweek in a Git rebase hell, struggled with linters, and looked up Bash commands I\\nhadn\u2019t used in years. I also began to notice the differences between software\\nengineering and computer science. Soon, my love of runtime guarantees and\\ndetailed architecture shifted to writing code that worked and passed CI/CD.\\n\\nMy time at Tenzir has been eye-opening for my software engineering journey. I\u2019ve\\nlearned about the industry standards of modern C++, the scale of open-source\\nprojects, and practical problem-solving.\\n\\nBeyond writing code, I have learned about effective remote collaboration and the\\nimportance of community in tech, something I plan to carry forward in my career.\\nI am still growing, both in trying to find the balance between big-picture vs\\nindividual requirements and figuring out when to ask for help. Ultimately,\\nTenzir has prepared me for the real world and continues to fuel my passion for\\nlow-level systems.\\n\\n## Projects\\n\\nHere are some of the projects I worked on at Tenzir.\\n\\n### Parquet and Feather Parsers and Printers\\n\\nApache Parquet (and to a lesser extent its sibling Apache Feather) is a widely\\nused data formats for storing tabular data. In line with other Tenzir operators,\\nwe designed a parser plugin and printer plugin to allow users to read and write\\nFeather and Parquet data. We also adapted the Feather streaming interface to\\nstream Feather data and allowed for buffering in the Parquet printer. This\\nenables the following example:\\n\\n```text {0} title=\\"Convert a Feather file to a Parquet file\\"\\nfrom /path/to/file.feather\\n| to /path/to/file.parquet\\n```\\n\\n### Projection Pushdown Optimization\\n\\nProjection pushdown is an optimization technique that reduces data movement in a\\npipeline by pushing the projection operation closer to the data source. Tenzir\\npipelines have several operators capable of projection pushdown optimization,\\ne.g., `select`, `summarize`, `enrich`, and `drop`. This project focused on three\\nareas:\\n\\n1. Creating the framework to extend future projection pushdown optimizations.\\n2. Implementing projection pushdown for the `select` operator.\\n3. Modifying the Feather and Parquet data formats to accept the projection\\n   pushdown.\\n\\nWe successfully moved the `select` operator up through the pipeline:\\n\\n```text {0} title=\\"What the user writes\\"\\nfrom ./example.json\\n| \u2026\\n| select col\\n```\\n\\n```text {0} title=\\"What Tenzir runs\\"\\nfrom ./example.json\\n| select col\\n| \u2026\\n```\\n\\nProjection pushdown smartly detects whether it is safe to move up a projection\\nwithin the pipeline, operator by operator, and makes it so that the projection\\nruns as early as possible.\\n\\nFinally, we modified the Feather and Parquet parsers to accept projection\\npushdown optimizations. For example, in `read parquet | select foo`, Tenzir now\\neliminates the `select` operator entirely and only reads the column `foo` in the\\nParquet parser. Before, it read everything in the Parquet parser, and then later\\non dropped all columns but the projected ones.\\n\\n### Print Operator\\n\\nWe added a `print` operator that allows users to convert records into strings,\\nproviding an inverse to the parse operator. The following is now possible:\\n\\n```json {0} title=\\"Input\\"\\n{\\n  \\"flow_id\\": 852833247340038,\\n  \\"flow\\": {\\n    \\"pkts_toserver\\": 1,\\n    \\"pkts_toclient\\": 0,\\n    \\"bytes_toserver\\": 54,\\n    \\"bytes_toclient\\": 0\\n  }\\n}\\n```\\n\\n```text {0} title=\\"Render the field flow as CSV\\"\\nfrom input.json\\n| print flow csv --no-header\\n```\\n\\n```json {0} title=\\"Output\\"\\n{\\n  \\"flow_id\\": 852833247340038,\\n  \\"flow\\": \\"1,0,54,0\\"\\n}\\n```\\n\\n### Miscellaneous\\n\\nIn between these larger projects, I worked on smaller features and bug fixes.\\n\\nSpecifically, we modified the GeoIP context to allow for users to create empty\\ncontexts and load data later\u2014from anywhere. The following is now possible:\\n\\n```text {0} title=\\"Create an empty GeoIP context\\"\\ncontext create countries geoip\\n```\\n\\n```text {0} title=\\"Load the GeoIP database from a remote location\\"\\nload s3://path/to/countries.mmdb\\n| context load countries\\n```\\n\\nWe also modified the `python` plugin to check for syntax errors before input\\narrives. For example, the following will now error before input is read:\\n\\n```text {0} title=\\"Syntax error: did you mean \'else\'?\\"\\n\u2026\\n| python \'self.x = \\"foo\\" if self.y esle \\"bar\\"\'\\n```\\n\\nFinally, we added two timeout flags to `lookup-table` update that attaches a\\ntimeout to each event. The following is now possible:\\n\\n```text {0} title=\\"Expire lookup table entries after 10 days, or if they\'re not read for 1 day\\"\\nfrom inventory.csv\\n| context update subnets --create-timeout=10d --update-timeout=1d\\n```\\n\\n## Reflection\\n\\nI am grateful for Dominik\u2019s mentorship and Tenzir, which provided a structured\\nenvironment with collaborative coding and exciting projects. Dominik has\\nsignificantly influenced my approach to writing code, encouraging me to consider\\nuser experience and maintainability.\\n\\nInterning at Tenzir was an extraordinary experience. I am thankful to Matthias\\nfor the opportunity and for his advice on blending passion with business. This\\ninternship was a journey of firsts: my first time in Germany, my first role as a\\nC++ developer, and my first real-world application of CS education. I eagerly\\nanticipate the \\"nexts,\\" both at Tenzir and in life.\\n\\n:::note From the Team at Tenzir\\nWe all loved working with Bala\u2014it felt like he arrived just yesterday and became\\npart of the team immediately. This was the first time we\'ve had an intern at\\nTenzir, and it certainly won\'t be the last. His application came out of the\\nblue\u2014there was no advertised role for interns, but after we got an idea of his\\nhigh skill ceiling in two quick interview rounds, we thought we\'d give it a\\nshot. And it was so worth it.\\n\\nWant to get in touch with Bala? Connect with him on\\n[LinkedIn](http://linkedin.com/in/balabv/) and make sure to follow him on\\n[GitHub](https://github.com/balavinaithirthan).\\n:::"},{"id":"/tenzir-v4.16","metadata":{"permalink":"/blog/tenzir-v4.16","source":"@site/blog/tenzir-v4.16/index.md","title":"Tenzir v4.16","description":"Pipelines now connect more flexibly than ever before with [Tenzir","date":"2024-06-05T00:00:00.000Z","formattedDate":"June 5, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"context","permalink":"/blog/tags/context"},{"label":"publish","permalink":"/blog/tags/publish"},{"label":"subscribe","permalink":"/blog/tags/subscribe"}],"readingTime":2.445,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.16","authors":["dominiklohmann"],"date":"2024-06-05T00:00:00.000Z","tags":["release","context","publish","subscribe","context"],"comments":true},"prevItem":{"title":"An Intern\'s Reflection","permalink":"/blog/an-interns-reflection"},"nextItem":{"title":"Tenzir v4.15","permalink":"/blog/tenzir-v4.15"}},"content":"Pipelines now connect more flexibly than ever before with [Tenzir\\nv4.16](https://github.com/tenzir/tenzir/releases/tag/v4.16.0) and its upgraded\\n[`publish`](/operators/publish) and [`subscribe`](/operators/subscribe)\\noperators.\\n\\n![Tenzir v4.16](tenzir-v4.16.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Multi-Producer Multi-Consumer\\n\\nThe introduction of the `publish` operator with [Tenzir\\nv4.12](/blog/tenzir-v4.12) enabled split-routing of events. We frequently saw\\nusers write pipelines like this:\\n\\n```text {0} title=\\"Pipeline 1: Publish alerts\\"\\n\u2026\\n| publish alerts\\n```\\n\\n```text {0} title=\\"Pipeline 2: Save alerts with a high risk score to Splunk\\"\\nsubscribe alerts\\n| where risk_score >= 90\\n| fluent-bit splunk \u2026\\n```\\n\\n```text {0} title=\\"Pipeline 3: Save all alerts to S3 for later reference\\"\\nsubscribe alerts\\n| to s3://bucket/alerts.json.zst write json --compact-output\\n```\\n\\nThis approach, however, fell apart as soon as another data source tried to\\npublish to the topic `alerts`. Trying to do so just displayed an  error.\\nFundamentally, the `publish` and `subscribe` operators were single-producer,\\nmulti-consumer (SPMC).\\n\\nWith Tenzir v4.16, the `publish` operator\'s topics no longer have to be unique,\\nmaking it possible to easily merge data flows back together:\\n\\n```text {0} title=\\"Pipeline 4: Publish further alerts\\"\\n\u2026\\n| publish alerts\\n```\\n\\nThis seemingly small change makes Tenzir\'s pipelines more flexible than ever\\nbefore. Now, you can write modular pipelines for individual parts of your use\\ncases.\\n\\nFor example, imagine that you have a pipeline that persists events to \\"cold\\nstorage\\" by writing them to S3 in a strongly compressed format:\\n\\n```text\\nsubscribe to-cold-storage\\n| to s3://bucket/cold_storage.json.zst write json --compact-output\\n```\\n\\nNow, you can re-use this building block easily from any pipeline:\\n\\n```text\\n\u2026\\n| publish to-cold-storage\\n```\\n\\n## Aggregation Functions for Percentiles\\n\\nWe\'ve added new aggregation functions for calculating percentiles: `p99`, `p95`,\\n`p90`, `p75`, and `p50`. For example, to plot the 99th percentile of the number\\nof packets sent per flow, you can now write:\\n\\n```text\\n\u2026\\n| where #schema == \\"suricata.flow\\"\\n| summarize p99(flow.pkts_toserver)\\n```\\n\\nWe\'ve additionally renamed the `approximate_median` function to `median`. We\\nfound the longer name to be unintuitive and cumbersome to write, so we decided\\nto simplify it.\\n\\n## Erase Lookup Table Entries\\n\\nA user recently showed me this abomination consisting of three pipelines:\\n\\n```text {0} title=\\"Pipeline 1: Save lookup table\\"\\ncontext inspect my-lookup-table\\n| to /tmp/my-lookup-table.json\\n```\\n\\n```text {0} title=\\"Pipeline 2: Wipe the lookup table\\"\\ncontext reset my-lookup-table\\n```\\n\\n```text {0} title=\\"Pipeline 3: Restore the lookup table without some keys\\"\\nfrom /tmp/my-lookup-table.json\\n| yield value\\n| where key !in [\\"foo\\", \\"bar\\", \\"baz\\"]\\n| context update my-lookup-table\\n```\\n\\nThis is a lot of work just to erase three values from a lookup table. With\\nTenzir v4.16, you can now erase entries from a lookup table directly.\\n\\n```text\\ncontext inspect my-lookup-table\\n| yield value\\n| where key in [\\"foo\\", \\"bar\\", \\"baz\\"]\\n| context update my-lookup-table --erase\\n```\\n\\n## Other Changes\\n\\nFor a full list of changes in this release, please check our\\n[changelog](/changelog#v4160), and play with the new changes at\\n[app.tenzir.com](https://app.tenzir.com).\\n\\nAre you using `publish` and `subscribe` to connect your pipelines already? We\'d\\nlike to hear your thoughts! Join [our Discord server](/discord)."},{"id":"/tenzir-v4.15","metadata":{"permalink":"/blog/tenzir-v4.15","source":"@site/blog/tenzir-v4.15/index.md","title":"Tenzir v4.15","description":"Tenzir v4.15 is now","date":"2024-05-31T00:00:00.000Z","formattedDate":"May 31, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"context","permalink":"/blog/tags/context"},{"label":"rpm","permalink":"/blog/tags/rpm"}],"readingTime":2.025,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.15","authors":["dominiklohmann"],"date":"2024-05-31T00:00:00.000Z","tags":["release","context","rpm"],"comments":true},"prevItem":{"title":"Tenzir v4.16","permalink":"/blog/tenzir-v4.16"},"nextItem":{"title":"Tenzir v4.14","permalink":"/blog/tenzir-v4.14"}},"content":"[Tenzir v4.15](https://github.com/tenzir/tenzir/releases/tag/v4.15.0) is now\\navailable for download. The Tenzir Platform now shows live-updating pipeline\\nactivity, and the Tenzir Node has improved support for subnet keys in lookup\\ntables, and installs natively for RedHat Linux and its derivatives.\\n\\n![Tenzir v4.15](tenzir-v4.15.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Pipeline Activity\\n\\nThe pipeline overview page on [app.tenzir.com](https://app.tenzir.com) now\\nfeatures a live-updating view of recent pipeline activity.\\n\\nGrab some popcorn, sit back, and watch the data flow! \ud83c\udf7f\\n\\n:::info Join the conversation!\\nWe plan to make pipelines easier to introspect and to manage. Do you have ideas,\\nor want to take a sneak peek at what\'s coming up?\\n\\nEvery Tuesday at at 8 AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST, we\'re hosting\\n[Office Hours](/blog/introducing-office-hours) in our [Discord](/discord). Come\\njoin the discussion.\\n:::\\n\\n## RedHat Linux Support\\n\\nInstalling a Tenzir Node is now easier than ever on RedHat Linux and its\\nderivatives. As of this release, it is now possible to install Tenzir as an RPM\\npackage. Just run the installer script to get started:\\n\\n```bash\\ncurl https://get.tenzir.app | sh\\n```\\n\\n## Subnet Keys in Lookup Tables\\n\\nThe [`lookup-table` context](/contexts/lookup-table) now handles subnet keys\\ncorrectly. Lookups with an IP address or subnet value now match if the key is\\nwithin any of the subnets used as keys. If multiple subnets match, then the best\\nmatch is returned.\\n\\n:::info Want to learn more?\\nWe wrote a new user guide about using subnet keys in lookup tables: [Enrich with\\nNetwork Inventory](/next/user-guides/enrich-with-network-inventory).\\n:::\\n\\n## Sort by Multiple Fields\\n\\nThe `sort` operator now supports sorting by multiple fields, specified in order\\nof precedence. The following two examples have the exact same behavior, making\\nsorting by multiple fields much easier:\\n\\n```text {0} title=\\"Before: sorting was limited to one field at a time\\"\\nsort baz\\n| sort --stable bar\\n| sort --stable foo\\n```\\n\\n```text {0} title=\\"After: sorting now supports multiple fields\\"\\nsort foo, bar, baz\\n```\\n\\n## Edit Pipelines\\n\\nThe `/pipeline/update` API endpoint now supports updating the definition of a\\npipeline. This functionality will soon be available in the app for nodes that\\nsupport it. Gone are the times where editing a pipeline required copying its\\ndefinition, redeploying it, deleting the old and starting the new version.\\n\\n## Other Changes\\n\\nFor a full list of enhancements, adjustments, and bug fixes in this release,\\nplease check our [changelog](/changelog#v4150).\\n\\nExplore the latest features at [app.tenzir.com](https://app.tenzir.com) and\\njoin the conversation on [our Discord server](/discord)."},{"id":"/tenzir-v4.14","metadata":{"permalink":"/blog/tenzir-v4.14","source":"@site/blog/tenzir-v4.14/index.md","title":"Tenzir v4.14","description":"Introducing [Tenzir","date":"2024-05-17T00:00:00.000Z","formattedDate":"May 17, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"slice","permalink":"/blog/tags/slice"},{"label":"summarize","permalink":"/blog/tags/summarize"},{"label":"streaming-aggregation","permalink":"/blog/tags/streaming-aggregation"}],"readingTime":1.42,"hasTruncateMarker":true,"authors":[{"name":"Johannes Misch","title":"Software Engineer","url":"https://github.com/IyeOnline","email":"johannes@tenzir.com","imageURL":"https://github.com/IyeOnline.png","key":"IyeOnline"}],"frontMatter":{"title":"Tenzir v4.14","authors":["IyeOnline"],"date":"2024-05-17T00:00:00.000Z","tags":["release","slice","summarize","streaming-aggregation"],"comments":true},"prevItem":{"title":"Tenzir v4.15","permalink":"/blog/tenzir-v4.15"},"nextItem":{"title":"Tenzir v4.13","permalink":"/blog/tenzir-v4.13"}},"content":"Introducing [Tenzir\\nv4.14](https://github.com/tenzir/tenzir/releases/tag/v4.14.0): A major update to\\nthe `summarize` operator with new aggreagtion functions, and support for slicing\\nwith strides.\\n\\n![Tenzir v4.14](tenzir-v4.14.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Introducing Streaming Aggregation\\n\\nOur `summarize` operator just got a whole lot smarter! With new `timeout` and\\n`update-timeout` options, you can now perform streaming aggregations. These\\nsettings determine how long a bucket remains active based on when the first and\\nlast events arrive. `timeout` keeps events for a specified duration, while\\n`update-timeout` helps finalize buckets sooner when grouped events arrive\\nquickly.\\n\\n## New Statistical Aggregation Functions\\n\\nGet ready to enhance your data analysis with four new aggregation functions:\\n\\n- `mean`: Calculates the average of grouped numeric values.\\n- `approximate_median`: Uses the t-digest algorithm to find an approximate\\n  median for grouped numbers.\\n- `stddev`: Computes the standard deviation of grouped numeric values.\\n- `variance`: Calculates the variance within the grouped data.\\n\\nThese functions will help you gain more insights and precision in your data\\nsummaries.\\n\\n## Enhanced Slicing with Strides\\n\\nThe `slice` operator now has a more flexible argument format: `<begin>:<end>`.\\nHere are some examples:\\n\\n- `slice 10:`: Skips the first ten events.\\n- `slice 10:20`: Includes events from 10 to 19.\\n- `slice :-10`: Omits the last ten events.\\n\\nWe\'ve also added support for strides. Use `slice <begin>:<end>:<stride>` to\\nspecify steps between events. Want to reverse the event order? The new `reverse`\\noperator does just that, equivalent to `slice ::-1`.\\n\\n## More Updates and Improvements\\n\\nFor detailed information on all the enhancements, adjustments, and fixes in this\\nrelease, check out our [changelog](/changelog#v4140).\\n\\nDive into the new features at [app.tenzir.com](https://app.tenzir.com), and be\\nsure to join the conversation on [our Discord server](/discord).\\n\\nWe hope you enjoy the enhancements in Tenzir v4.14!"},{"id":"/tenzir-v4.13","metadata":{"permalink":"/blog/tenzir-v4.13","source":"@site/blog/tenzir-v4.13/index.md","title":"Tenzir v4.13","description":"We\'ve just released [Tenzir","date":"2024-05-10T00:00:00.000Z","formattedDate":"May 10, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"maintenance","permalink":"/blog/tags/maintenance"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"leef","permalink":"/blog/tags/leef"},{"label":"syslog","permalink":"/blog/tags/syslog"}],"readingTime":2.125,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.13","authors":["dominiklohmann"],"date":"2024-05-10T00:00:00.000Z","tags":["release","maintenance","performance","leef","syslog"],"comments":true},"prevItem":{"title":"Tenzir v4.14","permalink":"/blog/tenzir-v4.14"},"nextItem":{"title":"Tenzir v4.12","permalink":"/blog/tenzir-v4.12"}},"content":"We\'ve just released [Tenzir\\nv4.13](https://github.com/tenzir/tenzir/releases/tag/v4.13.0), a release\\nfocusing on stability and incremental improvements over the [feature-packed past\\nreleases](/blog/tags/release).\\n\\n![Tenzir v4.13](tenzir-v4.13.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Acquire LEEF Over Syslog\\n\\nTenzir now speaks LEEF out of the box. The [Log Event Extended Format\\n(LEEF)][leef] is an event representation popularized by IBM QRadar. Many tools\\nsend LEEF over [Syslog](/formats/syslog).\\n\\n[leef]: https://www.ibm.com/docs/en/dsm?topic=overview-leef-event-components\\n\\nLEEF is a line-based format and every line begins with a *header* that is\\nfollowed by *attributes* in the form of key-value pairs.\\n\\nLEEF v1.0 defines 5 header fields and LEEF v2.0 has an additional field to\\ncustomize the key-value pair separator, which can be a single character or the\\nhex value prefixed by `0x` or `x`:\\n\\n```\\nLEEF:1.0|Vendor|Product|Version|EventID|\\nLEEF:2.0|Vendor|Product|Version|EventID|DelimiterCharacter|\\n```\\n\\nFor LEEF v1.0, the tab (`\\\\t`) character is hard-coded as attribute separator.\\n\\nHere are some real-world LEEF events:\\n\\n```\\nLEEF:1.0|Microsoft|MSExchange|2016|15345|src=10.50.1.1\\tdst=2.10.20.20\\tspt=1200\\nLEEF:2.0|Lancope|StealthWatch|1.0|41|^|src=10.0.1.8^dst=10.0.0.5^sev=5^srcPort=81^dstPort=21\\n```\\n\\nTenzir translates the event attributes into a nested record, where the key-value\\npairs map to record fields. Here is an example of the parsed events from above:\\n\\n```json\\n{\\n  \\"leef_version\\": \\"1.0\\",\\n  \\"vendor\\": \\"Microsoft\\",\\n  \\"product_name\\": \\"MSExchange\\",\\n  \\"product_version\\": \\"2016\\",\\n  \\"attributes\\": {\\n    \\"src\\": \\"10.50.1.1\\",\\n    \\"dst\\": \\"2.10.20.20\\",\\n    \\"spt\\": 1200,\\n  }\\n}\\n{\\n  \\"leef_version\\": \\"2.0\\",\\n  \\"vendor\\": \\"Lancope\\",\\n  \\"product_name\\": \\"StealthWatch\\",\\n  \\"product_version\\": \\"1.0\\",\\n  \\"attributes\\": {\\n    \\"src\\": \\"10.0.1.8\\",\\n    \\"dst\\": \\"10.0.0.5\\",\\n    \\"sev\\": 5,\\n    \\"srcPort\\": 81,\\n    \\"dstPort\\": 21\\n  }\\n}\\n```\\n\\nLEEF events typically transmit through Syslog as demonstrated here:\\n\\n```syslog\\n<12>Nov 21 13:44:35 LAPTOP-45Q5L6E5 Microsoft-Windows-Security-Mitigations[4340]: LEEF:2.0|Microsoft|Microsoft-Windows-Security-Mitigations|4.6.4640-trial|10|0x09|devTime=2019-11-21 \u2026 (truncated for brevity)\\n```\\n\\nWith Tenzir\'s [`parse`](/operators/parse) operator, parsing nested data\\nstructures like LEEF in Syslog becomes straightforward. For instance, the\\nfollowing pipeline reads Syslog containing LEEF over UDP:\\n\\n```\\nfrom udp://127.0.0.1:514 -n read syslog\\n| parse content leef\\n```\\n\\n```json\\n{\\n  \\"facility\\": 1,\\n  \\"severity\\": 4,\\n  \\"timestamp\\": \\"Nov 21 13:44:35\\",\\n  \\"hostname\\": \\"LAPTOP-45Q5L6E5\\",\\n  \\"app_name\\": \\"Microsoft-Windows-Security-Mitigations\\",\\n  \\"process_id\\": \\"4340\\",\\n  \\"content\\": {\\n    \\"leef_version\\": \\"2.0\\",\\n    \\"vendor\\": \\"Microsoft\\",\\n    \\"product_name\\": \\"Microsoft-Windows-Security-Mitigations\\",\\n    \\"product_version\\": \\"4.6.4640-trial\\",\\n    \\"attributes\\": {\\n      \\"devTime\\": \\"2019-11-21T13:44:35.000000\\",\\n      // \u2026 (truncated for brevity)\\n    }\\n  }\\n}\\n```\\n\\n## Cron Scheduling\\n\\nExpanding on our scheduling capabilities, Tenzir now includes a\\n[`cron`](https://docs.tenzir.com/next/language/operator-modifiers#cron) operator\\nmodifier enabling precise scheduling using [Crontab\\nsyntax](https://crontab.guru) instead of fixed intervals.\\n\\nFor example, the following pipeline does an API request every Sunday at 04:05 in\\nthe morning:\\n\\n```\\ncron \\"0 5 4 * * SUN\\" from https://example.com/api\\n```\\n\\n## Performance Improvements for Imports\\n\\nThe [`import`](/operators/import) operator introduces a slight delay (up to one\\nsecond) in event handling to batch events by schema, substantially enhancing\\nperformance.\\n\\n![Import Reordering](import-reordering.excalidraw.svg)\\n\\n## Other Changes\\n\\nFor a full list of enhancements, adjustments, and bug fixes in this release,\\nplease check our [changelog](/changelog#v4130).\\n\\nExplore the latest features at [app.tenzir.com](https://app.tenzir.com) and\\njoin the conversation on [our Discord server](/discord)."},{"id":"/tenzir-v4.12","metadata":{"permalink":"/blog/tenzir-v4.12","source":"@site/blog/tenzir-v4.12/index.md","title":"Tenzir v4.12","description":"We are thrilled to announce Tenzir","date":"2024-04-24T00:00:00.000Z","formattedDate":"April 24, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"tcp","permalink":"/blog/tags/tcp"},{"label":"udp","permalink":"/blog/tags/udp"},{"label":"publish","permalink":"/blog/tags/publish"},{"label":"subscribe","permalink":"/blog/tags/subscribe"},{"label":"deduplicate","permalink":"/blog/tags/deduplicate"},{"label":"unroll","permalink":"/blog/tags/unroll"},{"label":"syslog","permalink":"/blog/tags/syslog"},{"label":"every","permalink":"/blog/tags/every"}],"readingTime":4.385,"hasTruncateMarker":true,"authors":[{"name":"Jannis Christopher K\xf6hl","title":"Software Engineer","url":"https://github.com/jachris","email":"jannis@tenzir.com","imageURL":"https://github.com/jachris.png","key":"jachris"}],"frontMatter":{"title":"Tenzir v4.12","authors":["jachris"],"date":"2024-04-24T00:00:00.000Z","tags":["release","tcp","udp","publish","subscribe","deduplicate","unroll","syslog","every"],"comments":true},"prevItem":{"title":"Tenzir v4.13","permalink":"/blog/tenzir-v4.13"},"nextItem":{"title":"Reduce Cost and Noise with Deduplication","permalink":"/blog/reduce-cost-and-noise-with-deduplication"}},"content":"We are thrilled to announce Tenzir\\n[v4.12](https://github.com/tenzir/tenzir/releases/tag/v4.12.1), a feature-packed\\nrelease introducing numerous enhancements. Notable additions include list\\nunrolling, event deduplication, and the deployment of advanced pipeline\\narchitectures with publish-subscribe. We\'ve also added a download button,\\nextended support for UDP, and implemented many other refinements to improve your\\nexperience.\\n\\n![Tenzir v4.12](tenzir-v4.12.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Unroll and Deduplicate Events\\n\\nIn today\'s data-driven world, many data sources deliver information in the form\\nof lists/arrays. While seemingly simple, working with these lists can sometimes\\npose challenges, particularly when the data structure becomes intricate. Let\'s\\ntake the example of a connection summary stream in JSON format:\\n\\n```json\\n{\\n  \\"src\\": \\"192.0.2.1\\",\\n  \\"dst\\": [\\n    \\"192.0.2.143\\",\\n    \\"203.0.113.2\\"\\n  ]\\n}\\n// after 1min:\\n{\\n  \\"src\\": \\"192.0.2.1\\",\\n  \\"dst\\": [\\n    \\"203.0.113.2\\",\\n    \\"172.16.76.150\\",\\n    \\"192.0.2.143\\"\\n  ]\\n}\\n```\\n\\nTo overcome the hurdles of JSON list manipulation, we introduce the new\\n[`unroll`](/next/operators/unroll) operator, allowing the creation of an event\\nfor each item in the list. Let\'s `unroll dst`:\\n\\n```json\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"192.0.2.143\\"}\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"203.0.113.2\\"}\\n// after 1min:\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"203.0.113.2\\"}\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"172.16.76.150\\"}\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"192.0.2.143\\"}\\n```\\n\\nThe data is now significantly easier to work with.\\n\\nDo you see the duplicate host pairs? Let\'s remove them with the new\\n[`deduplicate`](/next/operators/deduplicate) operator. Run `deduplicate src, dst\\n--timeout 24h` to condense the above output to:\\n\\n```json\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"192.0.2.143\\"}\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"203.0.113.2\\"}\\n// after 1min:\\n{\\"src\\": \\"192.0.2.1\\", \\"dst\\": \\"172.16.76.150\\"}\\n```\\n\\nThe `--timeout` option is useful for controlling expiration of entries. In this\\nexample, if the same connection tuple doesn\'t come up within a 24h interval, the\\ncorresponding entry is removed from the operator\'s internal state.\\n\\nWe delved deeper into the power of the `deduplicate` operator in a [previous\\nblog post](reduce-cost-and-noise-with-deduplication).\\n\\nBuilding on this, the\\n[`every`](/language/operator-modifiers#scheduled-executions) operator\\n(prominently featured in the [previous\\nrelease](tenzir-v4.11#execute-sources-on-a-schedule)) can now also accompany\\ntransformations and sinks. To illustrate, let\'s answer this question: \\"With how\\nmany new destinations did each device communicate in the last minute?\\"\\n\\nUsing `every 1min summarize num=count(.) by src`, we get:\\n\\n```json\\n{\\"src\\": \\"192.0.2.1\\", \\"num\\": 2}\\n// after 1min:\\n{\\"src\\": \\"192.0.2.1\\", \\"num\\": 1}\\n```\\n\\nIn summary, these transformations provide powerful in-band capabilities leading\\nto substantial data reduction, simplifying the analysis, and making data shaping\\nmore efficient.\\n\\n## Publish and Subscribe\\n\\nExciting are also the new [`publish`](/next/operators/publish) and\\n[`subscribe`](/next/operators/publish) operators, which open up endless\\npossibilities for creating arbitrary dataflow topologies. For instance,\\nyou can set a publishing point within your data stream. It\'s as simple as `from\\ntcp://0.0.0.0:8000 | publish input`. This defines a channel `input` that you can\\nnow subscribe to with `subscribe`.\\n\\nLet\'s consider a case where we aim to route all alerts into Splunk, and\\nconcurrently import all other non-alert events into Tenzir\'s storage for further\\nanalysis and monitoring:\\n\\n``` title=\\"1st subscriber\\"\\nsubscribe input\\n| where alert == true\\n| to splunk\\n```\\n\\n``` title=\\"2nd subscriber\\"\\nsubscribe input\\n| where alert == false\\n| import\\n```\\n\\nHere, the first subscriber takes the events from the `input` channel where the\\nalert field is `true` and routes them to Splunk. In parallel, the second\\nsubscriber takes the remaining events with the alert field marked as `false` and\\nimports them into Tenzir\u2019s storage. Our new feature enables this precise,\\ndynamic routing, making data management more efficient and streamlined.\\n\\n## Contexts as Code\\n\\n[Tenzir v4.10](tenzir-v4.10) introduced introduced the ability to statically\\ndefine pipelines in Tenzir\'s configuration file: **Pipelines as Code (PaC)**.\\nThis release expands upon that capability by also allowing static configuration\\nof contexts.\\n\\n```yaml title=\\"tenzir.yaml\\"\\ntenzir:\\n  contexts:\\n    # A unique name for the context that\'s used in the context, enrich, and\\n    # lookup operators to refer to the context.\\n    indicators:\\n      # The type of the context (e.g., `lookup-table`, `geoip`, ...).\\n      type: bloom-filter\\n      # Arguments for creating the context, as described by the documentation of\\n      # the chosen context type.\\n      arguments:\\n        capacity: 1B\\n        fp-probability: 0.001\\n```\\n\\nOn a related note: The operators `context create`, `context reset`,\\n`context update`, and `context load` were changed to no longer return\\ninformation about the associated context. Instead, they now act as a sink.\\n\\n## Download Button\\n\\nEver wanted to just save the output from Explorer to your computer? With the new\\n**Download** button, you can do precisely that:\\n\\n![Download Button](download-button.png)\\n\\nJust select one of the available formats and you\'re good to go!\\n\\n## Other Changes\\n\\n- There\'s a new [`udp`](/next/connectors/udp) connector for sending and receiving\\n  UDP datagrams. Finally, you can now receive Syslog natively.\\n- Speaking of Syslog: we\'ve enhanced our parser to be *multi-line*. In case the\\n  next line isn\'t a valid Syslog message by itself, we interpret it as the\\n  continuation of the previous message.\\n- The [`tcp`](/next/connectors/tcp) loader now accepts multiple connections in\\n  parallel, e.g., when used as `from tcp://127.0.0.1:8000 read json`.\\n- We\'ve massively improved performance of our [Parquet](/next/formats/parquet)\\n  and [Feather](/next/formats/feather) formats for large files. For writing,\\n  they now both support streaming row groups and record batches, respectively,\\n  and for reading Feather now supports streaming via the Arrow IPC format as\\n  well. This comes in handy for those of you working in the Apache Arrow\\n  ecosystem and seeking seamless interoperability without loss of rich typing.\\n\\nAs usual, the complete list of bug fixes, adjustments, and enhancements\\ndelivered with this version can be found in our [changelog](/changelog#v4120).\\n\\nExplore the latest features at [app.tenzir.com](https://app.tenzir.com) and\\nchat with us at [our Discord server](/discord)."},{"id":"/reduce-cost-and-noise-with-deduplication","metadata":{"permalink":"/blog/reduce-cost-and-noise-with-deduplication","source":"@site/blog/reduce-cost-and-noise-with-deduplication/index.md","title":"Reduce Cost and Noise with Deduplication","description":"In the bustling world of data operations, handling large volumes of information","date":"2024-03-28T00:00:00.000Z","formattedDate":"March 28, 2024","tags":[{"label":"deduplicate","permalink":"/blog/tags/deduplicate"},{"label":"cost-savings","permalink":"/blog/tags/cost-savings"}],"readingTime":4.09,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Reduce Cost and Noise with Deduplication","authors":["mavam"],"date":"2024-03-28T00:00:00.000Z","tags":["deduplicate","cost-savings"],"comments":true},"prevItem":{"title":"Tenzir v4.12","permalink":"/blog/tenzir-v4.12"},"nextItem":{"title":"Tenzir v4.11","permalink":"/blog/tenzir-v4.11"}},"content":"In the bustling world of data operations, handling large volumes of information\\nis an everyday affair. Each day, countless bytes of data move around in systems,\\nchallenging organizations to maintain data accuracy, efficiency, and\\ncost-effectiveness. Amid this vast data landscape, one concept has emerged as a\\ncritical ally\u2014**deduplication**.\\n\\n![Deduplicate Example](deduplicate-example.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nDeduplication, the process of eliminating duplicate copies of repeating data,\\nisn\'t just about reducing storage needs\u2014it\'s about clarity, precision, and\\nreducing computational overload. It refines data to its most essential elements,\\nallowing us to gain vital insights with fewer extraneous distractions. And as\\nour data continues to grow in both volume and complexity, the need for effective\\nand efficient deduplication strategies cannot be underestimated. To make this\\ntask of deduplication easier, we\'re introducing an efficient and robust tool:\\nthe [`deduplicate`](/next/operators/deduplicate) operator. This operator serves\\nas a powerful ally in the fight against redundant data, enabling easy removal of\\nduplicate entries directly within a data stream or data set\u2014delivering cleaner,\\nleaner, and more useful data that holds the genuine insights your business\\nseeks.\\n\\nJoin us as we dive into the world of deduplication, explore the mechanics and\\nbenefits of the [`deduplicate`](/next/operators/deduplicate) operator, and\\nunderstand how it can redefine the way your organization approaches data\\nmanagement and analysis. The journey towards more effective and efficient data\\noperations starts here.\\n\\n:::note Available in Tenzir v4.12\\nWe\'re releasing Tenzir v4.12 next week, which will include the `deduplicate`\\noperator. Until then, you can play with it on the main branch on our [GitHub\\nrepo](https://github.com/tenzir/tenzir). Stay tuned.\\n:::\\n\\n## Why Deduplicate?\\n\\nDeduplication of operational event data is vital in various contexts dealing\\nwith large volumes of data, as it helps streamline information, eliminate\\nredundancies, and save storage. Here are three reasons for why you may want to\\nconsider deduplication as central tool for your data operations:\\n\\n### Enhanced Monitoring and Analytics\\n\\nDeduplication plays a crucial role in system and network monitoring, increasing\\nthe visibility of genuine operational patterns. In anomaly detection and\\nintrusion detection, deduplication refines the focus towards unique\\ninconsistencies or intrusions, paving the way for precise identification of\\nthreats, and thus, enhancing the accuracy and effectiveness of monitoring\\nsystems.\\n\\n### Efficient Threat Detection & Response\\n\\nDeduplication filters out multiple instances of the same security alert,\\nenabling a focus on unique threats. This allows for a swift, focused respons to\\nactual incidents, minimizing the distraction caused by redundant alerts and\\nenhancing overall operational efficiency and mean-time-to-respond (MTTR)\\nsignificantly.\\n\\n### Cost Optimization\\n\\nFor log management and SIEM systems, deduplication decreases ingest volume to\\nreduce costs associated with ingest-based pricing models. Additionally,\\ndeduplication reduces storage demands, optimizes search performance, and enables\\nmore effective threat detection, thus achieving both economic efficiency and\\noperational optimization in SIEM operations.\\n\\n## How Tenzir solves Deduplication\\n\\nLet\'s dive deeper into the `deduplicate` operator. It functions by marking out\\nduplicate events within a data stream, scrutinizing these duplicates based on\\nthe values of one or more fields. This discernment allows the operator to\\ndifferentiate between truly repetitive data and individually unique events. You\\ncan alter this behavioral pattern to suit your specific needs by manipulating\\nthree primary control knobs: **Limit**, **Distance**, and **Timeout**.\\n\\nThe diagram below illustrates these knobs intuitively:\\n\\n![Deduplicate](deduplicate.excalidraw.svg)\\n\\nHere\'s a more in-depth description of the controls:\\n\\n1. **Limit**: This knob controls the quantity of duplicate events that are\\n   permissible before they\'re classified as duplicates. Setting a limit of 1\\n   allows the emission of only unique events, making it the most stringent level\\n   of deduplication. A limit of N allows for the emission of a unique event N\\n   times before considering subsequent instances as duplicates. For instance,\\n   given a data stream `AAABAABBAC` with a limit of 2, the output\\n   post-deduplication would be `AABBC`.\\n\\n2. **Distance**: This knob is a measure of how far apart two events can be for\\n   them to be regarded as duplicates. Deduplicating a stream `AABABACBABB` with\\n   a distance set to 3 would yield `ABC`. A low distance value means that only\\n   closely placed events will be considered duplicates, while a higher value (or\\n   0 for infinity) broadens this criterion.\\n\\n3. **Timeout**: This parameter introduces a temporal aspect to the concept of\\n   duplicates. The timeout value dictates the duration that needs to elapse\\n   before a previously suppressed duplicate is considered a unique event again.\\n   If a duplicate event occurs before the specified timeout, the timer resets.\\n\\nYou can decide the fields to apply these knobs on by specifying a list of field\\nextractors. By default, the operator works on the entire event.\\n\\n:::tip Real-world examples\\nLet\'s make the abstract concrete and dive right in! Read our [user\\nguide](/next/user-guides/deduplicate-events) on deduplicating events for\\nhands-on examples.\\n:::\\n\\nBy bringing together these strategically implemented parameters, the\\n`deduplicate` becomes a flexible, customizable tool for decluttering your data\\nstreams. It makes the daunting process of manual deduplication a thing of the\\npast and allows data operators to focus on the rich insights that uniquely\\noccurring events can provide."},{"id":"/tenzir-v4.11","metadata":{"permalink":"/blog/tenzir-v4.11","source":"@site/blog/tenzir-v4.11/index.md","title":"Tenzir v4.11","description":"Our latest v4.11","date":"2024-03-22T00:00:00.000Z","formattedDate":"March 22, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"contexts","permalink":"/blog/tags/contexts"},{"label":"every","permalink":"/blog/tags/every"},{"label":"set","permalink":"/blog/tags/set"},{"label":"email","permalink":"/blog/tags/email"},{"label":"sqs","permalink":"/blog/tags/sqs"}],"readingTime":5.07,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.11","authors":["dominiklohmann"],"date":"2024-03-22T00:00:00.000Z","tags":["release","contexts","every","set","email","sqs"],"comments":true},"prevItem":{"title":"Reduce Cost and Noise with Deduplication","permalink":"/blog/reduce-cost-and-noise-with-deduplication"},"nextItem":{"title":"Tenzir v4.10","permalink":"/blog/tenzir-v4.10"}},"content":"Our latest [v4.11](https://github.com/tenzir/tenzir/releases/tag/v4.11.1)\\nrelease delivers powerful automation features, such as scheduling pipelines in a\\ngiven time interval and sending pipeline data as emails.\\n\\n![Tenzir v4.11](tenzir-v4.11.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Execute Sources on a Schedule\\n\\nOne feedback we\'ve heard often from users is that the\\n[`from <url>`](/connectors) invocation is indeed handy, but it\'s more practical\\nwhen it\'s not a one-time gig. Users expressed a need to retrieve data from\\nvarious sources more than just once, indicating a requirement for a more\\ncyclical or scheduled approach.\\n\\nGiven these requirements, we initially considered adding options for continuous\\ndata retrieval or polling for specific connectors, such as\\n[`http`](/connectors/http). However, we realized that the need for such\\nfunctionality ranged beyond a limited number of connectors. Hence, any solution\\nwe developed would ideally adapt to any source operator, providing wider\\nfunctionality.\\n\\nIn response to these needs, we developed a new [operator\\nmodifier](/next/language/operator-modifiers) that empowers any source operator\\nto execute at regular intervals: `every <interval>`.\\n\\nFor instance, the operator `every 1s from <url>` will enable the system to poll\\nthe specified URL every single second. The capability delivers continuous,\\nreal-time data access, considerably improving the feasibility and efficiency of\\ntasks requiring frequent data updates.\\n\\nOne area where we\'ve found the `every <interval>` modifier to be especially\\nvaluable is in the context of updating contexts. Consider a pipeline designed to\\nupdate a lookup-table context titled `threatfox-domains` once every hour. This\\noperation, which fetches IOCs (Indicators of Compromise) from the ThreatFox API,\\ncan be achieved using the following pipeline:\\n\\n```\\nevery 1 hour from https://threatfox-api.abuse.ch/api/v1/ query=get_iocs days:=1\\n| yield data[]\\n| where ioc_type == \\"domain\\"\\n| context update threatfox-domains --key ioc\\n```\\n\\nThis pipeline initiates a query to retrieve the IOCs for the day from the\\nThreatFox API. The pipeline subsequently filters out the data relevant to\\ndomains and updates the `threatfox-domains` context. The entire pipeline\\nrefreshes every hour as specified by the `every 1 hour` operator modifier.\\n\\nThe `every <interval>` operator modifier thus adds a powerful tool to our\\narsenal, increasing our capabilities by adapting to any source operator for\\nscheduled execution.\\n\\n## Enrich More Flexibly\\n\\nA customer of ours asked a seemingly simple question: If I have a lookup-table\\ncontext that contains entries in the form `{\\"key\\": \\"DE\\", \\"context\\": {\\"flag\\":\\n\\"\ud83c\udde9\ud83c\uddea\\"}}` and want to use it to replace country short codes with their respective\\nflag as an emoji, how can I do that?\\n\\nIf you\'re just replacing the value of a single field then it\'s easy\u2014you can just\\nuse [`put`](/operators/put) to replace the input value with its context after\\nthe enrichment. But this user wanted to look into every single string in every\\nevent, and replace all country short codes that it contained.\\n\\nTwo newly added options for the [`enrich`](/next/operators/enrich) operator make\\nthis easily possible:\\n\\n```\\n\u2026\\n| enrich country-flags --field :string --yield flag --replace\\n```\\n\\nThe `--replace` flag causes `enrich` to replace fields with their context, if\\nthey exists. The option `--yield <field>` trims down the enrichment to just a\\nspecific field within the context. The `--yield` option is also available for\\nthe [`lookup`](/operators/lookup) operator.\\n\\n```json title=\\"Before\\"\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"DE\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"US\\"\\n}\\n```\\n\\n```json title=\\"After\\"\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"\ud83c\udde9\ud83c\uddea\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"\ud83c\uddfa\ud83c\uddf8\\"\\n}\\n```\\n\\nThe other new option of `lookup` and `enrich` is `--separate`, which creates\\nseparate events for every enrichment. This causes events to be duplicated for\\nevery enrichment from a context that applies, with one enrichment per event in\\nthe result. This is particularly useful in `lookup` when evaluating a large set\\nof IOCs to create separate alerts per IOC even within a single event.\\n\\n```json title=\\"Enriched as one event\\"\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"DE\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"US\\",\\n  \\"flags\\": {\\n    \\"source_ip\\": {\\n      \\"value\\": \\"212.12.56.176\\"\\n      \\"timestamp\\": \\"2024-03-21T15:12:07.493155\\",\\n      \\"mode\\": \\"enrich\\",\\n      \\"context\\": {\\n        \\"flag\\": \\"\ud83c\udde9\ud83c\uddea\\"\\n      }\\n    }\\n    \\"dest_ip\\": {\\n      \\"value\\": \\"8.8.8.8\\"\\n      \\"timestamp\\": \\"2024-03-21T15:12:07.493155\\",\\n      \\"mode\\": \\"enrich\\",\\n      \\"context\\": {\\n        \\"flag\\": \\"\ud83c\uddfa\ud83c\uddf8\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\n```json title=\\"Enriched as separate events\\"\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"DE\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"US\\",\\n  \\"flags\\": {\\n    \\"path\\": \\"source_ip\\",\\n    \\"value\\": \\"212.12.56.176\\"\\n    \\"timestamp\\": \\"2024-03-21T15:12:07.493155\\",\\n    \\"mode\\": \\"enrich\\",\\n    \\"context\\": {\\n      \\"flag\\": \\"\ud83c\udde9\ud83c\uddea\\"\\n    }\\n  }\\n}\\n{\\n  \\"source_ip\\": 212.12.56.176,\\n  \\"source_iso_code\\": \\"DE\\",\\n  \\"dest_ip\\": 8.8.8.8,\\n  \\"dest_iso_code\\": \\"US\\",\\n  \\"flags\\": {\\n    \\"path\\": \\"source_ip\\",\\n    \\"value\\": \\"8.8.8.8\\"\\n    \\"timestamp\\": \\"2024-03-21T15:12:07.493155\\",\\n    \\"mode\\": \\"enrich\\",\\n    \\"context\\": {\\n      \\"flag\\": \\"\ud83c\uddfa\ud83c\uddf8\\"\\n    }\\n  }\\n}\\n```\\n\\n## The Sweet Spot Between Extend and Replace\\n\\nThe [`set`](/next/operators/set) operator \\"upserts\\" into events. Its syntax\\nexactly matches the syntax of the existing [`extend`](/next/operators/extend),\\n[`replace`](/next/operators/replace), and [`put`](/next/operators/put)\\noperators.\\n\\nIf a specified field already exists, the `set` operator replaces its value. If\\nit does not, the `set` operator extends the event with new field. We found this\\nbehavior to be quite intuitive, and in most cases we now reach for `set` instead\\nof `replace` and `extend`.\\n\\n:::tip Setting the Schema Name\\nThe `set`, `put`, and `replace` operator support changing the schema name of\\nevents. For example, `set #schema=\\"foo.bar\\"` will show up as a schema `bar` in\\nthe category `foo` in the Explorer on [app.tenzir.com](https://app.tenzir.com).\\n:::\\n\\n## Send Emails from a Pipeline\\n\\nThe new [`email`](/next/connectors/email) saver sends away pipeline contents as\\nmails. This is especially handy for integrating with traditional escalation\\npathways that rely on email-based dispatching methods.\\n\\nFor example, to send all Suricata alerts arriving at a node via email, use:\\n\\n```\\nexport --live\\n| where #schema == \\"suricata.alert\\"\\n| write json\\n| save email alerts@example.org --from \\"tenzir@example.org\\" --subject Alert\\n```\\n\\nThe `email` saver supports both SMTP and SMTPS. The default endpoint is\\n`smtp://localhost:25`, but you can provide any other server. Instead of copying\\nthe rendered JSON directly into the email body, you can also provide the\\n`--mime` to send a MIME-encoded chunk that uses the MIME type according to the\\nformat you provided.\\n\\n## Working with Amazon SQS Queues\\n\\nThe new [`sqs`](/next/connectors/sqs) enables reading from and writing to Amazon\\nSQS queues. For example, importing JSON from an SQS queue named `tenzir` into a\\nnode looks like this:\\n\\n```\\nfrom sqs://tenzir | import\\n```\\n\\n## Other Changes\\n\\nAs usual, the complete list of bug fixes, adjustments, and enhancements\\ndelivered with this version can be found in [the changelog](/changelog#v4110).\\n\\nExplore the latest features at [app.tenzir.com](https://app.tenzir.com) and\\nconnect with us on [our Discord server](/discord)."},{"id":"/tenzir-v4.10","metadata":{"permalink":"/blog/tenzir-v4.10","source":"@site/blog/tenzir-v4.10/index.md","title":"Tenzir v4.10","description":"Today, we\'re releasing [Tenzir","date":"2024-03-11T00:00:00.000Z","formattedDate":"March 11, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"pipelines-as-code","permalink":"/blog/tags/pipelines-as-code"},{"label":"contexts","permalink":"/blog/tags/contexts"},{"label":"arm64","permalink":"/blog/tags/arm-64"}],"readingTime":4.065,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.10","authors":["dominiklohmann"],"date":"2024-03-11T00:00:00.000Z","tags":["release","pipelines-as-code","contexts","arm64"],"comments":true},"prevItem":{"title":"Tenzir v4.11","permalink":"/blog/tenzir-v4.11"},"nextItem":{"title":"Tenzir v4.9","permalink":"/blog/tenzir-v4.9"}},"content":"Today, we\'re releasing [Tenzir\\nv4.10](https://github.com/tenzir/tenzir/releases/tag/v4.10.0), which improves\\nhow Tenzir integrates with modern deployment practices.\\n\\n![Tenzir v4.10](tenzir-v4.10.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Pipelines as Code\\n\\nIn today\'s deployment landscape, best practices emphasize GitOps in synergy with\\nInfrastructure as Code (IaC). With the goal of integrating our services into\\nthese existing mechanisms, we\'re excited to introduce Pipelines as Code (PaC) in\\nTenzir v4.10.\\n\\nPaC differs from traditional deployment methods in two key aspects. Firstly,\\npipelines deployed as code always start with the Tenzir node, ensuring\\ncontinuous operation. Secondly, to safeguard them, deletion via the user\\ninterface is disallowed for pipelines deployed as code.\\n\\nHere\'s a simple example to get you started:\\n\\n```yaml {0} title=\\"<prefix>/etc/tenzir/tenzir.yaml\\"\\ntenzir:\\n  pipelines:\\n    suricata-over-tcp:\\n      name: Import Suricata from TCP\\n      definition: |\\n        from tcp://0.0.0.0:34343 read suricata\\n        | import\\n      start:\\n        failed: true  # always restart on failure\\n```\\n\\n:::tip Want to learn more?\\nRead our guide on PaC: \ud83d\udc49 [Deploy Pipelines as\\nCode](/user-guides/run-pipelines#as-code)\\n:::\\n\\n## arm64 Docker Images\\n\\nDid you ever try to run Tenzir in Docker on a new-ish MacBook and encountered\\nthis error?\\n\\n```text {0} title=\\"\u276f docker run tenzir/tenzir:v4.9.0 version\\"\\nWARNING: The requested image\'s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\\ntenzir: error while loading shared libraries: libfluent-bit.so: cannot enable executable stack as shared object requires: Invalid argument\\n```\\n\\nNow, this works as expected:\\n\\n```json {0} title=\\"\u276f docker run tenzir/tenzir:v4.10.0 version\\"\\n{\\n  \\"version\\": \\"4.10.0\\",\\n  \\"build\\": \\"\\",\\n  \\"major\\": 4,\\n  \\"minor\\": 10,\\n  \\"patch\\": 0\\n}\\n```\\n\\nThis works because the Tenzir Docker images now are multi-archecture images\\nbuilt natively for both `linux/amd64` and `linux/arm64/v8`. In addition to\\nsupporting M-series MacBooks, this also allows the Docker images to run without\\nemulation on other arm64-based systems like AWS Graviton.\\n\\n## Reimagining Unsafe Pipelines\\n\\nWe\'ve substituted the `tenzir.allow-unsafe-pipelines` feature with\\n`tenzir.no-location-overrides`, flipping the default set-up and enhancing user\\nexperience.\\n\\n`tenzir.allow-unsafe-pipelines` had been historically puzzling for newcomers\\ngiven its seemingly fearsome name and ambiguous implications. Why would someone\\nconsciously permit unsafe pipelines? And why have we now defaulted to allowing\\nthem?\\n\\nPipelines have the ability to execute in multiple processes. For instance,\\nexecuting `tenzir \'from file.json | import\'` would prompt `from file.json` to\\nrun in the `tenzir` process, and `import` in the connected `tenzir-node`\\nprocess. An operator\'s _location_ can be assigned as local, anywhere, or remote.\\nOn initializing a pipeline, Tenzir\'s executor intelligently divides the pipeline\\naccording to location change between local and remote, starts separated\\npipelines at their respective locations, connects them to one another.\\n\\nHowever, operator locations can also be manually manipulated. For instance, when\\ncapturing PCAPs, users might desire to prevent unnecessary inter-process\\ncommunication and directly connect the Tenzir Node to the network\\ninterface\u2014achieved by executing `tenzir \'remote from nic \u2026\'`. This command\\ninstructs the executor to consistently run `from nic \u2026` directly at the node.\\nWhen introducing this feature during the Tenzir v4.0 release, we wanted to be\\ncautious about unrestricted use of this feature, leading to the creation of the\\n`tenzir.allow-unsafe-pipelines` option, which by default was set to false. This\\noption prohibits the use of location overrides when enabled but simultaneously\\nposed puzzlement to new users being the lone feature disallowed in an \\"unsafe\\"\\npipeline.\\n\\nIn response to feedback, we\'ve improved our approach. Location overrides are now\\npermitted by default and can be disallowed by using the new option\\n`tenzir.no-location-overrides`.\\n\\n## Apply Contexts to Multiple Fields\\n\\nDid you ever want to act on multiple fields in `enrich` or `lookup`? Now you\\ncan!\\n\\nFor example, you can now use a [GeoIP context](/contexts/geoip) on all IP\\naddresses in your data as simple as this:\\n\\n```text {0} title=\\"Enrich with a geoip context named country\\"\\n\u2026\\n| enrich country --field :ip\\n```\\n\\nYou can also specify multiple fields explicitly:\\n\\n```\\n\u2026\\n| enrich country --field src_ip,dest_ip\\n```\\n\\nThe output of `lookup` and `enrich` changed slightly to accomodate multiple\\ncontexts in the same event. Under the output field (that defaults to the context\\nname), there is now a new record named `context`, under which we replicate the\\npath to the enriched fields for placing the context. That is, the context of\\n`id.orig_h` in this example is accessible as `country.context.id.orig_h`:\\n\\n```json {0} title=\\"export | enrich country\\"\\n{\\n  \\"ts\\": \\"2021-11-17T13:53:51.022351\\",\\n  \\"uid\\": \\"CVtvt83MWz8MBNTWWd\\",\\n  \\"id\\": {\\n    \\"orig_h\\": \\"244.69.36.0\\",\\n    \\"orig_p\\": 45228,\\n    \\"resp_h\\": \\"242.239.167.49\\",\\n    \\"resp_p\\": 34774\\n  },\\n  \\"proto\\": \\"udp\\",\\n  // ...\\n  \\"country\\": {\\n    \\"timestamp\\": \\"2024-03-11T15:58:00.596027\\",\\n    \\"mode\\": \\"enrich\\",\\n    \\"context\\": {\\n      \\"id\\": {\\n        \\"orig_h\\": {\\n          \\"country\\": {\\n            \\"geoname_id\\": 1861060,\\n            \\"iso_code\\": \\"JP\\",\\n            \\"names\\": {\\n              \\"de\\": \\"Japan\\",\\n              \\"en\\": \\"Japan\\",\\n              \\"es\\": \\"Jap\xf3n\\",\\n              \\"fr\\": \\"Japon\\",\\n              \\"ja\\": \\"\u65e5\u672c\\",\\n              \\"pt-BR\\": \\"Jap\xe3o\\",\\n              \\"ru\\": \\"\u042f\u043f\u043e\u043d\u0438\u044f\\",\\n              \\"zh-CN\\": \\"\u65e5\u672c\\"\\n            }\\n          }\\n        },\\n        \\"resp_h\\": {\\n          \\"country\\": {\\n            \\"geoname_id\\": 1861060,\\n            \\"iso_code\\": \\"JP\\",\\n            \\"names\\": {\\n              \\"de\\": \\"Japan\\",\\n              \\"en\\": \\"Japan\\",\\n              \\"es\\": \\"Jap\xf3n\\",\\n              \\"fr\\": \\"Japon\\",\\n              \\"ja\\": \\"\u65e5\u672c\\",\\n              \\"pt-BR\\": \\"Jap\xe3o\\",\\n              \\"ru\\": \\"\u042f\u043f\u043e\u043d\u0438\u044f\\",\\n              \\"zh-CN\\": \\"\u65e5\u672c\\"\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\n## Other Changes\\n\\nFor the curious, [the changelog](/changelog#v4100) includes the full list of bug\\nfixes, changes and improvements introduced with this release.\\n\\nPlay with the new features at [app.tenzir.com](https://app.tenzir.com) and join\\nus on [our Discord server](/discord)."},{"id":"/tenzir-v4.9","metadata":{"permalink":"/blog/tenzir-v4.9","source":"@site/blog/tenzir-v4.9/index.md","title":"Tenzir v4.9","description":"We\'re thrilled to announce the release of [Tenzir","date":"2024-02-21T00:00:00.000Z","formattedDate":"February 21, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"context","permalink":"/blog/tags/context"},{"label":"bloom-filter","permalink":"/blog/tags/bloom-filter"},{"label":"chart","permalink":"/blog/tags/chart"},{"label":"dashboard","permalink":"/blog/tags/dashboard"}],"readingTime":3.22,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.9","authors":["dominiklohmann"],"date":"2024-02-21T00:00:00.000Z","tags":["release","context","bloom-filter","chart","dashboard"],"comments":true},"prevItem":{"title":"Tenzir v4.10","permalink":"/blog/tenzir-v4.10"},"nextItem":{"title":"Introducing Office Hours","permalink":"/blog/introducing-office-hours"}},"content":"We\'re thrilled to announce the release of [Tenzir\\nv4.9](https://github.com/tenzir/tenzir/releases/tag/v4.9.0), enhancing the\\nExplorer further to empower you with the capability of rendering your data as a\\nchart.\\n\\n![Tenzir v4.9](tenzir-v4.9.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Chart Operator\\n\\nThe new [`chart`](/next/operators/chart) operator transforms the way you\\nvisualize your data on [app.tenzir.com](https://app.tenzir.com). It lets you\\ndepict your events graphically instead of in table form.\\n\\nCharting integrates seamlessly into your pipelines by simply adding the `chart`\\noperator. For instance, plotting a bar chart representing the frequency of\\noccurrences for each protocol in `zeek.conn` events can be as simple as this:\\n\\n```\\nexport\\n| where #schema == \\"zeek.conn\\"\\n| top proto\\n| chart bar --title \\"Protocols\\"\\n```\\n\\n![Protocols](https://github.com/tenzir/tenzir/assets/4488655/075cf3af-ed51-4aca-8885-6f682284831c)\\n\\nThis line chart depicts the load average over 15 minutes, making use of the\\nrecently added `metrics` operator:\\n\\n```\\nmetrics\\n| where #schema == \\"tenzir.metrics.cpu\\"\\n| sort timestamp\\n| chart line -x timestamp -y loadavg_15m --title \\"Load Average (15 min)\\"\\n```\\n\\n![Load Average (15 min)](https://github.com/tenzir/tenzir/assets/4488655/453bc8da-4be8-4a2c-9ef2-10328f02d682)\\n\\nThis area chart displays the total ingress across all pipelines for the past 10\\nminutes in MiB/s.\\n\\n```\\nmetrics\\n| where #schema == \\"tenzir.metrics.operator\\"\\n| where timestamp > 10 min ago\\n| where source == true\\n| where internal == false\\n| sort timestamp\\n| python \'self.egress_rate = self.output.approx_bytes / self.duration.total_seconds() / 2**20\'\\n| chart area -x timestamp -y egress_rate --title \\"Total Ingress (MiB/s)\\"\\n```\\n\\n![Total Ingress (MiB/s)](https://github.com/tenzir/tenzir/assets/4488655/a5313261-fe5d-413c-a7d9-8da781871aba)\\n\\nThis pie chart shows the distribution of events stored at the node by disk\\nusage:\\n\\n```\\nshow partitions\\n| summarize diskusage=sum(diskusage) by schema\\n| chart pie --title \\"Disk Usage (bytes)\\"\\n```\\n\\n![Disk Usage](https://github.com/tenzir/tenzir/assets/4488655/103bdb72-7708-414b-ac8c-d19562295ea3)\\n\\nWe\'re just getting started with charting! If you want to see further chart types\\nadded, have feedback on charting, or want to share examples of your\\nvisualizations with the chart operator, we would love to [hear from\\nyou](/discord).\\n\\n:::info Coming Soon: Dashboards\\nThe `chart` operator is a first step towards having dashboards directly in\\nTenzir. Any result that you see in the Explorer you will soon be able to pin and\\nfreely arrange on a customizable dashboard.\\n:::\\n\\n## Bloom Filter Context\\n\\nThe new [`bloom-filter`](/next/contexts/bloom-filter) context makes it possible\\nto use large datasets for enrichment. It uses a [Bloom\\nfilter](https://en.wikipedia.org/wiki/Bloom_filter) to store sets in a compact\\nway, at the cost of potential false positives when looking up an item.\\n\\nIf you have massive amounts of indicators or a large amount of things you would\\nlike to contextualize, this feature is for you.\\n\\nCreate a Bloom filter context by using `bloom-filter` as context type:\\n\\n```\\ncontext create indicators bloom-filter\\n```\\n\\nThen populate it with a pipeline, exactly like a [lookup\\ntable](/next/contexts/lookup-table):\\n\\n```\\nfrom /tmp/iocs.csv\\n| context update bloom-filter --key ioc\\n```\\n\\nThereafter use it for enrichment, e.g., in this example pipeline:\\n\\n```\\nexport --live\\n| where #schema == \\"suricata.dns\\"\\n| enrich indicators --field dns.rrname\\n```\\n\\nThe `enrich` operator gained a new `--filter` option to remove events it could\\nnot enrich. Use the new option to remove anything that is not included in the\\nBloom filter:\\n\\n```\\nexport --live\\n| where #schema == \\"suricata.dns\\"\\n| enrich indicators --field dns.rrname --filter\\n```\\n\\n## Housekeeping\\n\\nOther noteworthy changes and improvements:\\n- `tenzir.db-directory` is now `tenzir.state-directory`. The old option remains\\n  functional, but will be phased out in an upcoming release.\\n- On the command-line, Tenzir now respects [`NO_COLOR`](https://no-color.org)\\n  when printing diagnostics. Additionally, colors are automatically disabled\\n  when the output device is not a terminal.\\n- RFC 5424-style Syslog parsing now emits a record with the structured data\\n  fields.\\n- The `--selector` option for the JSON parser now works with nested and\\n  non-string fields.\\n- The `python` operator gained a `--file` option to read from a file instead of\\n  expecting the Python code as a positional argument.\\n- The `csv`, `tsv`, and `ssv` parsers now fill in nulls for missing values.\\n\\nFor the curious, [the changelog](/changelog#v490) has the full scoop.\\n\\nExperience the new features at [app.tenzir.com](https://app.tenzir.com) and join\\nus on [our Discord server](/discord)."},{"id":"/introducing-office-hours","metadata":{"permalink":"/blog/introducing-office-hours","source":"@site/blog/introducing-office-hours/index.md","title":"Introducing Office Hours","description":"Did you ever want to get a sneak peek behind the scenes at Tenzir? Now you can!","date":"2024-02-13T00:00:00.000Z","formattedDate":"February 13, 2024","tags":[{"label":"discord","permalink":"/blog/tags/discord"},{"label":"office-hours","permalink":"/blog/tags/office-hours"}],"readingTime":1.615,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Introducing Office Hours","authors":["dominiklohmann"],"date":"2024-02-13T00:00:00.000Z","tags":["discord","office-hours"],"comments":true},"prevItem":{"title":"Tenzir v4.9","permalink":"/blog/tenzir-v4.9"},"nextItem":{"title":"Tenzir v4.8","permalink":"/blog/tenzir-v4.8"}},"content":"Did you ever want to get a sneak peek behind the scenes at Tenzir? Now you can!\\n\\n[![Office Hours](office-hours.excalidraw.svg)](https://discord.gg/JGS2N2Dwf6?event=1202543209662906393)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What are Office Hours?\\n\\nStarting immediately, our team will host a dedicated Office Hours event every\\nweek on our Discord server. This is an open-door event where every one of you is\\nwelcome, no matter what your level of experience or expertise.\\n\\n## What should you expect from this?\\n\\nOffice Hours is an exclusive space where you bring up your questions, thoughts,\\nsuggestions, or just pop in to say hi. From giving insights about our operations\\nover discussing the latest updates to providing exclusive previews of upcoming\\nfeatures, we will ensure that each session is loaded up with useful information\\nfor you. It\'s also an excellent opportunity for you to get a sneak peek behind\\nthe scenes, know us better, and see how we make the magic happen!\\n\\n## The topics we\'ll cover\\n\\nWe encourage you to bring any kind of questions to the table. Whether it\'s about\\nour work, our team, our future plans, or whatever else comes to your mind. While\\nwe want these hours to be helpful, we also want to make it a space where we can\\nshare, learn, and grow together as a community.\\n\\n## How you can participate\\n\\nGetting involved is simple:\\n\\n1. If you\'re not already a part of our Discord server, [join\\n   us](https://tenzir.com/discord)!\\n2. We\'ll announce the hours before they are starting every second Tuesday at 8\\n   AM EST / 11 AM EST / 5 PM CET / 9.30 PM IST.\\n3. During the event, head over to the dedicated Office Hours voice channel on\\n   our Discord server to join.\\n\\nEt voil\xe0! You are all set.\\n\\nOffice Hours are designed to welcome all voices, experiences, and perspectives.\\nWe look forward to vibrant, engaging, informative sessions! Whether you want to\\nparticipate actively or just listen in, we\'re looking forward to seeing you\\nthere."},{"id":"/tenzir-v4.8","metadata":{"permalink":"/blog/tenzir-v4.8","source":"@site/blog/tenzir-v4.8/index.md","title":"Tenzir v4.8","description":"Hot off the press: [Tenzir","date":"2024-01-22T00:00:00.000Z","formattedDate":"January 22, 2024","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"fluent-bit","permalink":"/blog/tags/fluent-bit"},{"label":"http","permalink":"/blog/tags/http"},{"label":"context","permalink":"/blog/tags/context"},{"label":"lookup","permalink":"/blog/tags/lookup"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":2.83,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"},{"name":"Jannis Christopher K\xf6hl","title":"Software Engineer","url":"https://github.com/jachris","email":"jannis@tenzir.com","imageURL":"https://github.com/jachris.png","key":"jachris"}],"frontMatter":{"title":"Tenzir v4.8","authors":["dominiklohmann","jachris"],"date":"2024-01-22T00:00:00.000Z","tags":["release","fluent-bit","http","context","lookup","performance"],"comments":true},"prevItem":{"title":"Introducing Office Hours","permalink":"/blog/introducing-office-hours"},"nextItem":{"title":"Switching Fluent Bit from JSON to MsgPack","permalink":"/blog/switching-fluentbit-from-json-to-msgpack"}},"content":"Hot off the press: [Tenzir\\nv4.8](https://github.com/tenzir/tenzir/releases/tag/v4.8.0). This release is\\nfilled with goodness.\\n\\n![lookup](lookup.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Lookup Operator\\n\\nThe new [`lookup`](/next/operators/lookup) operator is a unique vehicle to\\nperform live- and retro-matching simultaneously. Think of it as enrichment of\\nall data that gets ingested into a node, plus a historical query for every\\nchange in the enrichment context.\\n\\n## Graylog Support\\n\\nThe new [`gelf`](/next/formats/gelf) parser makes it possible to read a stream\\nof [Graylog Extended Log Format\\n(GELF)](https://go2docs.graylog.org/5-0/getting_in_log_data/gelf.html) messages.\\n\\nYou can now point your GELF feed to a Tenzir pipeline. Read our [Graylog\\nintegration page](/next/integrations/graylog) for the details. The TL;DR is:\\n\\n```\\nfrom tcp://0.0.0.0:12201 read gelf\\n| import\\n```\\n\\n## Shift Timestamps and Delay Events\\n\\nThe new [`timeshift`](/next/operators/timeshift) and\\n[`delay`](/next/operators/timeshift) operators make it possible to rewrite\\ntimestamps and act on them to replay data flexibly.\\n\\nThe `timeshift` operator adjusts a series of time values by anchoring them\\naround a given start time. You can rewrite and scale timestamps:\\n\\n![timeshift](timeshift.excalidraw.svg)\\n\\nFor example, use `timeshift` to re-align our Zeek example dataset to January 1,\\n1984, and make the trace 100x slower:\\n\\n```\\nfrom https://storage.googleapis.com/tenzir-datasets/M57/zeek-all.log.zst read zeek-tsv\\n| timeshift --start 1984-01-01 --speed 0.01 ts\\n```\\n\\nWhile `timeshift` rewrites timestamps, `delay` acts on them by yielding events\\naccording to a given time field. Delaying events comes in handy when replaying a\\ntrace or logs. Delaying means effectively introducing sleeping periods\\nproportional to the inter-arrival times of the events. As with `timeshift`, you\\ncan scale the behavior with a multiplicative constant to speed things up.\\n\\nHere is visual explanation of how `delay` works:\\n\\n![delay](delay.excalidraw.svg)\\n\\n## HTTP Saver\\n\\nThe [`http`](/next/connectors/http) connector now also has a saver in addition\\nto the already existing loader. Here\'s how they work in a nutshell:\\n\\n![HTTP Connector](http.excalidraw.svg)\\n\\nFor the loader, you specify the request body and the response body is input for\\nthe pipeline. For the saver, the pipeline contents determine the request body\\nand the response body isn\'t processed.\\n\\n## Fluent Bit Performance\\n\\nThe [`fluent-bit`](/operators/fluent-bit) source operator got a significant\\nperformance boost as a byproduct of changing the Fluent Bit data exchange format\\nfrom JSON to MsgPack:\\n\\n![Fluent Bit Performance](fluent-bit-speedup.svg)\\n\\nRead the [dedicated blog post on this\\nissue](/blog/switching-fluentbit-from-json-to-msgpack).\\n\\nThanks to Christoph Lobmeyer and Yannik Meinhardt for reporting this issue! \ud83d\ude4f\\n\\n## Improved Pipeline State Persistence\\n\\nWe\'ve improved the [state management of\\npipelines](/next/user-guides/manage-a-pipeline) when nodes restart or crash.\\nRecall the state machine of a pipeline:\\n\\n![Pipeline States](pipeline-states.excalidraw.svg)\\n\\nThe gray buttons on the state transition arrows correspond to actions you can\\ntake.\\n\\nHere\'s what changed on node restart and/or crash:\\n\\n- Running pipelines remain in *Running* state. Previously, the node stopped all\\n  running pipelines when shutting down. The unexpected behavior was that a\\n  restart of a node didn\'t automatically resume previously running pipelines.\\n  This is now the case.\\n- Paused pipelines transition to the *Stopped* state. The difference between\\n  *Paused* and *Stopped* is that paused pipelines can be quickly resumed without\\n  losing in-memory state. Stopping a pipeline fully evicts it. A node restart\\n  necessarily evicts the state of a pipeline, hence the transition from *Paused*\\n  to *Stopped*. Previously, paused pipelines were considered *Failed* after a\\n  node restart.\\n\\n## Here & There\\n\\nLots of smaller bug fixes landed in this release. We urge everyone to upgrade.\\nIf you\'re curious, [our changelog](/changelog#v480) has the full list of\\nchanges.\\n\\nVisit [app.tenzir.com](https://app.tenzir.com) to try the new\\nfeatures and swing by [our Discord server](/discord) to get help and talk about\\nyour use cases."},{"id":"/switching-fluentbit-from-json-to-msgpack","metadata":{"permalink":"/blog/switching-fluentbit-from-json-to-msgpack","source":"@site/blog/switching-fluentbit-from-json-to-msgpack/index.md","title":"Switching Fluent Bit from JSON to MsgPack","description":"We re-wired Tenzir\'s fluent-bit operator and","date":"2024-01-10T00:00:00.000Z","formattedDate":"January 10, 2024","tags":[{"label":"fluent-bit","permalink":"/blog/tags/fluent-bit"},{"label":"json","permalink":"/blog/tags/json"},{"label":"msgpack","permalink":"/blog/tags/msgpack"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":2.54,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Switching Fluent Bit from JSON to MsgPack","authors":["mavam"],"date":"2024-01-10T00:00:00.000Z","tags":["fluent-bit","json","msgpack","performance"],"comments":true},"prevItem":{"title":"Tenzir v4.8","permalink":"/blog/tenzir-v4.8"},"nextItem":{"title":"Tenzir v4.7","permalink":"/blog/tenzir-v4.7"}},"content":"We re-wired Tenzir\'s [`fluent-bit`](/operators/fluent-bit) operator and\\nintroduced a significant performance boost as a side effect: A 3\u20135x gain for\\nthroughput in events per second (EPS) and 4\u20138x improvement of latency in terms\\nof processing time.\\n\\n![Fluent Bit Speedup](fluent-bit-speedup.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nWhy were these gains available? Because we eliminated one round-trip of internal\\nJSON printing and parsing.\\n\\n## The Issue\\n\\nOur primary goal was actually working around an issue\\nwith the Fluent Bit `lib` output plugin, which we use whenever we have a\\n`fluent-bit` source operator. For example, if you configure an `elasticsearch`\\nFluent Bit source, Tenzir\'s `fluent-bit` operator autocomplete the `lib` output\\nplugin. This plugin has two modes of accepting input: JSON or\\n[MsgPack](https://msgpack.org/).\\n\\nUp to now, we relied on the JSON transfer mode because it was faster to get\\nstarted. However, during testing with the `elasticsearch` input that receives\\nlarge Windows event logs via Winlogbeat, we noticed that Fluent Bit\'s `lib`\\noutput produces messages of the form `[timestamp, object]` where `object` was\\ncropped. This basically generated invalid JSON.\\n\\nThe fix involved switching the exchange format of the `lib` output plugin from\\nJSON to MsgPack. If you\'re curious, take a look at\\n[#3770](https://github.com/tenzir/tenzir/pull/3770) for the full scoop. The\\nimprovement is already in the current development version and will be available\\nwith the next release.\\n\\n## Evaluation\\n\\nWe were curious how much this removal of the extra layer of printing and parsing\\nactually buys us. To this end, we use the following pipeline:\\n\\n```bash\\ntenzir --dump-metrics \'fluent-bit stdin | head 10M | discard\' < eve.json\\n```\\n\\nAdding `--dump-metrics` adds detailed per-operator metrics that help us\\nunderstand where operators spend their time. The [`head`](/operators/head)\\noperator take the first 10 million events, and [`discard`](/operators/discard)\\nsimply drops its input. The `eve.json` input into the `tenzir` binary is from\\nour Suricata dataset that we use in the [user guides](/user-guides). We measured\\nran our measurements on a 2021 Apple MacBook Pro M1 Max, as well as on a Manjaro\\nLinux laptop with a 14-core Intel i7 CPU.\\n\\nOur intuition was that we won\'t see major improvements, because generating JSON\\nisn\'t that expensive and we use [simdjson](https://simdjson.org/) to parse JSON.\\nBut the results surprised us:\\n\\n![Fluent Bit Performance](fluent-bit-performance.svg)\\n\\nOn macOS, events per second tripled from 50k to 150k, and the pipeline runtime\\nwent from 42 to 10 seconds. On Linux, the improvements were even higher. We\\ndon\'t have a good explanation for the rather stark difference between the\\noperating systems. Our hunch is that the allocator performance is the high-order\\nbit explaining the difference.\\n\\n## Summary\\n\\nWe switched from JSON to MsgPack for our [`fluent-bit`](/operators/fluent-bit)\\nsource operator. This removed one round-trip of printing JSON (in Fluent Bit)\\nand parsing JSON (in Tenzir). We were surprised to see that this change resulted\\nin such substantial performance improvements. As a result, you can now run many\\nmore Fluent Bit ingestion pipelines in parallel at a single node with the same\\nresources, or vertically scale your Fluent Bit pipeline to new limits.\\n\\n:::tip Acknowledgements\\nThanks to Christoph Lobmeyer and Yannik Meinhardt for reporting this issue! \ud83d\ude4f\\n:::"},{"id":"/tenzir-v4.7","metadata":{"permalink":"/blog/tenzir-v4.7","source":"@site/blog/tenzir-v4.7/index.md","title":"Tenzir v4.7","description":"Tenzir v4.7 brings a new","date":"2023-12-19T00:00:00.000Z","formattedDate":"December 19, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"grok","permalink":"/blog/tags/grok"},{"label":"kv","permalink":"/blog/tags/kv"},{"label":"geoip","permalink":"/blog/tags/geoip"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.54,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.7","authors":["dominiklohmann"],"date":"2023-12-19T00:00:00.000Z","tags":["release","grok","kv","geoip","performance"],"comments":true},"prevItem":{"title":"Switching Fluent Bit from JSON to MsgPack","permalink":"/blog/switching-fluentbit-from-json-to-msgpack"},"nextItem":{"title":"Contextualization Made Simple","permalink":"/blog/contextualization-made-simple"}},"content":"[Tenzir v4.7](https://github.com/tenzir/tenzir/releases/tag/v4.7.0) brings a new\\ncontext type, two parsers, four new operators, improvements to existing parsers,\\nand a sizable under-the-hood performance improvement.\\n\\n![Tenzir v4.7](tenzir-v4.7.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Enrich with the GeoIP context\\n\\nUse the [`geoip`](/next/contexts/geoip) context to enrich events with\\ninformation from a MaxMind GeoIP\xae database.\\n\\nTo get started, [download the freely available GeoLite2 MaxMind\\ndatabase](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data), or use\\nany other MaxMind database. We\'ll use the country database file\\n`GeoLite2-Country.mmdb`.\\n\\n```text {0} title=\\"Create a \'geoip\' context named \'country\'\\"\\ncontext create country geoip --db-path /path/to/GeoLite2-Country.mmdb\\n```\\n\\n```text {0} title=\\"Enrich Suricata events with the \'country\' context\\"\\nexport\\n| where #schema == /suricata.*/\\n/* Apply the context to both source and destination IP address fields */\\n| enrich src_country=country --field src_ip\\n| enrich dest_country=country --field dest_ip\\n/* Use just the country\'s isocode, and discard the rest of the information */\\n| replace src_country=src_country.context.country.iso_code,\\n          dest_country=dest_country.context.country.iso_code\\n```\\n\\n```json {0} title=\\"Possible output\\"\\n{\\n  \\"timestamp\\": \\"2021-11-17T14:02:38.165570\\",\\n  \\"flow_id\\": 1837021175481117,\\n  \\"pcap_cnt\\": 357,\\n  \\"vlan\\": null,\\n  \\"in_iface\\": null,\\n  \\"src_ip\\": \\"45.137.23.27\\",\\n  \\"src_port\\": 47958,\\n  \\"dest_ip\\": \\"198.71.247.91\\",\\n  \\"dest_port\\": 53,\\n  \\"proto\\": \\"UDP\\",\\n  \\"event_type\\": \\"dns\\",\\n  \\"community_id\\": \\"1:0nZC/6S/pr+IceCZ04RjDZbX+KI=\\",\\n  \\"dns\\": {\\n    // ...\\n  },\\n  \\"src_country\\": \\"NL\\",\\n  \\"dest_country\\": \\"US\\"\\n}\\n```\\n\\nThe [`geoip`](/next/contexts/geoip) context is a powerful building block for\\nin-band enrichments. Besides country codes and country names you can add region\\ncodes, region names, cities, zip codes, and geographic coordinates. With the\\nflexibility of the [contextualization framework](/next/contexts) this\\ninformation you can now get this information in real-time.\\n\\n:::info Follow our Blog Post Series\\nRead more about contexts in our blog post series:\\n1. [Enrichment Complexity in the Wild](/blog/enrichment-complexity-in-the-wild)\\n2. [Contextualization Made Simple](/blog/contextualization-made-simple)\\n:::\\n\\n## Grok and KV Parsers\\n\\nThe [`kv`](/next/formats/kv) and [`grok`](/next/formats/grok) parsers combine\\nwell with the [`parse`](/next/operators/parse) operator introduced with Tenzir\\nv4.6. The former reads key-value pairs by splitting strings based on regular\\nexpressions, and the latter uses a parser modeled after the [Logstash\\n`grok` plugin][logstash-grok] in Elasticsearch.\\n\\n[logstash-grok]: https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html\\n\\nParse a fictional HTTP request log with `grok`:\\n\\n```json {0} title=\\"Example input\\"\\n{\\n  \\"message\\": \\"55.3.244.1 GET /index.html 15824 0.043\\"\\n}\\n```\\n\\n```text {0} title=\\"Parse with grok\\"\\nparse message grok \\"%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}\\"\\n```\\n\\n```json {0} title=\\"Example output\\"\\n{\\n  \\"message\\": {\\n    \\"client\\": \\"55.3.244.1\\",\\n    \\"method\\": \\"GET\\",\\n    \\"request\\": \\"/index.html\\",\\n    \\"bytes\\": 15824,\\n    \\"duration\\": 0.043\\n  }\\n}\\n```\\n\\nExtract space-separated `key=value` pairs with `kv`:\\n\\n```json {0} title=\\"Example input\\"\\n{\\n  \\"message\\": \\"foo=1 bar=2 baz=3 qux=4\\"\\n}\\n```\\n\\n```text {0} title=\\"Parse with kv\\"\\nparse message kv \\"\\\\s+\\" \\"=\\"\\n```\\n\\n```json {0} title=\\"Example output\\"\\n{\\n  \\"message\\": {\\n    \\"foo\\": 1,\\n    \\"bar\\": 2,\\n    \\"baz\\": 3,\\n    \\"qux\\": 4\\n  }\\n}\\n```\\n\\n## Slice and Dice Events\\n\\nThe [`slice`](/next/operators/slice) operator is a more powerful version of the\\n`head` and `tail` operators. It allows for selecting a contiguous range of\\nevents given a half-closed interval.\\n\\n```text {0} title=\\"Get the second 100 events\\"\\nslice --begin 100 --end 200\\n```\\n\\nNegative values for the interval count from the end rather than from the start:\\n\\n```text {0} title=\\"Get the last 5 events\\"\\nslice --begin -5\\n```\\n\\nPositive and negative values can also be combined:\\n\\n```text {0} title=\\"Get everything but the first 10 and the last 10 events\\"\\nslice --begin 10 --end -10\\n```\\n\\n## Lightweight Endpoint Snapshot\\n\\nUse the [`processes`](/next/operators/processes),\\n[`sockets`](/next/operators/sockets), and [`nics`](/next/operators/nics) sources\\nto get a snapshot of running processes, sockets, and available network\\ninterfaces, respectively.\\n\\n```text {0} title=\\"Top three running processes by name\\"\\nprocesses\\n| top name\\n| head 3\\n```\\n\\n```json {0} title=\\"Possible output\\"\\n{\\n  \\"name\\": \\"MTLCompilerService\\",\\n  \\"count\\": 24\\n}\\n{\\n  \\"name\\": \\"zsh\\",\\n  \\"count\\": 16\\n}\\n{\\n  \\"name\\": \\"VTDecoderXPCServ\\",\\n  \\"count\\": 9\\n}\\n```\\n\\n## Performance Improvements\\n\\nWe\'ve fixed a long-standing bug in Tenzir\'s pipeline execution engine that\\nimprove performance for some operators:\\n\\n1. Operators and loaders that interface with blocking third-party APIs sometimes\\n   delayed partial results until the next partial result arrived through the\\n   blocking API. This bug affected the `tcp`, `zmq`, `kafka`, and `nic` loaders\\n   and the `shell`, `fluent-bit`, `velociraptor`, and `python` operators. These\\n   loaders and operators are now generally more responsive.\\n2. The time-to-first-result for pipelines with many operators is now shorter,\\n   and the first result no longer takes an additional 20ms per operator in the\\n   pipeline to arrive.\\n\\n## Want More?\\n\\nWe provide a full list of changes [in our changelog](/changelog#v470).\\n\\nHead over to [app.tenzir.com](https://app.tenzir.com) to play with the new\\nfeatures and join [our Discord server](/discord)\u2014the perfect place to ask\\nquestions, chat with Tenzir users and developers, and to discuss your feature\\nideas!"},{"id":"/contextualization-made-simple","metadata":{"permalink":"/blog/contextualization-made-simple","source":"@site/blog/contextualization-made-simple/index.md","title":"Contextualization Made Simple","description":"How would you create a contextualization engine? What are the essential building","date":"2023-12-07T00:00:00.000Z","formattedDate":"December 7, 2023","tags":[{"label":"context","permalink":"/blog/tags/context"},{"label":"enrich","permalink":"/blog/tags/enrich"},{"label":"node","permalink":"/blog/tags/node"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"suricata","permalink":"/blog/tags/suricata"},{"label":"threat-intel","permalink":"/blog/tags/threat-intel"},{"label":"iocs","permalink":"/blog/tags/iocs"}],"readingTime":8.955,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Contextualization Made Simple","authors":["mavam"],"date":"2023-12-07T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["context","enrich","node","pipelines","suricata","threat-intel","iocs"],"comments":true},"prevItem":{"title":"Tenzir v4.7","permalink":"/blog/tenzir-v4.7"},"nextItem":{"title":"Tenzir v4.6","permalink":"/blog/tenzir-v4.6"}},"content":"How would you create a contextualization engine? What are the essential building\\nblocks? We asked ourselves these questions after studying what\'s out there and\\nbuilt from scratch a high-performance contextualization framework in Tenzir.\\nThis blog post introduces this brand-new framework, provides usage examples, and\\ndescribes how you can build your own context plugin.\\n\\n![Contextualization](contextualization.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\nThis is the second post of the our contextualization series. If you haven\'t read\\nthe first post, go check it out and learn [how Splunk, Elastic, and Sentinel\\nsupport contextualization](/blog/enrichment-complexity-in-the-wild).\\n\\n## Requirements\\n\\nAfter studying how others tackle the enrichment use case and talking to numerous\\npractitioners in the SecOps community, we went to the drawing board to identify\\nwhat we really need.\\n\\n1. **Dynamic context state updates**. In security, we\'re especially interested\\n   in use cases where the enrichment context is dynamic and changes over time.\\n   For example, the threat landscape is often represented in the form of\\n   observables, IOCs, or TTPs. Their utility quickly decays over time. Many\\n   indicators are only useful for a couple of days, as attacker infrastructure\\n   can be ephemeral and change rapidly. As a result, we need the ability to\\n   change our context state to keep a useful representation.\\n\\n2. **Decoupled context management and use**. Conceptually, a context has a write\\n   path to update its state, and a read path access its state. These two paths\\n   operate independently and its the job of the context to coordinate access to\\n   its shared state so that reads and writes do not conflict.\\n\\n3. **Flexible notions of context type**. Most systems out there treat enrichment\\n   as a join that brings two tables together. But what about Bloom filters? And\\n   ML model inference? What about API calls? Or custom libraries that shield a\\n   context? We\'re not always enriching with just a table, but many other types\\n   of context. Hence we need a dedicated abstraction what constitutes a context.\\n\\nA corollary of (3) is that we would like to support various *lookup modes*:\\n\\n![Contextualization Modes](contextualization-modes.excalidraw.svg)\\n\\nWe define these lookup modes as follows:\\n\\n1. **In-band**. The context data is co-located with the dataflow where it should\\n   act upon. This is especially important for high-velocity dataflows where\\n   there\'s a small time budget to perform an enrichment. For example, we\'ve seen\\n   network monitors like [Zeek](https://zeek.org) and\\n   [Suricata](https://suricata.io) links produce structured logs at 250k EPS,\\n   which would mean that enrichment cannot take more than 4 microseconds\\n   per event.\\n2. **Out-of-band**. The context data is far away from the to-be-contextualized\\n   dataflow. We encounter this mode when the context is intellectual property,\\n   when the context state is massive and maintenance is complex, or when it\'s\\n   created on-the-fly based on request by a service. A REST API is the most\\n   common example.\\n3. **Hybrid**. When both performance matters and state is not possible to ship\\n   to the contextualization point itself, then a hybrid approach can be a viable\\n   middle ground. [Google Safe Browsing][safebrowsing] is an example of this\\n   kind, where the Chrome browser keeps a subset of context state that\\n   represents threat actor URLs in the form of partial hashes, and when a users\\n   visits a URL where a partial match occurs, Chrome performs a candidate check\\n   using an API call. More than 99% of checked URLs never make it to the remote\\n   API, making this approach scalable. Note that the extra layer of hashing also\\n   protects the privacy of the entity performing the context lookup.\\n\\n[safebrowsing]: https://security.googleblog.com/2022/08/how-hash-based-safe-browsing-works-in.html\\n\\n## The Tenzir Contextualization Framework\\n\\nAs principled engineers, we took those requirements to the drawing board and\\nbuilt a solution that meets all of them. Two foundations of the Tenzir\\narchitecture made it possible to arrive at an elegant solution that results in a\\nsimple-yet-powerful user experience: (1) a pipeline-based data flow model, and\\n(2) the ability to manage state at continuously running Tenzir nodes. Let\'s walk\\nthrough a typical use case that explains the building blocks.\\n\\n:::info Example Scenario\\nDuring a compromise assessment, the security engineer Pada Wan is tasked with\\nfinding out whether the constituency initiates any connections to known\\ncommand-and-control servers. Pada takes the\\n[ThreatFox](https://threatfox.abuse.ch/) OSINT feed, a community malware where\\npractioners can share IOCs containing IPs, domains, URLs, and hashes. Pada\'s\\norganization uses Suricata to monitor their networks and now wants to leverage\\nDNS logs to identify possible lookups to known attacker infrastructure.\\n\\nPada, follwing first principles, remembers: *\\"Through a pipeline strong and wise,\\nsafe the constituency will stay, hmm.\\"*\\n:::\\n\\n### Create a context\\n\\nFirst create a **context** in a Tenzir node by running the following pipeline:\\n\\n```\\ncontext create threatfox lookup-table\\n```\\n\\nThis yields the following output:\\n\\n```json\\n{\\n  \\"num_entries\\": 0,\\n  \\"name\\": \\"threatfox\\"\\n}\\n```\\n\\nThe `context` operator manages context instances. It takes a context name and\\ntype as positional arguments. The `lookup-table` type is a key-value mapping\\nwhere a key is used to perform the context lookup and the value can be any\\nstructured additional data.\\n\\n### Load data into the context\\n\\nNext we fill the context with the contents of the ThreatFox feed. Here\'s the how\\nwe query the API with a HTTP POST request:\\n\\n```\\nfrom https://threatfox-api.abuse.ch/api/v1/ query=get_iocs days:=1\\n```\\n\\nThe response looks as follows:\\n\\n```json\\n{\\n  \\"query_status\\": \\"ok\\",\\n  \\"data\\": [\\n    {\\n      \\"id\\": \\"1209500\\",\\n      \\"ioc\\": \\"8.219.229.99:4433\\",\\n      \\"threat_type\\": \\"botnet_cc\\",\\n      \\"threat_type_desc\\": \\"Indicator that identifies a botnet command&control server (C&C)\\",\\n      \\"ioc_type\\": \\"ip:port\\",\\n      \\"ioc_type_desc\\": \\"ip:port combination that is used for botnet Command&control (C&C)\\",\\n      \\"malware\\": \\"win.cobalt_strike\\",\\n      \\"malware_printable\\": \\"Cobalt Strike\\",\\n      \\"malware_alias\\": \\"Agentemis,BEACON,CobaltStrike,cobeacon\\",\\n      \\"malware_malpedia\\": \\"https://malpedia.caad.fkie.fraunhofer.de/details/win.cobalt_strike\\",\\n      \\"confidence_level\\": 80,\\n      \\"first_seen\\": \\"2023-12-04 16:00:16 UTC\\",\\n      \\"last_seen\\": null,\\n      \\"reference\\": null,\\n      \\"reporter\\": \\"malpulse\\",\\n      \\"tags\\": null\\n    },\\n    {\\n    ..\\n    },\\n    {\\n    ..\\n    }\\n  ]\\n}\\n```\\n\\nUnfortunately the data is not yet in right shape yet. We need one IOC event per\\nlookup table entry, but the above is one giant event with all IOCs in the nested\\n`data` array. We can get to the desired shape with the\\n[`yield`](/next/operators/yield) operator hoists the array elements\\ninto top-level events. Let\'s take a look at one of the events:\\n\\n```\\nfrom https://threatfox-api.abuse.ch/api/v1/ query=get_iocs days:=1\\n| yield data[]\\n| head 1\\n```\\n\\n```json\\n{\\n  \\"id\\": \\"1209500\\",\\n  \\"ioc\\": \\"8.219.229.99:4433\\",\\n  \\"threat_type\\": \\"botnet_cc\\",\\n  \\"threat_type_desc\\": \\"Indicator that identifies a botnet command&control server (C&C)\\",\\n  \\"ioc_type\\": \\"ip:port\\",\\n  \\"ioc_type_desc\\": \\"ip:port combination that is used for botnet Command&control (C&C)\\",\\n  \\"malware\\": \\"win.cobalt_strike\\",\\n  \\"malware_printable\\": \\"Cobalt Strike\\",\\n  \\"malware_alias\\": \\"Agentemis,BEACON,CobaltStrike,cobeacon\\",\\n  \\"malware_malpedia\\": \\"https://malpedia.caad.fkie.fraunhofer.de/details/win.cobalt_strike\\",\\n  \\"confidence_level\\": 80,\\n  \\"first_seen\\": \\"2023-12-04 16:00:16 UTC\\",\\n  \\"last_seen\\": null,\\n  \\"reference\\": null,\\n  \\"reporter\\": \\"malpulse\\",\\n  \\"tags\\": null\\n}\\n```\\n\\nYes, this is something the `context` update can work with. Now that the data is\\nin the right shape, all we need is piping it to `context update`:\\n\\n```\\nfrom https://threatfox-api.abuse.ch/api/v1/ query=get_iocs days:=1\\n| yield data[]\\n| where ioc_type == \\"domain\\"\\n| context update threatfox --key ioc\\n```\\n\\nThis outputs:\\n\\n```json\\n{\\n  \\"num_entries\\": 57,\\n  \\"name\\": \\"threatfox\\"\\n}\\n```\\n\\nThat is, 57 entries have been added successfully to the `threatfox` context.\\n\\n### Enrich with the context\\n\\nWe\'ve now loaded the context and can use it in other pipelines. As we\'re in a\\ncompromise assessment as example, we\'re interested in a realtime view of the\\nnetwork traffic. So we\'d like to hook the feed of all flow logs streaming into a\\nTenzir node. Let\'s say we have a Suricata `eve.json` file that we follow\\ncontinuously and import into a running node:\\n\\n```\\nfrom file --follow /suricata/eve.json read suricata\\n| import\\n```\\n\\nNow we hook into the DNS live feed for enrichment, keep only the matches, and\\nforward them to a Slack channel via [`fluent-bit`](/next/operators/fluent-bit):\\n\\n```\\nexport --live\\n| where #schema == \\"suricata.dns\\"\\n| enrich threatfox --field dns.rrname\\n| where threatfox.key != null\\n| fluent-bit slack webhook=IR_TEAM_SLACK_CHANNEL_URL\\n```\\n\\nIn more detail:\\n- `export --live` hooks into the import data feed at the node\\n- `where #schema == \\"suricata.dns\\"` restricts the feed to Suricata DNS events\\n- `enrich threatfox --field dns.rrname` joins the lookup table with the RR name\\n  of the DNS request\\n- `where threatfox.key != null` ignores non-matching enrichments\\n- `fluent-bit slack webhook=IR_TEAM_SLACK_CHANNEL_URL` sends the events to a\\n  Slack channel\\n\\nOne such matching enrichment may looks like this:\\n\\n```json\\n{\\n  \\"timestamp\\": \\"2021-11-17T16:57:42.389824\\",\\n  \\"flow_id\\": 1542499730911936,\\n  \\"pcap_cnt\\": 3167,\\n  \\"vlan\\": null,\\n  \\"in_iface\\": null,\\n  \\"src_ip\\": \\"45.85.90.164\\",\\n  \\"src_port\\": 56462,\\n  \\"dest_ip\\": \\"198.71.247.91\\",\\n  \\"dest_port\\": 53,\\n  \\"proto\\": \\"UDP\\",\\n  \\"event_type\\": \\"dns\\",\\n  \\"community_id\\": null,\\n  \\"dns\\": {\\n    \\"version\\": null,\\n    \\"type\\": \\"query\\",\\n    \\"id\\": 1,\\n    \\"flags\\": null,\\n    \\"qr\\": null,\\n    \\"rd\\": null,\\n    \\"ra\\": null,\\n    \\"aa\\": null,\\n    \\"tc\\": null,\\n    \\"rrname\\": \\"bza.fartit.com\\",\\n    \\"rrtype\\": \\"RRSIG\\",\\n    \\"rcode\\": null,\\n    \\"ttl\\": null,\\n    \\"tx_id\\": 0,\\n    \\"grouped\\": null,\\n    \\"answers\\": null\\n  },\\n  \\"threatfox\\": {\\n    \\"key\\": \\"bza.fartit.com\\",\\n    \\"context\\": {\\n      \\"id\\": \\"1209087\\",\\n      \\"ioc\\": \\"bza.fartit.com\\",\\n      \\"threat_type\\": \\"payload_delivery\\",\\n      \\"threat_type_desc\\": \\"Indicator that identifies a malware distribution server (payload delivery)\\",\\n      \\"ioc_type\\": \\"domain\\",\\n      \\"ioc_type_desc\\": \\"Domain name that delivers a malware payload\\",\\n      \\"malware\\": \\"apk.irata\\",\\n      \\"malware_printable\\": \\"IRATA\\",\\n      \\"malware_alias\\": null,\\n      \\"malware_malpedia\\": \\"https://malpedia.caad.fkie.fraunhofer.de/details/apk.irata\\",\\n      \\"confidence_level\\": 100,\\n      \\"first_seen\\": \\"2023-12-03 14:05:20 UTC\\",\\n      \\"last_seen\\": null,\\n      \\"reference\\": \\"\\",\\n      \\"reporter\\": \\"onecert_ir\\",\\n      \\"tags\\": [\\n        \\"irata\\"\\n      ]\\n    },\\n    \\"timestamp\\": \\"2023-12-04T13:52:49.043157\\"\\n  }\\n}\\n```\\n\\nNote the new field `threatfox` that is the context name. The `key` that matched\\nhas the value `bza.fartit.com`, which is also `dns.rrname`. There\'s also a\\n`timestamp` field when the enrichment took place, and the full data that we\\nloaded into the context under a given key.\\n\\n### Summary\\n\\nLet\'s recap what we did:\\n\\n1. Create a context via `context create` that is a lookup table.\\n2. Populate the context via `context update` with the ThreatFox OSINT feed.\\n3. Use the context via `enrich` to filter matching events.\\n4. Forward the enriched events to a Slack channel.\\n\\nThe `enrich` pipeline uses a lookup table to perform an in-band enrichment. Our\\nfirst measurements indicate that there is no noticeable performance overhead.\\n\\nWe can visualize this pipeline as follows:\\n\\n![Contextualization Example](contextualization-example.excalidraw.svg)\\n\\n## Comparison\\n\\nHow is this different to others, e.g., Splunk, Elastic, and Sentinel? If you\\ndon\'t recall how these three work, go back to our [previous blog\\npost](/blog/enrichment-complexity-in-the-wild).\\n\\n1. **Simplicity**. The core abstraction is incredibly simple\u2014an opaque context\\n   that can be used from two sides. You can simultaneously feed the context with\\n   a pipeline to update its state, and use it many other places to enrich your\\n   dataflows.\\n\\n2. **Flexibility**. The `enrich` operator gives you full control where you want\\n   to perform the contextualization. Place it before `import`, and it\'s an\\n   ingest-time enrichment. Put it after `export`, and it\'s a search-time\\n   enrichment. The abstraction is always same, regardless of the location.\\n\\n3. **Extensibility**. This blog post showed only one context type, the lookup\\n   table. This covers the most common enrichment scenario. But you can implement\\n   your own context types. A context plugin receives the full pipeline dataflow,\\n   and as a developer, you get [Apache Arrow](https://arrow.apache.org) record\\n   batches. This columnar representation works seamlessly with many data tools.\\n\\nStay tuned for more context plugins. Up next on our roadmap are three other\\nin-band context types: a Bloom filter, Sigma rules, and a\\n[MaxMind](https://github.com/maxmind/libmaxminddb)-based GeoIP context.\\n\\nYou can try all of this yourself by heading over to\\n[app.tenzir.com](https://app.tenzir.com). Deploy a cloud-based demo node and\\nenrich your life. As always, we\'re here to help and are looking forward to\\nmeeting you in our [Discord community](/discord)."},{"id":"/tenzir-v4.6","metadata":{"permalink":"/blog/tenzir-v4.6","source":"@site/blog/tenzir-v4.6/index.md","title":"Tenzir v4.6","description":"Tenzir v4.6 is here, and","date":"2023-12-01T00:00:00.000Z","formattedDate":"December 1, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"context","permalink":"/blog/tags/context"},{"label":"enrich","permalink":"/blog/tags/enrich"},{"label":"syslog","permalink":"/blog/tags/syslog"},{"label":"tcp","permalink":"/blog/tags/tcp"},{"label":"parse","permalink":"/blog/tags/parse"},{"label":"python","permalink":"/blog/tags/python"}],"readingTime":5.745,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.6","authors":["dominiklohmann"],"date":"2023-12-01T00:00:00.000Z","tags":["release","context","enrich","syslog","tcp","parse","python"],"comments":true},"prevItem":{"title":"Contextualization Made Simple","permalink":"/blog/contextualization-made-simple"},"nextItem":{"title":"Enrichment Complexity in the Wild","permalink":"/blog/enrichment-complexity-in-the-wild"}},"content":"[Tenzir v4.6](https://github.com/tenzir/tenzir/releases/tag/v4.6.0) is here, and\\nit is our biggest release yet. The headlining feature is the all-new **context**\\nfeature, powered by the `context` and `enrich` operators and the new **context\\nplugin** type.\\n\\n![Tenzir v4.6](tenzir-v4.6.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## Enrich with Contexts\\n\\nContexts enable enriching events in real-time with additional information.\\n\\nThis is best explained on an example. We\'ll use the publicly available [Feodo IP\\nBlock List](https://feodotracker.abuse.ch) to enrich events with information\\nthat have a source IP address that is on the block list.\\n\\n```text {0} title=\\"Create a \'lookup-table\' context named \'feodo\'\\"\\ncontext create feodo lookup-table\\n```\\n\\n```text {0} title=\\"Fill the \'feodo\' context with information\\"\\nfrom https://feodotracker.abuse.ch/downloads/ipblocklist_aggressive.csv read csv --allow-comments\\n| context update feodo --key dst_ip\\n```\\n\\n```text {0} title=\\"Enrich Suricata events with the \'feodo\' context\\"\\nexport\\n| where #schema == /suricata.*/\\n| enrich feodo --field dest_ip\\n```\\n\\n```json {0} title=\\"Possible output\\"\\n{\\n  \\"timestamp\\": \\"2023-11-30T14:52:57.716360+0200\\",\\n  \\"event_type\\": \\"alert\\",\\n  \\"dest_ip\\": \\"167.179.103.206\\",\\n  // ...\\n  \\"feodo\\": {\\n    \\"key\\": \\"167.179.103.206\\",\\n    \\"context\\": {\\n      \\"first_seen_utc\\": \\"2023-11-03T07:55:23\\",\\n      \\"dst_ip\\": \\"167.179.103.206\\",\\n      \\"dst_port\\": 2083,\\n      \\"c2_status\\": \\"offline\\",\\n      \\"last_online\\": \\"2023-11-03T00:00:00\\",\\n      \\"malware\\": \\"Pikabot\\"\\n    },\\n    \\"timestamp\\": \\"2023-11-30T14:58:23.172832+0200\\"\\n  }\\n}\\n```\\n\\nContexts are\u2014just like formats, connectors, and operators\u2014designed to be a\\nbuilding block of TQL. We\'re starting out with just the open-source\\n`lookup-table` context plugin, and already have plans for more context plugins\\nand other operators besides `enrich` that use context plugins in the near\\nfuture. Stay tuned!\\n\\n:::info Follow our Blog Post Series\\nWe\'re so excited about enrichments that we wrote an entire blog post series on\\nit. The [first post](/blog/enrichment-complexity-in-the-wild) is already live,\\nand sets the scene. The second post will soon follow to explain in-depth just\\nhow Tenzir makes enrichments easy. Stay tuned!\\n:::\\n\\n## Onboard Data Faster Than Ever\\n\\nWant to read CEF over syslog from TCP? Not a problem with four all-new features\\nof Tenzir v4.6\u2014are you able to spot all four?\\n\\n```\\nfrom tcp://localhost:514 read syslog\\n| parse content cef\\n```\\n\\nHere\'s an example input and output:\\n\\n```syslog {0} title=\\"Input\\"\\nNov 13 16:59:59 host123 FOO: CEF:0|FORCEPOINT|Firewall|6.6.1|0|Generic|0|deviceExternalId=Master FW node 1 dvc=10.1.1.40 dvchost=10.1.1.40 msg=log server connection established deviceFacility=Logging System rt=Jan 17 2020 08:52:10\\n```\\n\\n```json {0} title=\\"Output\\"\\n{\\n  \\"facility\\": null,\\n  \\"severity\\": null,\\n  \\"timestamp\\": \\"Nov 13 16:59:59\\",\\n  \\"hostname\\": \\"host123\\",\\n  \\"tag\\": \\"FOO\\",\\n  \\"content\\": {\\n    \\"cef_version\\": 0,\\n    \\"device_vendor\\": \\"FORCEPOINT\\",\\n    \\"device_product\\": \\"Firewall\\",\\n    \\"device_version\\": \\"6.6.1\\",\\n    \\"signature_id\\": \\"0\\",\\n    \\"name\\": \\"Generic\\",\\n    \\"severity\\": \\"0\\",\\n    \\"extension\\": {\\n      \\"deviceExternalId\\": \\"Master FW node 1\\",\\n      \\"dvc\\": \\"10.1.1.40\\",\\n      \\"dvchost\\": \\"10.1.1.40\\",\\n      \\"msg\\": \\"log server connection established\\",\\n      \\"deviceFacility\\": \\"Logging System\\",\\n      \\"rt\\": \\"Jan 17 2020 08:52:10\\"\\n    }\\n  }\\n}\\n```\\n\\n### URLs and Paths\\n\\n`from`, `load`, `to` and `save` now support working with URLs and paths\\ndirectly, and no longer require specifying a connector and format explicitly.\\nAdditionally, they now support automatic compression and decompression.\\n\\nFor example, this pipeline reads events from a Zeek TSV log file in the local\\nfile system, and stores them as Zstd-compressed CSV file in an S3 bucket:\\n\\n![URL Expansion](tenzir-v4.6-url-expansion.excalidraw.svg)\\n\\n### TCP Connector\\n\\nAcquire data over TCP (or TLS) directly with the new `tcp` loader. Use the\\n`--tls` option to read from TLS instead, and `--listen` to open a server rather\\nthan connect as a client.\\n\\n### Syslog Format\\n\\nRead syslog RFC 3164 and RFC 5424 with the new `syslog` parser. The parser\\nautomatically disambiguates between the two common syslog standards.\\n\\n### Parse Operator\\n\\nUse the `parse` operator to structurally decompose fields with any parser. This\\nenables parsing of structured data embedded as strings inside another format.\\n\\nOnboarding custom data sources is a pain for every SOC operations team. We\'ve\\nseen CSV where some columns are NDJSON, CEF in syslog, and grok patterns in CEF\\nin syslog. With the `parse` operator, this no longer has to be as painful, and\\nyou can finally spend more time working with your data than onboarding it.\\n\\n## Rapid Prototyping with the Python Operator\\n\\nThe `python` operator allows for modifying events using Python. Here are some\\ncool things that we\'ve done in the first days of playing with the operator:\\n\\n```text {0} title=\\"Calculate the square root of a field\\"\\npython \'\\n  import math\\n  self.sq_x = math.sqrt(self.x)\\n\'\\n```\\n\\n```text {0} title=\\"Add a duration field to Suricata flow events\\"\\nwhere #schema == \\"suricata.flow\\"\\n| python \'self.flow.duration = self.flow.end - self.flow.start\'\\n```\\n\\n```text {0} title=\\"Parse a URL into components\\"\\nwhere #schema == \\"suricata.flow\\"\\n| python \'\\n  import urllib\\n  from collections import namedtuple\\n  self.http.url = f\\"http://{self.http.hostname}{self.http.url}\\"\\n  self.http.parsed = urllib.parse.urlsplit(self.http.url)._asdict()\\n  self.http.parsed.qs = urllib.parse.parse_qs(self.http.parsed.query)\\n\'\\n```\\n\\n:::caution Renamed Python Package\\nAs part of this release, we completely remodeled our Python package and renamed\\nit from `pytenzir` to `tenzir`. The old package continues to work, but is\\ndeprecated and no longer maintained.\\n:::\\n\\n## Long-Poll Support for Serve\\n\\nThe `/serve` endpoint, which allows for fetching events from a REST API for\\npipelines with the `serve` sink operator, now supports long-polling.\\n\\nPreviously, the endpoint had `timeout` and `max_events` parameters. The latter\\ndefines how many events the response may contain at most. The former defines an\\nupper bound for the duration that the endpoint waits before it returns less than\\nthe desired number of events.\\n\\nIn addition, the endpoint now supports a `min_events` parameter, which makes it\\nreturn eagerly as soon as at least the specified number of events arrived at the\\nsink of the pipeline. Setting a low value for the minimum number of events in\\ncombination with a high timeout effectively enables long-polling.\\n\\nPipelines run in the Explorer on [app.tenzir.com](https://app.tenzir.com) use an\\nimplicit `serve` sink to transport events to the results table. For slow-running\\npipelines with few results we found long-polling to improve responsiveness of\\nthe Explorer noticeably, as first results will be displayed much earlier now.\\n\\n## Changes to Ingress and Egress\\n\\nInteracting with the node no longer counts as pipeline ingress or egress. This\\nis best explained visually:\\n\\n![Ingress & Egress Changes](tenzir-v4.6-ingress-egress-changes.excalidraw.svg)\\n\\n## Want More?\\n\\n- The CSV, SSV, and TSV parsers are now more robust and consistent with their\\n  respective printers for null values, empty strings, and lists of values. A new\\n  `--allow-comments` option allows for stripping away lines that begin with `#`.\\n\\n- The JSON parser supports a new option `--arrays-of-objects` to read a stream\\n  of arrays of objects rather than a stream of objects. This is particularly\\n  useful when fetching events from REST APIs, which usually aim to return all\\n  events at in one array rather than stream objects one at a time.\\n\\n- The new `yield` operator \\"zooms in\\" on a record field.\\n\\n- Use the `show` operator without any arguments to see all aspects of a node\\n  instead of just the specified aspect.\\n\\n- The new `apply` operator includes a pipeline defined in an external file. For\\n  example, `apply frobnify` will search for a file named `frobnify.tql`, first\\n  in the current directory, and then in the `apply/` sub-directories of the\\n  config directory of Tenzir.\\n\\nWe provide a full list of changes [in our changelog](/changelog#v460).\\n\\nHead over to [app.tenzir.com](https://app.tenzir.com) to play with the new\\nfeatures and join [our Discord server](/discord)\u2014the perfect place to ask\\nquestions, chat with Tenzir users and developers, and to discuss your feature\\nideas!"},{"id":"/enrichment-complexity-in-the-wild","metadata":{"permalink":"/blog/enrichment-complexity-in-the-wild","source":"@site/blog/enrichment-complexity-in-the-wild/index.md","title":"Enrichment Complexity in the Wild","description":"Enrichment is a major part of a security data lifecycle and can take on many","date":"2023-11-27T00:00:00.000Z","formattedDate":"November 27, 2023","tags":[{"label":"enrichment","permalink":"/blog/tags/enrichment"},{"label":"context","permalink":"/blog/tags/context"},{"label":"splunk","permalink":"/blog/tags/splunk"},{"label":"elastic","permalink":"/blog/tags/elastic"},{"label":"azure","permalink":"/blog/tags/azure"},{"label":"sentinel","permalink":"/blog/tags/sentinel"},{"label":"kusto","permalink":"/blog/tags/kusto"}],"readingTime":5.14,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Enrichment Complexity in the Wild","authors":["mavam"],"date":"2023-11-27T00:00:00.000Z","tags":["enrichment","context","splunk","elastic","azure","sentinel","kusto"],"comments":true},"prevItem":{"title":"Tenzir v4.6","permalink":"/blog/tenzir-v4.6"},"nextItem":{"title":"Tenzir v4.5","permalink":"/blog/tenzir-v4.5"}},"content":"Enrichment is a major part of a security data lifecycle and can take on many\\nforms: adding GeoIP locations for all IP addresses in a log, attaching asset\\ninventory data via user or hostname lookups, or extending alerts with magic\\nscore to bump it up the triaging queue. The goal is always to make the data more\\n*actionable* by providing a better ground for decision making.\\n\\nThis is the first part of series of blog posts on contextualization. We kick\\nthings off by looking at how existing systems do enrichment. In the next blog\\npost, we introduce how we address this use case with pipeline-first mindset in\\nthe Tenzir stack.\\n\\n![Enrichment Location](enrichment-location.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nWhen we refer to \\"enrichment\\" we typically mean performing an API call, joining\\nthe data at hand with another table/index in a SIEM, or a doing a lookup with\\nstatic data in CSV file. As shown above, we can do this either at *ingest-time*\\nbefore persisting the data, or at *search-time* when we retrieve historical\\ndata. There are also streaming-only use cases without historical data, but\\nthese can often be modeled as ingest-time enrichment with a different sink.\\n\\n## Existing Solutions\\n\\nLet\'s see how Splunk, Elastic, and Azure Data Explorer handle enrichment.\\n\\n### Splunk\\n\\n[Alex Teixeira](https://www.linkedin.com/in/inode/) wrote a great article on\\n[maintaining dynamic lookups in\\nSplunk](https://detect.fyi/the-salamander-method-how-to-maintain-dynamic-splunk-lookups-4fdae5868e7e). Here\'s the visual summary of his approach:\\n\\n![Salamander Method](salamander-method.png)\\n\\nSuppose we have a Splunk index called `web_access_logs` where we log web access\\nevents, and we want to maintain a weekly updated lookup table of unique visitor\\nIP addresses and their last visit date. Let\'s create an initial lookup table,\\nnamed `weekly_visitors.csv`, with fields like `ip_address` and\\n`last_visit_date`. Then we\'ll set up a scheduled search to run weekly. The\\nsearch should:\\n\\n- Extract the latest week\'s unique IP addresses and their last visit date\\n from `web_access_logs`.\\n- Load the existing `weekly_visitors.csv` using `inputlookup`.\\n- Merge and update the data, discarding IPs older than 7 days.\\n- Output the updated table using `outputlookup`.\\n\\nHere\'s the SPL for for the scheduled search:\\n\\n```spl\\nindex=\\"web_access_logs\\" earliest=-7d@d latest=@d\\n| stats latest(_time) as last_visit by ip_address\\n| eval last_visit_date=strftime(last_visit, \\"%F\\")\\n| inputlookup append=true weekly_visitors.csv\\n| dedup ip_address sortby -last_visit\\n| where last_visit >= relative_time(now(), \\"-7d@d\\")\\n| outputlookup weekly_visitors.csv\\n```\\n\\nIn detail:\\n\\n- `index=\\"web_access_logs\\" earliest=-7d@d latest=@d`: Fetches the last week\'s\\n  web access logs.\\n- `stats latest(_time) as last_visit by ip_address`: Aggregates the latest visit\\n  time for each IP.\\n- `eval last_visit_date=strftime(last_visit, \\"%F\\")`: Formats the last visit\\n  date.\\n- `inputlookup append=true weekly_visitors.csv`: Appends current lookup data for\\n  comparison.\\n- `dedup ip_address sortby -last_visit`: Removes duplicate IPs, keeping the most\\n  recent.\\n- `where last_visit >= relative_time(now(), \\"-7d@d\\")`: Filters out IPs older\\n  than 7 days.\\n- `outputlookup weekly_visitors.csv`: Updates the lookup table with the new\\n  data.\\n\\nThis query demonstrates Alex\' \\"Salamander Method\\" by regularly updating the\\nlookup table with recent data while discarding outdated records, maintaining an\\nup-to-date context for data enrichment.\\n\\nYou\'d use it as follows:\\n\\n```\\nindex=\\"network_security_events\\"\\n| lookup weekly_visitors.csv ip_address as source_ip OUTPUT last_visit_date\\n```\\n\\nThe `lookup` command enriches each event with the `last_visit_date` from\\n`weekly_visitors.csv` based on the matching `ip_address`. In this scenario,\\nyou\'re adding a temporal context to the security events by identifying when each\\nIP address involved in these events last visited your network. This can be\\nparticularly useful for quickly assessing whether a security event is related to\\na new or returning visitor, potentially aiding in the rapid assessment of the\\nevent\'s nature and severity.\\n\\n### Elastic\\n\\nElastic\'s [new ES|QL language](/blog/a-first-look-at-esql) also\\n[supports\\nenrichment](https://www.elastic.co/guide/en/elasticsearch/reference/master/esql-enrich-data.html)\\nusing the [`ENRICH`\\ncommand](https://www.elastic.co/guide/en/elasticsearch/reference/master/esql-commands.html#esql-enrich).\\nEnrichment is a key-based lookup using special index type. The diagram below\\nshows how it works.\\n\\n![ES|QL Enrichment](esql-enrich.png)\\n\\nAssume you have an index `network_security_logs` with fields like `source_ip`\\nand an enrich policy `threat_intel_policy` with data based on IP addresses and a\\nfield `threat_level`.\\n\\n```\\nSELECT e.*, threat.threat_level\\nFROM network_security_logs AS e\\nENRICH threat_intel_policy\\nON e.source_ip\\nWITH threat_level\\n```\\n\\nThis query enriches each record in `network_security_logs` with the\\n`threat_level` field from the threat intelligence data, providing an additional\\nlayer of context.\\n\\n### Sentinel\\n\\nIn Sentinel or Azure Data Explorer, you have a more data-centric view on the\\nproblem, using a combination of the Kusto operators\\n[`lookup`](https://docs.microsoft.com/azure/data-explorer/kusto/query/lookupoperator)\\nand\\n[`join`](https://docs.microsoft.com/azure/data-explorer/kusto/query/joinoperator):\\n\\n```\\nSecurityEvent\\n| lookup kind=leftouter GeoIPTable on $left.IPAddress == $right.GeoIP\\n| join kind=leftouter ThreatIntelTable on $left.IPAddress == $right.ThreatIP\\n```\\n\\nThis query takes a security event, enriches it with the `GeoIPTable` data based\\non the source IP, and then joins it with threat intelligence data from the\\n`ThreatIntelTable`.\\n\\n## Search vs. Ingest Time Enrichment\\n\\nWe\'ve now seen three examples for *search-time* enrichment. For *ingest-time*\\nenrichments, Splunk users need to adapt a config file `transforms.conf`.\\nElastic users can either use a separate tool\\n[Logstash](https://www.elastic.co/logstash) or resort to [Node Ingest\\nPipelines](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html).\\nAzure users can user to [Event\\nHubs](https://docs.microsoft.com/azure/event-hubs/), [Azure Stream\\nAnalytics](https://docs.microsoft.com/azure/stream-analytics/), or [Azure\\nFunctions](https://docs.microsoft.com/azure/azure-functions/). As a Sentinel\\nusers, you can use Data Collection Rules (DCRs) to apply KQL transformations to\\nincoming data before it\'s stored in your workspace. Here\'s an example of how\\nthis can be done:\\n\\n```\\nlet GeoIP = externaldata(country:string, city:string, [ip_range:string])\\n@\\"https://example.com/geoipdata.csv\\" \\nwith (format=\'csv\', ignoreFirstRecord=True);\\nSecurityEvent\\n| extend parsedIP = parse_ipv4(ip_address)\\n| lookup kind=leftouter GeoIP on $left.parsedIP between $right.ip_range\\n```\\n\\nWhat\'s left is taking this transformation and adding it to the data collection\\nrule. Here\'s a diagram from the\\n[documentation](https://learn.microsoft.com/en-us/azure/sentinel/data-transformation)\\non how the Azure pieces fit together:\\n\\n![Data Transformation Architecture](data-transformation-architecture.png)\\n\\n## Conclusion\\n\\nWe reviewed three existing approaches to enrichment by looking at Splunk,\\nElastic, and Sentinel. Common among all systems is the idea of first building a\\ndataset for contextualization, and then using that in a second step. In\\nparticular, the use of the context is decoupled from the management of the\\ncontext.\\n\\nWe could argue a lot about syntax ergonomics and system idiosyncrasies. But that\\nwouldn\'t move the needle much. The foundational mechanisms are the same in the\\ndifferent systems. That said, we did ask ourselves: how can we make enrichment\\n*as easy, fast, and flexible as possible*? Our next blog will have the answer.\\n\\nIn the meantime, feel free to browse through our docs, read our blog posts, or\\njoin our [Discord server](/discord) to talk to the power users in our\\ncommunity. You can always skip everything and dive right in at\\n[app.tenzir.com](https://app.tenzir.com)."},{"id":"/tenzir-v4.5","metadata":{"permalink":"/blog/tenzir-v4.5","source":"@site/blog/tenzir-v4.5/index.md","title":"Tenzir v4.5","description":"Here comes Tenzir v4.5!","date":"2023-11-16T00:00:00.000Z","formattedDate":"November 16, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"operators","permalink":"/blog/tags/operators"},{"label":"expression","permalink":"/blog/tags/expression"},{"label":"index","permalink":"/blog/tags/index"},{"label":"api","permalink":"/blog/tags/api"},{"label":"demo-node","permalink":"/blog/tags/demo-node"}],"readingTime":3.05,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.5","authors":["dominiklohmann"],"date":"2023-11-16T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","operators","expression","index","api","demo-node"],"comments":true},"prevItem":{"title":"Enrichment Complexity in the Wild","permalink":"/blog/enrichment-complexity-in-the-wild"},"nextItem":{"title":"Tenzir v4.4","permalink":"/blog/tenzir-v4.4"}},"content":"Here comes [Tenzir v4.5](https://github.com/tenzir/tenzir/releases/tag/v4.5.0)!\\nThis release ships a potpourri of smaller improvements that result in faster\\nhistorical query execution and better deployability.\\n\\n![Tenzir v4.5](tenzir-v4.5.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## More Robust Numeric Query Expressions\\n\\nEver scratched your head what numeric literal to use in a query so that you hit\\nthe right fields in events? No more! You can now write `x < 42`, `x < +42`, or\\n`x < 42.0` in an expression to filter events where the field `x` is less than\\n42, regardless of whether `x` is of type `uint64`, `int64`, or `double`.\\nPreviously, you had to know the exact number type for the expression to bind\\nproperly to the event schema.\\n\\nThis incremental improvement is part of a larger thrust to improve the language.\\nWe have plans to convert additional literals during expression bindings to\\nprovide deeper reach into the data, without users really having to deal with\\nschemas.\\n\\n## Sparse Indexes for Number Types\\n\\nTenzir\'s storage engine builds sparse indexes per partition. These sketch\\ndata structures, like Bloom filters and min-max synopses, accelerate historical\\nqueries by ensuring that only relevant partitions get loaded into memory.\\n\\nWe\'ve now added additional sketches for types `bool`, `time`, `duration`,\\n`int64`, `uint64`, and `double` to accelerate a broader range of queries. For\\nexample, this query now runs faster:\\n\\n```\\nexport\\n| where :time > 1 hour ago && dest_port == 80\\n```\\n\\n## The Rest API as an Operator\\n\\nWe exposed [the Rest API](/api) as a new operator called\\n[`api`](/next/operators/api). The benefit primarily materializes for\\ndevelopers, who can now rapidly prototype integrations by using the app or\\n`tenzir` command line tool, without having to spin up the integrated web\\nserver and do gymnastics with `curl` and `jq`.\\n\\nFor example, to list all pipelines that were created through the API:\\n\\n```\\napi /pipeline/list\\n```\\n\\nThis creates a new pipeline and starts it immediately:\\n\\n```\\napi /pipeline/create \'{\\"name\\": \\"Suricata Import\\", \\"definition\\": \\"from file /tmp/eve.sock read suricata\\", \\"autostart\\": {\\"created\\": true}}\'\\n```\\n\\n## Fine-Grained Operator, Format, and Connector Block Lists\\n\\nWe made it easier to disallow potentially unsafe operators, formats, and\\nconnectors. The new `tenzir.disable-plugins` option is a list of names of\\nplugins to explicitly forbid from being used. For example, adding `export` will\\nprohibit use of the `export` operator builtin, thereby disabling the ability to\\nrun historical queries. This method allows for a more fine-grained control than\\nthe coarse `tenzir.allow-unsafe-pipelines` option.\\n\\nWhy does it matter? Well, when running pipelines in a node, some operators allow\\nyou to fully interact with the system through a pipeline. The\\n[`shell`](/next/operators/shell) operator is the best example, which allows for\\narbitrary command execution. This can be both a huge relief and serve as escape\\nhatch to integrate third-party tools, but it is equally a security risk.\\n\\n## This & That\\n\\n- When you deploy a demo node at [app.tenzir.com](https://app.tenzir.com), it\\n  now starts up faster, and the pre-loaded pipelines come with labels and have been\\n  ported to use the new `api` operator instead of relying on `curl` for setup.\\n\\n- It is now possible to reference nested records in many operators that wrangle\\n  data, such as `select`, `extend`, `put`, and `replace`.\\n\\n- The `summarize` operator now yields a result even if it receives no input\\n  (assuming there is no grouping with `by`). For example, `summarize\\n  num=count(foo)` returns `{\\"num\\": 0}` instead of returning nothing.\\n\\n- The `import` operator now flushes events to disk automatically before\\n  returning, ensuring that they are available immediately for subsequent uses of\\n  the `export` operator.\\n\\nWe provide a full list of changes [in our changelog](/changelog#v450).\\n\\nHead over to [app.tenzir.com](https://app.tenzir.com) to check out what\'s new.\\nGot questions? Swing by our friendly [our Discord server](/discord) and let us\\nknow."},{"id":"/tenzir-v4.4","metadata":{"permalink":"/blog/tenzir-v4.4","source":"@site/blog/tenzir-v4.4/index.md","title":"Tenzir v4.4","description":"Tenzir v4.4 is out!","date":"2023-11-06T00:00:00.000Z","formattedDate":"November 6, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"operators","permalink":"/blog/tags/operators"},{"label":"velociraptor","permalink":"/blog/tags/velociraptor"},{"label":"yara","permalink":"/blog/tags/yara"},{"label":"amqp","permalink":"/blog/tags/amqp"}],"readingTime":2.535,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.4","authors":["dominiklohmann"],"date":"2023-11-06T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","operators","velociraptor","yara","amqp"],"comments":true},"prevItem":{"title":"Tenzir v4.5","permalink":"/blog/tenzir-v4.5"},"nextItem":{"title":"Matching YARA Rules in Byte Pipelines","permalink":"/blog/matching-yara-rules-in-byte-pipelines"}},"content":"[Tenzir v4.4](https://github.com/tenzir/tenzir/releases/tag/v4.4.0) is out!\\nWe\'ve focused this release on integrations with two pillars of the digital\\nforensics and incident response (DFIR) ecosystem: [YARA][yara] and\\n[Velociraptor][velociraptor].\\n\\n[yara]: https://yara.readthedocs.io\\n[velociraptor]: https://docs.velociraptor.app\\n\\n![Tenzir v4.4](tenzir-v4.4.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## YARA Operator\\n\\nThe star feature of this release is the new [`yara`](/next/operators/yara)\\noperator. You can now match [YARA][yara] rules directly within byte pipelines.\\nThis is a game-changer for threat intelligence and cybersecurity workflows, as\\nit brings together all of Tenzir\'s connectors with the community\'s rich\\necosystem of YARA rules for efficient malware detection and analysis. Evaluating\\na set of rules on a file located in an S3 bucket has never been easier:\\n\\n```\\nload s3 bucket/file.exe\\n| yara path/to/rules/\\n```\\n\\n:::info\\nWe\'ve written a blog post on the YARA operator that shows just how it works and\\nexplains in-depth how you can use it: [Matching YARA Rules in Byte\\nPipelines](/blog/matching-yara-rules-in-byte-pipelines)\\n:::\\n\\n## Velociraptor Operator\\n\\n[Velociraptor][velociraptor] is an advanced DFIR tool that enhances your\\nvisibility into your endpoints. Not unlike [our own TQL](/language),\\nVelociraptor comes with its own language for interacting with it\\nprogrammatically: VQL. The `velociraptor` operator makes it possible to submit\\nVQL queries to a Velociraptor server, as well as subscribe to artifacts\\nin hunt flows over a large fleet of assets, making endpoint telemetry\\ncollection and processing a breeze.\\n\\n:::info\\nRead our blog post on how we built this integration and how you can utilize it:\\n[Integrating Velociraptor into Tenzir\\nPipelines](/blog/integrating-velociraptor-into-tenzir-pipelines)\\n:::\\n\\n## AMQP Connector\\n\\nThe new [`amqp`](/next/connectors/amqp) connector brings a full-fledged AMQP\\n0-9-1 client to the table. Relying on the battle-proven [RabbitMQ C client\\nlibrary](https://github.com/alanxz/rabbitmq-c), the operator makes it possible\\nyou to interact with queues and exchanges as shown in the diagram below:\\n\\n![AMQP](amqp.excalidraw.svg)\\n\\n## Noteworthy Improvements\\n\\nBesides the new operators, I would like to highlight the following changes:\\n\\n- **Live Exports:** Start your pipeline with `export --live` to get all events\\n  in one pipeline as they are imported.\\n\\n- **Blob Type:** We\'ve added a new `blob` type that allows you to handle binary\\n  data. Use the `blob` type over the `string` type for binary payloads that are\\n  not UTF8-encoded.\\n\\n- **Rich Schema Inference for CSV:** Inferring schemas for CSV files has been\\n  significantly enhanced. It now provides more precise types, leading to more\\n  insightful analysis.\\n\\n- **Automated Pipeline Management:** New controls for auto-restart, auto-delete\\n  and a runtime limit are now available when creating a pipeline. For a more\\n  granular control of the auto-restart and auto-delete behavior, the _Stopped_\\n  state for pipeline has now been divided into _Stopped_, _Completed_, and\\n  _Failed_. The states reflect whether a pipeline was manually stopped, ended\\n  naturally, or encountered an error, respectively.\\n\\n- **Label Support for Pipelines:** You can now visually group related pipelines\\n  using the new labels feature. This helps you in organizing your pipelines\\n  better for improved visibility and accessibility.\\n\\nWe provide a full list of changes [in our changelog](/changelog#v440).\\n\\nCheck out the new features on [app.tenzir.com](https://app.tenzir.com). We\'re\\nexcited to see the amazing things you will accomplish with them!\\n\\nYour feedback matters and drives our growth. Join the discussion in [our\\nDiscord](/discord)!"},{"id":"/matching-yara-rules-in-byte-pipelines","metadata":{"permalink":"/blog/matching-yara-rules-in-byte-pipelines","source":"@site/blog/matching-yara-rules-in-byte-pipelines/index.md","title":"Matching YARA Rules in Byte Pipelines","description":"The new yara operator matches YARA rules on bytes,","date":"2023-11-01T00:00:00.000Z","formattedDate":"November 1, 2023","tags":[{"label":"yara","permalink":"/blog/tags/yara"},{"label":"operator","permalink":"/blog/tags/operator"},{"label":"dfir","permalink":"/blog/tags/dfir"},{"label":"detection engineering","permalink":"/blog/tags/detection-engineering"}],"readingTime":5.8,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Matching YARA Rules in Byte Pipelines","authors":["mavam"],"date":"2023-11-01T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["yara","operator","dfir","detection engineering"],"comments":true},"prevItem":{"title":"Tenzir v4.4","permalink":"/blog/tenzir-v4.4"},"nextItem":{"title":"Integrating Velociraptor into Tenzir Pipelines","permalink":"/blog/integrating-velociraptor-into-tenzir-pipelines"}},"content":"The new [`yara`][yara-operator] operator matches [YARA][yara] rules on bytes,\\nproducing a structured match output to conveniently integrate alerting tools or\\ntrigger next processing steps in your detection workflows.\\n\\n[yara]: https://virustotal.github.io/yara/\\n[yara-operator]: /next/operators/yara\\n\\n![YARA Operator](yara-operator.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n[YARA][yara] rules are a bedrock piece when it comes to writing detections on\\nbinary data. Malware analysts develop them based on sandbox results or threat\\nreports, incident responders capture the attacker\'s toolchain on disk images or\\nin memory, and security engineers share them with their peers.\\n\\n## Operationalize YARA rules\\n\\nThe most straight-forward way to execute a YARA rule is the official [`yara`\\ncommand-line utility](https://yara.readthedocs.io/en/stable/commandline.html).\\nConsider this rule:\\n\\n```\\nrule test {\\n  meta:\\n    string = \\"string meta data\\"\\n    integer = 42\\n    boolean = true\\n\\n  strings:\\n    $foo = \\"foo\\"\\n    $bar = \\"bar\\"\\n    $baz = \\"baz\\"\\n\\n  condition:\\n    ($foo and $bar) or $baz\\n}\\n```\\n\\nRunning `yara -g -e -s -L test.yara test.txt` on a file `test.txt` with contents\\n`foo bar` yields the following output:\\n\\n```\\ndefault:test [] test.txt\\n0x0:3:$foo: foo\\n0x4:3:$bar: bar\\n```\\n\\nThere are other ways to execute YARA rules, e.g.,\\n[ClamAV](https://www.clamav.net/),\\n[osquery](https://osquery.readthedocs.io/en/stable/deployment/yara/), or\\n[Velociraptor](https://docs.velociraptor.app/vql_reference/plugin/yara/)\u2014which\\nwe also [integrated as pipeline\\noperator](/blog/integrating-velociraptor-into-tenzir-pipelines).\\n\\nAnd now there\'s also Tenzir, with a [`yara`][yara-operator] operator that\\naccepts bytes as input and produces events as output. Let\'s take the simple case\\nof running the above example on string input:\\n\\n```bash\\necho \'foo bar\' | tenzir \'load stdin | yara /tmp/test.yara\'\\n```\\n\\nThe operator generates one `yara.match` event per matching rule:\\n\\n```json\\n{\\n  \\"rule\\": {\\n    \\"identifier\\": \\"test\\",\\n    \\"namespace\\": \\"default\\",\\n    \\"tags\\": [],\\n    \\"meta\\": {\\n      \\"string\\": \\"string meta data\\",\\n      \\"integer\\": 42,\\n      \\"boolean\\": true\\n    },\\n    \\"strings\\": {\\n      \\"$foo\\": \\"foo\\",\\n      \\"$bar\\": \\"bar\\",\\n      \\"$baz\\": \\"baz\\"\\n    }\\n  },\\n  \\"matches\\": {\\n    \\"$foo\\": [\\n      {\\n        \\"data\\": \\"Zm9v\\",\\n        \\"base\\": 0,\\n        \\"offset\\": 0,\\n        \\"match_length\\": 3\\n      }\\n    ],\\n    \\"$bar\\": [\\n      {\\n        \\"data\\": \\"YmFy\\",\\n        \\"base\\": 0,\\n        \\"offset\\": 4,\\n        \\"match_length\\": 3\\n      }\\n    ]\\n  }\\n}\\n```\\n\\nEach match has a `rule` field describing the rule and a `matches` record indexed\\nby string identifier to report a list of matches per rule string. E.g., there is\\none match for `$bar` at byte offset 4 and match length 3. The Base64-encoded\\nexcerpt for the match is `YmFy` (= `\\"bar\\"`).[^1]\\n\\n[^1]: JSON doesn\'t distinguish binary blobs from strings. However, our type\\n    system does, so we encode blob values as Base64-encoded strings for formats\\n    that do not have a native blog representation.\\n\\n## Building a YARA streaming engine\\n\\n:::note Implementation Details\\nYou can skip this section if you are not interested in the inner workings, but\\nit may help understand how YARA works under the hood.\\n:::\\n\\nTenzir byte pipelines consist of a stream of variable-size chunks of memory.\\nE.g., when loading the raw bytes of file via `load file`, the dataflow may\\nconsist of multiple chunks. YARA scanners can also operate on multiple blocks of\\ndata. It might be tempting to treat these as contiguous, adjacent blocks of\\nmemory (we did this initially) and think that it should be possible to match a\\nrule across adjacent a blocks, like this:\\n\\n![YARA scanner blocks](yara-implementation.excalidraw.svg)\\n\\n[This is not the case](https://github.com/VirusTotal/yara/issues/1994). While it\\n*may* work, it\'s possible to write rules where this fails. As a result, simply\\nkeeping the input blocks in memory and feeding them to a scanner *might cause\\nfalse negatives* if you have a rule that should match across chunk boundaries.\\nIn other words, it\'s not possible to build an incremental streaming engine with\\nthe current YARA architecture. Moreover, YARA may perform multiple passes over\\nthe input, so it\'s neither possible to construct a one-pass streaming engine.\\n\\nThis is the reason why the `yara` operator supports two modes of operation:\\n\\n1. **Accumulating**: Accumulate all chunks perform a scan at the end. (default)\\n2. **Blockwise**: scan each block of memory as self-contained unit.\\n   (`--blockwise`)\\n\\nMode (1) copies all chunks in a single buffer. Mode (2) does work in streaming\\nmode, but it only makes sense if each chunk of memory is a self-contained unit,\\ne.g., when getting memory chunks from a message broker.\\n\\n## Mix and match loaders\\n\\nThe [`stdin`](/connectors/stdin) loader in the above example produces chunks of\\nbytes. But you can use any connector of your choice that yields bytes. In\\nparticular, you can use the [`file`](/connectors/file) loader:\\n\\n```bash\\ntenzir \'load file --mmap /tmp/test.txt | yara /tmp/test.yara\'\\n```\\n\\n:::note Memory-mapping files\\nPassing `--mmap` to the `file` loader is purely an optimization that results in\\nthe creation of a single memory block as input to the `yara` operator. This\\nmeans the YARA scanner doesn\'t have to iterate over multiple blocks of memory,\\nwhich may be beneficial for intricate rules that require random access into the\\nfile.\\n:::\\n\\nIf you have a [ZeroMQ socket](/connectors/zmq) where you publish malware samples\\nto be scanned, then you only need to change the pipeline source:\\n\\n```bash\\ntenzir \'load zmq | yara /tmp/test.yara\'\\n```\\n\\nThis is where the [separation between structured and unstructured\\ndata][separation-of-concerns] in pipelines pays off. You plug in any loader\\nwhile leaving the remainder of `yara` pipeline in place.\\n\\n[separation-of-concerns]: /blog/five-design-principles-for-building-a-data-pipeline-engine#p1-separation-of-concerns\\n\\n## Post-process matches\\n\\nBecause the matches are structured events, you can use all existing operators to\\npost-process them. For example, send them to a Slack channel via\\n[`fluent-bit`](/next/operators/fluent-bit):\\n\\n```\\nload file --mmap /tmp/test.txt\\n| yara /tmp/test.yara\\n| fluent-bit slack webhook=URL\\n```\\n\\nOr store them with [`import`](/next/operators/import) at a Tenzir node to\\ngenerate match statistics later on:\\n\\n```\\nload file --mmap /tmp/test.txt\\n| yara /tmp/test.yara\\n| import\\n```\\n\\n## Create a YARA rule matching service\\n\\nUsing just a few pipelines, you can quickly deploy a YARA rule scanning service\\nthat sends the matches to a Slack webhook. Let\'s that you want to scan malware\\nsample that you receive over a [Kafka](../../connectors/kafka) topic\\n`malware`. Launch the processing pipeline as follows:\\n\\n```\\nload kafka --topic malware\\n| yara --blockwise /path/to/rules\\n| fluent-bit slack webhook=URL\\n```\\n\\nThis pipeline requires that every Kafka message is a self-contained malware\\nsample. Because the pipeline runs continuously, we supply the `--blockwise`\\noption so that the `yara` triggers a scan for every Kafka message, as opposed to\\naccumulating all messages indefinitely and only initiating a scan when the input\\nexhausts.\\n\\nYou can now submit a malware sample by sending it to the `malware` Kafka topic:\\n\\n```\\nload file --mmap evil.exe | save kafka --topic malware\\n```\\n\\nThe matches should now arrive as JSON message in the Slack channel associated\\nwith the webhook.\\n\\n## Summary\\n\\nWe\'ve introduced the [`yara`][yara-operator] operator as a byte-to-events\\ntransformation that exposes YARA rule matches as structured events, making them\\neasy to post-process with the existing collection of Tenzir operators. We also\\nexplained how you can create a simple YARA rule scanning service that accepts\\nmalware samples via Kafka and sends the matches to a Slack channel.\\n\\nTry it yourself. Deploy detection pipelines with the `yara` operator for free\\nwith our Community Edition at [app.tenzir.com](https://app.tenzir.com). Missing\\nany other operators that operationalize detections? Swing by our [Discord\\nserver](/discord) and let us know!\\n\\n:::note Acknowledgements\\nThanks to [Thomas Patzke](https://github.com/thomaspatzke) for reviewing this\\nblog post and suggesting to make the default behavior of the operator more safe\\nto use. \ud83d\ude4f\\n:::"},{"id":"/integrating-velociraptor-into-tenzir-pipelines","metadata":{"permalink":"/blog/integrating-velociraptor-into-tenzir-pipelines","source":"@site/blog/integrating-velociraptor-into-tenzir-pipelines/index.md","title":"Integrating Velociraptor into Tenzir Pipelines","description":"The new velociraptor operator allows you to run","date":"2023-10-19T00:00:00.000Z","formattedDate":"October 19, 2023","tags":[{"label":"velociraptor","permalink":"/blog/tags/velociraptor"},{"label":"operator","permalink":"/blog/tags/operator"},{"label":"dfir","permalink":"/blog/tags/dfir"}],"readingTime":3.395,"hasTruncateMarker":true,"authors":[{"name":"Christoph Lobmeyer","title":"Senior Expert Incident Response (External)","url":"https://github.com/lo-chr","image_url":"https://github.com/lo-chr.png","imageURL":"https://github.com/lo-chr.png"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Integrating Velociraptor into Tenzir Pipelines","authors":[{"name":"Christoph Lobmeyer","title":"Senior Expert Incident Response (External)","url":"https://github.com/lo-chr","image_url":"https://github.com/lo-chr.png","imageURL":"https://github.com/lo-chr.png"},"mavam"],"date":"2023-10-19T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["velociraptor","operator","dfir"],"comments":true},"prevItem":{"title":"Matching YARA Rules in Byte Pipelines","permalink":"/blog/matching-yara-rules-in-byte-pipelines"},"nextItem":{"title":"Five Design Principles for Building a Data Pipeline Engine","permalink":"/blog/five-design-principles-for-building-a-data-pipeline-engine"}},"content":"The new [`velociraptor`][velociraptor-operator] operator allows you to run\\n[Velociraptor Query Language (VQL)][vql] expressions against a\\n[Velociraptor][velociraptor] server and process the results in a Tenzir\\npipeline. You can also subscribe to matching artifacts in hunt flows over a\\nlarge fleet of assets, making endpoint telemetry collection and processing a\\nbreeze.\\n\\n[velociraptor]: https://docs.velociraptor.app/\\n[velociraptor-operator]: /next/operators/velociraptor\\n[vql]: https://docs.velociraptor.app/docs/vql\\n\\n![Velociraptor and Tenzir](velociraptor-and-tenzir.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n[Velociraptor][velociraptor] is a powerful digital forensics and incident\\nresponse (DFIR) tool for managing and interrogating endpoints. Not only does it\\nsupport ad-hoc extraction of forensic artifacts, but also continuous event\\nmonitoring to get alerted when suspicious things happen, such as the\\ninstallation of new scheduled tasks on a Windows machine.\\n\\nWe have been asked to make it possible to process the data collected at\\nendpoints in a Tenzir pipeline, so that you can store it cost-effectively,\\nfilter it, reshape it, and route it to your destination of choice. The\\n`velociraptor` operator honors this request. Thanks to Velociraptor\'s [gRPC\\nAPI][api] and [Python library][pyvelociraptor] that ship with the\\n[Protobuf][proto] definition, the implementation in C++ was straight-forward.\\n\\n[api]: https://docs.velociraptor.app/docs/server_automation/server_api/\\n[pyvelociraptor]: https://github.com/Velocidex/pyvelociraptor\\n[proto]: https://github.com/Velocidex/pyvelociraptor/blob/master/pyvelociraptor/api.proto\\n\\n## Usage\\n\\nThe `velociraptor` operator is a source that emits events. We implemented two\\nways to interact with a Velociraptor server:\\n\\n1. Send a [VQL][vql] query to a server and process the response.\\n\\n2. Use the `--subscribe <artifact>` option to hook into a continuous feed of\\n   artifacts that match the `<artifact>` regular expression. Whenever a client\\n   responds to a hunt that contains this artifact, the response will be\\n   forwarded to the pipeline and emit the artifact payload in the response field\\n   `HuntResults`.\\n\\n### Raw VQL\\n\\nHere\'s how you execute a VQL query and store the result at a Tenzir node:\\n\\n```bash\\nvelociraptor --query \\"select * from pslist()\\"\\n| import\\n```\\n\\nStoring it via [`import`](/next/operators/import) is just one of many options.\\nFor ad-hoc investigations, you often just want to analyze the result, for which\\na variety of transformations come in handy. For example:\\n\\n```bash\\nvelociraptor --query \\"select * from pslist()\\"\\n| select Name, Pid, PPid, CommandLine\\n| where Name == \\"remotemanagement\\"\\n```\\n\\n### Artifact Subscription\\n\\nIf you use Velociraptor to perform interactive investigations in DFIR cases, you\\nprobably hunt for forensic artifacts (like dropped files or specific entries in\\nthe Windows registry) on assets connected to your Velociraptor server. For\\nenrichment or to correlate the results with other security related data, you\\nmight want to post-process results of Velociraptor hunts.\\n\\nWith this feature Tenzir can subscribe to results of hunts, containing\\nVelociraptor artifacts of your choice [like the ones shipped with\\nVelociraptor](https://docs.velociraptor.app/artifact_references/). Every time a\\nclient reports back on an artifact that matches the given Regex (like `Windows`\\nor `Windows.Sys.StartupItems`) Tenzir will ingest the result of the underlying\\nquery into the pipeline.\\n\\n```bash\\nvelociraptor --subscribe Windows.Sys.StartupItems | import\\n```\\n\\nThere are many examples of anomalies to search for, like malware families\\npersisting in Windows RunKeys. You can find some inspirations in the procedure\\nexamples of [MITRE ATT&CK Sub-Technique\\nT1547.001](https://attack.mitre.org/techniques/T1547/001/).\\n\\nThe implementation of this feature\u2014specifically the underlying VQL query\u2014is\\ninspired by the built-in capability of Velociraptor to upload results of hunts\\n(the flows) to an elastic server utilizing the [Elastic.Flows.Upload\\nartifact](https://docs.velociraptor.app/artifact_references/pages/elastic.flows.upload/).\\n\\n## Preparation\\n\\nThe `velociraptor` pipeline operator acts as client and it establishes a\\nconnection to a Velociraptor server via gRPC. All Velociraptor client-to-server\\ncommunication is mutually authenticated and encrypted via TLS certificates. This\\nmeans you must provide a client-side certificate, which you can generate as\\nfollows. (Velociraptor ships as a static binary that we\\nrefer to as `velociraptor-binary` here.)\\n\\n1. Create a server configuration `server.yaml`:\\n   ```bash\\n   velociraptor-binary config generate > server.yaml\\n   ```\\n\\n2. Create an API client:\\n   ```bash\\n   velociraptor-binary -c server.yaml config api_client --name tenzir client.yaml\\n   ```\\n\\n   Copy the generated `client.yaml` to your Tenzir plugin configuration\\n   directory as `velociraptor.yaml` so that the operator can find it:\\n   ```bash\\n   cp client.yaml /etc/tenzir/plugin/velociraptor.yaml\\n   ```\\n\\n3. Run the frontend with the server configuration:\\n   ```bash\\n   velociraptor-binary -c server.yaml frontend\\n   ```\\n\\nNow you are ready to run VQL queries!\\n\\n:::note Acknowledgements\\nBig thanks to [Christoph Lobmeyer](https://github.com/lo-chr) who\\ncontributed the intricate expression that is behind the `--subscribe <artifact>`\\noption and wrote parts of this blog post. \ud83d\ude4f\\n:::"},{"id":"/five-design-principles-for-building-a-data-pipeline-engine","metadata":{"permalink":"/blog/five-design-principles-for-building-a-data-pipeline-engine","source":"@site/blog/five-design-principles-for-building-a-data-pipeline-engine/index.md","title":"Five Design Principles for Building a Data Pipeline Engine","description":"One thing we are observing is that organizations are actively seeking out","date":"2023-10-17T00:00:00.000Z","formattedDate":"October 17, 2023","tags":[{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"design","permalink":"/blog/tags/design"}],"readingTime":14.555,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Five Design Principles for Building a Data Pipeline Engine","authors":["mavam"],"date":"2023-10-17T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["pipelines","design"],"comments":true},"prevItem":{"title":"Integrating Velociraptor into Tenzir Pipelines","permalink":"/blog/integrating-velociraptor-into-tenzir-pipelines"},"nextItem":{"title":"Tenzir v4.3","permalink":"/blog/tenzir-v4.3"}},"content":"One thing we are observing is that organizations are actively seeking out\\nsolutions to better manage their security data operations. Until recently, they\\nhave been aggressively repurposing common data and observability tools. I\\nbelieve that this is a stop-gap measure because there was no alternative. But\\nnow there is a growing ecosystem of security data operations tools to support\\nthe modern security data stack. Ross Haleliuk\'s [epic\\narticle](https://ventureinsecurity.net/p/security-is-about-data-how-different)\\nlays this out at length.\\n\\nIn this article I am explaining the underlying design principles for developing\\nour own data pipeline engine, coming from the perspective of security teams that\\nare building out their detection and response architecture. These principles\\nemerged during design and implementation. Many times, we asked ourselves \\"what\'s\\nthe right way of solving this problem?\\" We often went back to the drawing board\\nand started challenging existing approaches, such as what a data source is, or\\nwhat a connector should do. To our surprise, we found a coherent way to answer\\nthese questions without having to make compromises. When things feel Just Right,\\nit is a good sign to have found the right solution for a particular problem.\\nWhat we are describing here are the lessons learned from studying other systems,\\ndistilled as principles to follow for others.\\n\\n![Five Design Principles](five-design-principles.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\nThis article comes from the data engineering perspective. What makes a data\\npipeline \\"security\\" is left for a future post. Technical SOC architects, CTOs,\\nor Principal Engineers our audience. Enjoy!\\n\\n## A Value-First Approach to Data Pipelines\\n\\nOne way to think of it is that data pipelines offer **data as a service**. The\\npipeline provides the data in the right shape, at the right time, at the right\\nplace. As a data consumer, I do not want to bother with how it got there. I just\\nwant to use it to solve my actual problem. That\'s the value.\\n\\nBut data has to jump through many hoops to get to this goldilocks state. The\\n*pyramid of data value* is an attempt to describe these hoops on a discrete\\nspectrum:\\n\\n![Pyramid of Data Value](pyramid-of-data-value.excalidraw.svg)\\n\\nThe idea we want to convey is that you begin with a massive amount of\\nunstructured data, which makes up for the bulk of data in many organizations.\\nOnce you lift it into a more digestible form by structuring (= parsing) it, you\\ninvest cycles and time to make it easier to extract value from it. A byproduct\\nis often that you end up with less data afterwards. You can continue the process\\nto reshape the data to the exact form to solve a given problem, which often\\naccounts for just a tiny fraction of the original dataset. Data is often still\\nstructured then, but you invest a lot of time in deduplicating, aggregating,\\nenriching, and more, just to get the needed information for the consumer. This\\ndata massaging can take a lot of human effort!\\n\\nIt\'s incredibly costly for security teams to spend 80% of their time massaging\\nthe data. They should spend their focus on executing their mission, which is\\nhunting threats to protect their constituency. It\'s certainly not building tools\\nto wrangle data.\\n\\nWe argue that making it easier for domain experts to work with data\\nfundamentally requires a solution that can seamlessly cross all layers of this\\npyramid, as the boundaries are the places where efficiency leaks. The following\\nprinciples allowed us to get there.\\n\\n## P1: Separation of Concerns\\n\\nA central realization for us was that different types of data need different\\nabstractions that coherently fit together. When following the data journey\\nthrough the pyramid, we start with unstructured data. This means working with\\nraw bytes. You may get them over HTTP, Kafka, a file, or any other carrier\\nmedium. The next step is translating bytes into structured data. Parsing gets\\nthe data onto the \\"reshaping highway\\" where it can be manipulated at ease in a\\nrich data model.\\n\\nIn our nomenclature, [connectors](/connectors) are responsible for loading and\\nsaving raw, unstructured bytes; [formats](/formats) translate unstructured bytes\\ninto structured bytes; a rich set of computational [operators](/operators)\\nenable transformations on structured data.\\n\\nThe diagram below shows these key abstractions:\\n\\n![Connectors, Formats, Operators](connectors-formats-operators.excalidraw.svg)\\n\\nOne thing we miss looking at existing systems is a *symmetry* in these\\nabstractions. Data acquisition is often looked just one-way: getting data in and\\nthen calling it a day. No, please don\'t stop here! A structured data\\nrepresentation is easy to work with, but throwing the output as JSON over the\\nfence is not a one-size-fits-all solution. I may need CSV or YAML. I may want a\\nParquet file. I may need it in a special binary form. Deciding in what format to\\nconsume that data is critical for flexibility and performance. This is why we\\ndesigned our connectors and formats to be symmetric: a **loader** takes bytes\\nin, and a **saver** sends bytes out. A **parser** translates bytes to events,\\nand a **printer** translates them back.\\n\\nHere is an example that makes symmetric use of connectors, formats, and\\noperators:\\n\\n```\\nload gcs bucket/in/my/cloud/logs.zstd\\n| decompress zstd\\n| read json\\n| where #schema == \\"ocsf.network_activity\\"\\n| select connection_info.protocol_name,\\n         src_endpoint.ip,\\n         src_endpoint.port,\\n         dst_endpoint.ip,\\n         dst_endpoint.port\\n| to s3 bucket/in/my/other/cloud write parquet\\n```\\n\\nThis pipeline starts by taking compressed, unstructured data, then makes the\\ndata structured by parsing it as JSON, selects the connection 5-tuple, and\\nfinally writes it as a Parquet file into an S3 bucket.\\n\\n:::info Pipeline Languages Everywhere\\nNew pipeline languages are mushrooming all over the place. Splunk started with\\nits own Search Processing Language (SPL) and now released SPL2 to make it\\nmore pipeline-ish. And Elastic also doubled down as well with [their new\\nES|QL](/blog/a-first-look-at-esql). When we designed the [Tenzir Query Language\\n(TQL)](/language), we drew a lot of inspiration from\\n[PRQL](https://prql-lang.org/), [Nu](https://www.nushell.sh/), and\\n[Zed](https://zed.brimdata.io/). These may sound esoteric to some, but they are\\nremarkably well designed evolutions of pipeline languages that are *not* SQL.\\nDon\'t get us wrong, we love SQL when we have our data engineering hat on, but\\nour users shouldn\'t have to wear that hat. Splunk gets a lot of flak for its\\npricing, but one thing we admire is how well it caters to a broad audience of\\nusers that are not data engineering wizards. Our slightly longer [FAQ\\narticle](/faqs) elaborates on this topic.\\n:::\\n\\n## P2: Typed Operators\\n\\nIn Tenzir [pipelines](/pipelines), we call the atomic building blocks\\n**operators**. They are sometimes also called \\"steps\\" or \\"commands\\". Data flows\\nbetween them:\\n\\n![Pipeline Chaining](pipeline-chaining.excalidraw.svg)\\n\\nMany systems, including Tenzir, distinguish three types of operators:\\n**sources** that produce data, **sinks** that consume data, and\\n**transformations** that do both.\\n\\n![Pipeline Structure](pipeline-structure.excalidraw.svg)\\n\\nMost pipeline engines support one type of data that flows between the operators.\\nIf they exchange raw bytes, they\'d be in the \\"unstructured\\" layer of the\\npyramid. If they exchange JSON or data frames, they\'d be in the \\"structured\\"\\nlayer.\\n\\nBut in order to cut through the pyramid above, Tenzir pipelines make one more\\ndistinction: the elements that operators push through the pipeline are\\n*typed*. Every operator has an input and an output type:\\n\\n![Input and Output Types](operator-pieces.excalidraw.svg)\\n\\nWhen composing pipelines, these types have to match. Otherwise the pipeline is\\nmalformed. Let\'s take the above pipeline example and zoom out to just the\\ntyping:\\n\\n![Visualization](load-decompress-select-to.excalidraw.svg)\\n\\nWith the concept of input and output types in mind, the operator type become\\nmore apparent:\\n\\n![Operator Types](operator-types.excalidraw.svg)\\n\\nThis is quite powerful, because you can also *undulate* between bytes and events\\nwithin a pipeline before it ends in void. Consider this example:\\n\\n```\\nload nic eth0\\n| read pcap\\n| decapsulate\\n| where src_ip !in [0.0.0.0, ::]\\n| write pcap\\n| zeek\\n| write parquet\\n| save file /tmp/zeek-logs.parquet\\n```\\n\\nThis pipeline starts with PCAPs, transforms the acquired packets to events,\\n[decapsulates](/next/operators/decapsulate) them to filter on some packet\\nheaders, goes back to PCAP, runs Zeek[^1] on the filtered trace, and then writes\\nthe log as Parquet file to disk.\\n\\n[^1]: The `zeek` operator is [user-defined\\n    operator](/next/language/user-defined-operators) for `shell \\"zeek -r - \u2026\\" |\\n    read zeek-tsv`. We wrote a [blog post on how you can use `shell` as escape\\n    hatch to integrate arbitrary\\n    tools](/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir) in a\\n    pipeline.\\n\\nVisually, this pipeline has the following operator typing:\\n\\n![Undulating Pipeline](undulating-pipeline.excalidraw.svg)\\n\\n## P3: Multi-Schema Dataflows\\n\\nTo further unlock value within the structured data layer of the pyramid, we made\\nour pipelines **multi-schema**: a single pipeline can process heterogeneous\\ntypes of events, each of which have their own schemas. Multi-schema dataflows\\nrequire automatic schema inference at parse time, which all our parsers support.\\n\\nThis behavior is different from engines that work with structured data where\\noperators typically work with fixed set of tables. While schema-less systems,\\nsuch as document-oriented databases, offer more simplicity, their\\none-record-at-a-time processing comes at the cost of performance. In the\\nspectrum of performance and ease of use, Tenzir therefore [fills a\\ngap](/why-tenzir):\\n\\n![Structured vs.\\nDocument-Oriented](structured-vs-document-oriented.excalidraw.svg)\\n\\n:::info Eclectic & Super-structured Data\\n[Zed](https://amyousterhout.com/papers/zed_cidr23.pdf) has a type system similar\\nto Tenzir, with the difference that Zed associates types *with every single\\nvalue*. Unlike Zed, Tenzir uses a \\"data frame\\" abstraction and relies on\\nhomogeneous Arrow record batches of up to 65,535 rows.\\n:::\\n\\nIf the schema in a pipeline changes, we simply create a new batch of events. The\\nworst case for Tenzir is a ordered stream of schema-switching events, with every\\nevent having a new schema than the previous one. That said, even for those data\\nstreams we can efficiently build homogeneous batches when the inter-event order\\ndoes not matter significantly. Similar to predicate pushdown, Tenzir operators\\nsupport \\"ordering pushdown\\" to signal to upstream operators that the event order\\nonly matters intra-schema but not inter-schema. In this case we transparently\\ndemultiplex a heterogeneous stream into *N* homogeneous streams, each of which\\nyields batches of up to 65k events. The [`import`](/next/operators/import)\\noperator is an example of such an operator, and it pushes its ordering upstream\\nso that we can efficiently parse, say, a diverse stream of NDJSON records, such\\nas Suricata\'s EVE JSON or Zeek\'s streaming JSON.\\n\\nYou could call multi-schema dataflows *multiplexed* and there exist dedicated\\noperators to demultiplex a stream. As of now, this is hard-coded per operator.\\nFor example, [`to directory /tmp/dir write parquet`](/connectors/directory)\\ndemultiplexes a stream of events so that batches with the same schema go to the\\nsame Parquet file.\\n\\nThe diagram below illustrates the multi-schema aspect of dataflows for schemas\\nA, B, and C:\\n\\n![Multi-schema Example](multi-schema-example.excalidraw.svg)\\n\\nSome operators only work with exactly one instance per schema internally, such\\nas [`write`](/next/operators/write) when combined with the\\n[`parquet`](/formats/parquet), [`feather`](/formats/feather), or\\n[`csv`](/formats/csv) formats. These formats cannot handle multiple input\\nschemas at once. A demultiplexing operator like `to directory .. write <format>`\\nremoves this limitation by writing one file per schema instead.\\n\\nWe are having ideas to make this schema (de)multiplexing explicit with a\\n`per-schema` [operator modifier](/next/language/operator-modifiers) that you can\\nwrite in front of every operator. Similarly, we are going to add union types in\\nthe future, making it possible to convert a heterogeneous stream of structured\\ndata into a homogeneous one.\\n\\nIt\'s important to note that most of the time you don\'t have to worry about\\nschemas. They are there for you when you want to work with them, but it\'s often\\nenough to just specified the fields that you want to work with, e.g., `where\\nid.orig_h in 10.0.0.0/8`, or `select src_ip, dest_ip, proto`. Schemas are\\ninferred automatically in parsers, but you can also seed a parser with a schema\\nthat you define explicitly.\\n\\n## P4: Unified Live Stream Processing and Historical Queries\\n\\nSystems for stream processing and running historical queries have different\\nrequirements, and combining them into a single system can be a daunting\\nchallenge. But there is an architectural sweetspot at the right level of\\nabstraction where you can elegantly combine them. From a user persepctive, our\\ngoal was to seamlessly exchange the beginning of a pipeline to select the source\\nof the data, be it a historical or continuous one:\\n\\n![Unified Processing](unified-processing.excalidraw.svg)\\n\\nOur desired user experience for interacting with historical looks like this:\\n\\n1. **Ingest**: to persist data at a node, create a pipeline that ends with the\\n   [`import`](/next/operators/import) sink.\\n2. **Query**: to run a historical query, create a pipeline that begins with the\\n   [`export`](/next/operators/export) operator.\\n\\nFor example, to ingest JSON from a Kafka, you write `from kafka --topic foo |\\nimport`. To query the stored data, you write `export | where file == 42`. The\\nlatter example suggests that the pipeline *first* exports everything, and only\\n*then* starts filtering with `where`, performing a full scan over the stored\\ndata. But this is not what\'s happening. Our pipelines support **predicate\\npushdown** for every operator. This means that `export` receives the filter\\nexpression before it starts executing, enabling index lookups or other\\noptimizations to efficiently execute queries with high selectivity where scans\\nwould be sub-optimal.\\n\\nThe central insight here is to ensure that predicate pushdown (as well as other\\nforms of signalling) exist throughout the entire pipeline engine, and that the\\nengine can communicate this context to the storage engine.\\n\\nOur own storage engine is not a full-fledged database, but rather a thin\\nindexing layer over a set of Parquet/Feather files. The sparse indexes (sketch\\ndata structures, such as min-max synopses, Bloom filters, etc.) avoid full scans\\nfor every query. The storage engine also has a *catalog* that tracks evolving\\nschemas, performs expression binding, and provides a transactional interface to\\nadd and replace partitions during compaction.\\n\\nThe diagram below shows the main components of the database engine:\\n\\n![Database Architecture](database-architecture.excalidraw.svg)\\n\\nBecause of this transparent optimization, you can just exchange the source of a\\npipeline and switch between historical and streaming execution without degrading\\nperformance. A typical use case begins some exploratory data analysis involving\\na few `export` pipelines, but then would deploy the pipeline on streaming data\\nby exchanging the source with, say, `from kafka`.\\n\\nThe difference between `import` and `from file <path> read parquet` (or `export`\\nand `to file <path> write parquet`) is that the storage engine has the extra\\ncatalog and indexes, managing the complexity of dealing with a large set of raw\\nParquet files.\\n\\n:::info Delta, Iceberg, and Hudi?\\nWe kept the catalog purposefully simple to iterate fast and gain experience in a\\ncontrolled system, rather than starting Lakehouse-grade with [Delta\\nLake](https://delta.io/), [Iceberg](https://iceberg.apache.org/), or\\n[Hudi](https://hudi.apache.org/). We are looking forward to having the resources\\nto integrate with the existing lake management tooling.\\n:::\\n\\n## P5: Built-in Networking to Create Data Fabrics\\n\\n:::info Control Plane vs. Data Plane\\nThe term *data fabric* is woven into many meanings. From a Tenzir perspective,\\nthe set of interconnected pipelines through which data flows constitutes the\\n**data plane**, whereas the surrounding management platform at\\n[app.tenzir.com](https://app.tenzir.com) to control the nodes constitute the\\n**control plane**. When we refer to \\"data fabric\\" we mean to the data plane\\naspect.\\n:::\\n\\nTenzir pipelines have built-in network communication, allowing you to create a\\ndistributed fabric of dataflows to express intricate use cases. There are two\\ntypes of network connections: *implicit* and *explicit* ones:\\n\\n![Implicit vs. Explicit](implicit-vs-explicit-networking.excalidraw.svg)\\n\\nAn implicit network connection exists, for example, when you use the `tenzir`\\nbinary on the command line to run a pipeline that ends in\\n[`import`](/next/operators/import):\\n\\n```bash\\ntenzir \'load gcs bkt/eve.json\\n       | read suricata\\n       | where #schema != \\"suricata.stats\\"\\n       | import\\n       \'\\n```\\n\\nThis results in the following pipeline execution:\\n\\n![Import Networking](import-networking.excalidraw.svg)\\n\\nA historical query, like `export | where <expr> | to <connector>`, has the\\nnetwork connection at the other end:\\n\\n![Export Networking](export-networking.excalidraw.svg)\\n\\nTenzir pipelines are eschewing networking to minimize latency and maximize\\nthroughput. So we generally transfer ownership of operators between processes as\\nlate as possible to prefer local, high-bandwidth communication. For maximum\\ncontrol over placement of computation, you can override the automatic operator\\nlocation with the `local` and `remote` [operator\\nmodifiers](/next/language/operator-modifiers).\\n\\nThe above examples are implicit network connections because they\'re not visible\\nin the pipeline definition. An explicit network connection terminates a pipeline\\nas source or sink:\\n\\n![Pipeline Fabric](pipeline-fabric.excalidraw.svg)\\n\\nThis fictive data fabric above consists of a heterogeneous set of technologies,\\ninterconnected by pipelines. You can also turn any pipeline into an API using\\nthe [`serve`](/next/operators/serve) sink, effectively creating a dataflow\\nmicroservice that you can access with a HTTP client from the other side:\\n\\n![Serve Operator](serve.excalidraw.svg)\\n\\nBecause you have full control over the location where you run the pipeline, you\\ncan push it all the way to the \\"last mile.\\" This helps especially when there\\nare compliance and data residency concerns that must be properly addressed.\\n\\n## Summary\\n\\nWe\'ve presented for design principles that we found to be key enabler to extract\\nvalue out of data pipelines:\\n\\n1. Separating the different data processing concerns, it is possible to\\n   achieve high modularity and composability. Tenzir therefore has connectors,\\n   formats, and operators as central processing building blocks.\\n2. Typed operators make it possible to process multiple types of data in the\\n   same engine, avoiding the need to switch tools just because the pipeline\\n   engine has a narrow focus.\\n3. Multi-schema dataflows give us the best of structured and document-oriented\\n   engines. Coupled with schema inference, this creates a user experience where\\n   schemas are optional, but still can be applied when strict typing is needed.\\n4. Unifying live and historical data processing is the holy grail to covering a\\n   wide variety of workloads. Our engine offers a new way to combine the two\\n   with an intuitive language.\\n5. Built-in networking makes it possible to create data fabrics at ease.\\n   Spanning pipelines across multiple nodes, either implicitly or explicitly\\n   (via ZeroMQ, Kafka, AMQP, etc.), provides a powerful mechanism to meet the\\n   most intricate architectural requirements.\\n\\nTenzir pipelines embody all of these principles. Try it yourself with our free\\nCommunity Edition at [app.tenzir.com](https://app.tenzir.com)."},{"id":"/tenzir-v4.3","metadata":{"permalink":"/blog/tenzir-v4.3","source":"@site/blog/tenzir-v4.3/index.md","title":"Tenzir v4.3","description":"Exciting times, Tenzir v4.3 is out! The headlining feature is [Fluent","date":"2023-10-10T00:00:00.000Z","formattedDate":"October 10, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"operators","permalink":"/blog/tags/operators"},{"label":"observability","permalink":"/blog/tags/observability"},{"label":"fluent-bit","permalink":"/blog/tags/fluent-bit"},{"label":"json","permalink":"/blog/tags/json"},{"label":"yaml","permalink":"/blog/tags/yaml"},{"label":"labels","permalink":"/blog/tags/labels"}],"readingTime":6.595,"hasTruncateMarker":true,"authors":[{"name":"Jannis Christopher K\xf6hl","title":"Software Engineer","url":"https://github.com/jachris","email":"jannis@tenzir.com","imageURL":"https://github.com/jachris.png","key":"jachris"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Tenzir v4.3","authors":["jachris","mavam"],"date":"2023-10-10T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","operators","observability","fluent-bit","json","yaml","labels"]},"prevItem":{"title":"Five Design Principles for Building a Data Pipeline Engine","permalink":"/blog/five-design-principles-for-building-a-data-pipeline-engine"},"nextItem":{"title":"We Need to Talk About the Cost of Security Operations Infrastructure","permalink":"/blog/we-need-to-talk-about-the-cost-of-security-operations-infrastructure"}},"content":"Exciting times, Tenzir v4.3 is out! The headlining feature is [Fluent\\nBit][fluentbit] support with the `fluent-bit` [source][fluentbit-source] and\\n[sink][fluentbit-sink] operators. Imagine you can use all Fluent Bit connectors\\n*plus* what Tenzir already offers. What a treat!\\n\\n[fluentbit]: https://fluentbit.io/\\n[fluentbit-source]: /next/operators/fluent-bit\\n[fluentbit-sink]: /next/operators/fluent-bit\\n\\n![Tenzir v4.3](tenzir-v4.3.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Fluent Bit on Steroids\\n\\n[Fluent Bit][fluentbit] is a remarkable piece of open source software that\\noffers observability pipelines\u2014quite similar to Tenzir actually. That said, our\\ntarget audience is different: rather than targeting observability, we focus on\\nthe intersection of the security and data.\\n\\nBy bringing the two ecosystems together, you, dear user, benefit from the union\\nof features. Before diving into some examples, let\'s briefly compare the tech.\\nFluent Bit features [inputs][inputs] and [outputs][outputs] to get data in and\\nout of the ecosystem. These are equivalent to Tenzir\'s\\n[connectors](/connectors). Fluent Bit also has [parsers][parsers] that map to\\nthe equally named concept of Tenzir [parsers](/formats). Fluent Bit\'s\\n[filters][filters] would be implemented as *transformations* in Tenzir, i.e.,\\noperators that have a non-void input and output. The diagram illustrates these\\nrelationships:\\n\\n[inputs]: https://docs.fluentbit.io/manual/pipeline/inputs\\n[outputs]: https://docs.fluentbit.io/manual/pipeline/outputs\\n[parsers]: https://docs.fluentbit.io/manual/pipeline/parsers\\n[filters]: https://docs.fluentbit.io/manual/pipeline/filters\\n\\n![Tenzir vs. Fluent Bit](tenzir-vs-fluentbit.excalidraw.svg)\\n\\nIt\'s important to note that Tenzir pipelines separate I/O and computation.\\nConnectors do I/O and formats are responsible for (un)structuring data. The data\\npaths are symmetric in that a loader ships bytes to a parser that in turn\\nproduces events, and a printer accepts events and turns them in to bytes that\\na corresponding saver sends away.\\n\\nWe implemented the `fluent-bit` operator as a fusion of connector and format. We\\ndid not integrate Fluent Bit\'s powerful parser abstraction, as we have an\\nexisting framework in place for that. Similarly, we did not integrate Fluent\\nBit\'s filters, as we have a variety of transformation operators for that.\\n\\n### How do I use it?\\n\\nThe `fluent-bit` [source][fluentbit-source] and [sink][fluentbit-sink] operator\\nis where the action happens. They have the following syntax:\\n\\n```\\nfluent-bit <plugin> [<key=value>..]\\n```\\n\\nBoth operators are very similar to the `fluent-bit` command line tool, which\\nhas the usage `fluent-bit -i <input> -o <output> -p key=value`. In Tenzir, the\\nsource operator implies the options `-i` and the sink `-o`, so you don\'t have to\\nwrite them. Similarly, appending properties in the form of key-value pairs is\\nso common that you can omit the `-p` options.\\n\\nLet\'s walk through some examples. Say you want to sample three values with\\nFluent Bit\'s [`random`][random] input:\\n\\n[random]: https://docs.fluentbit.io/manual/pipeline/inputs/random\\n\\n```bash\\ntenzir \'fluent-bit random | head 3 | write json -c\'\\n```\\n\\nThis prints:\\n\\n```json\\n{\\"timestamp\\": \\"2023-09-23T07:56:47.957369\\", \\"message\\": {\\"rand_value\\": 8106944690543729752}}\\n{\\"timestamp\\": \\"2023-09-23T07:56:48.959997\\", \\"message\\": {\\"rand_value\\": 2072095294278847853}}\\n{\\"timestamp\\": \\"2023-09-23T07:56:49.959988\\", \\"message\\": {\\"rand_value\\": 5606209024700423100}}\\n```\\n\\nRegarding the framing: [Fluent Bit\'s event format][event-format] produces events\\nin the form of an array that can have one of two possible shapes:\\n\\n1. `[[TIMESTAMP, METADATA], MESSAGE]`\\n2. `[TIMESTAMP, MESSAGE]`\\n\\n[event-format]: https://docs.fluentbit.io/manual/concepts/key-concepts#event-format\\n\\nWe convert this into an event record with 3 fields:\\n\\n1. `timestamp`: the event timestamp\\n2. `metadata`: object with key-value pairs\\n3. `message`: arbitrary object with inferred schema\\n\\nThe field `metadata` is optional, as shown in the above example.\\n\\nMany Fluent Bit inputs perform network I/O. Here\'s a TCP socket example:\\n\\n```bash\\n# Terminal A\\ntenzir \'fluent-bit tcp\'\\n# Terminal B\\necho \'{\\"foo\\": {\\"bar\\": 42}}\' | nc 127.0.0.1 5170\\n```\\n\\nThis outputs in terminal A:\\n\\n```json\\n{\\n  \\"timestamp\\": \\"2023-09-23T09:35:10.623745\\",\\n  \\"message\\": {\\n    \\"foo\\": {\\n      \\"bar\\": 42\\n    }\\n  }\\n}\\n```\\n\\nLet\'s pick another input, [`opentelemetry`][opentelemetry]:\\n\\n[opentelemetry]: https://docs.fluentbit.io/manual/pipeline/inputs/opentelemetry\\n\\n```bash\\ntenzir \'fluent-bit opentelemetry\'\\n```\\n\\nThis opens a socket on port 4318 that you can send now telemetry to. Instead of\\n`curl`, we\'re using our own HTTPie-like [`http`](/connectors/http) connector to\\nissue a POST request:\\n\\n```bash\\ntenzir \'from http POST 127.0.0.1:4318/v1/logs resourceLogs:=[{\\"resource\\":{},\\"scopeLogs\\":[{\\"scope\\":{},\\"logRecords\\":[{\\"timeUnixNano\\":\\"1660296023390371588\\",\\"body\\":{\\"stringValue\\":\\"{\\\\\\"message\\\\\\":\\\\\\"dummy\\\\\\"}\\"},\\"traceId\\":\\"\\",\\"spanId\\":\\"\\"}]}]}]\'\\n```\\n\\nYou should then see:\\n\\n```json\\n{\\n  \\"timestamp\\": \\"2022-08-12T09:20:24.698112\\",\\n  \\"message\\": {\\n    \\"log\\": {\\n      \\"message\\": \\"dummy\\"\\n    }\\n  }\\n}\\n```\\n\\nMore powerful inputs mimic other applications, like [Splunk][splunk] or\\n[ElasticSearch][elasticsearch]. Want Tenzir to be like Splunk via Fluent Bit?\\nHere you go:\\n\\n[splunk]: https://docs.fluentbit.io/manual/pipeline/inputs/splunk\\n[elasticsearch]: https://docs.fluentbit.io/manual/pipeline/inputs/elasticsearch\\n\\n```bash\\ntenzir \'fluent-bit splunk\'\\n```\\n\\nYou just got a Splunk HEC API waiting for you at port 9880. This is one of the\\nmost amazing things about this integration. The entire Fluent Bit connector\\necosystem is now at your fingertips!\\n\\nThis extends to the outputs as well. Most mundanely, you can use the `stdout`\\noutput from Fluent Bit as follows:\\n\\n```bash\\ntenzir \'show operators | head 3 | fluent-bit stdout\'\\n```\\n\\nThis yields:\\n\\n```\\n[0] lib.0: [[1695494117.866096973, {}], {\\"name\\"=>\\"batch\\", \\"source\\"=>false, \\"transformation\\"=>true, \\"sink\\"=>false}]\\n[1] lib.0: [[1695494117.866101980, {}], {\\"name\\"=>\\"compress\\", \\"source\\"=>false, \\"transformation\\"=>true, \\"sink\\"=>false}]\\n[2] lib.0: [[1695494117.866103887, {}], {\\"name\\"=>\\"decapsulate\\", \\"source\\"=>false, \\"transformation\\"=>true, \\"sink\\"=>false}]\\n```\\n\\nWant to send Suricata alerts to Slack? Here is your pipeline:\\n\\n```\\nfrom file --follow eve.json\\n| where #schema == \\"suricata.alert\\"\\n| fluent-bit slack webhook=https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\\n```\\n\\nOr send \'em to Splunk by changing the sink to:\\n\\n```\\nfluent-bit splunk host=127.0.0.1 port=8088 tls=on tls.verify=off\\n```\\n\\nOh wait, Elastic? Here you go:\\n\\n```\\nfluent-bit es host=192.168.2.3 port=9200 index=my_index type=my_type\\n```\\n\\nHopefully the general pattern is clear now.\\n\\nFinally, there\'s a cool property of Fluent Bit: it\'s symmetric like Tenzir.\\nRemember how you can use ZeroMQ sockets to bridge pipelines?\\n\\n```bash\\n# Terminal A\\ntenzir \'from zmq\'\\n# Terminal B\\ntenzir \'show version | to zmq\'\\n```\\n\\nYou can do the same with Fluent Bit\'s *Forward* protocol:\\n\\n```bash\\n# Terminal A\\ntenzir \'fluent-bit forward\'\\n# Terminal B\\ntenzir \'show version | fluent-bit forward\'\\n```\\n\\n(We\'ll leave it to you to do the same with Kafka.)\\n\\n:::info Implementation: MsgPack vs. Arrow\\nFluent Bit uses [MsgPack](https://msgpack.org/) internally, a binary version of\\nJSON with a slightly richer set of types. Once a Fluent Bit input onboards\\ndata into the internal format, all operations compute on MsgPack. And before\\ndata exits Fluent Bit, it gets converted from MsgPack to the native format of\\nthe output.\\n\\nIncidentally, Tenzir also had an optional MsgPack implementation of its data\\nplane. However, we dropped the MsgPack encoding and switched to [Apache\\nArrow](https://arrow.apache.org) exclusively. The reason is that most of our\\nworkloads are analytical, where a columnar representation (especially with large\\nbatching) outperforms due data locality and the ability to tap into vectorized\\ncomputations. Moreover, our objective is to soon integrate natively with several\\ndata tools, such as DuckDB, pandas, polars, etc.\u2014all of which speak Arrow.\\n:::\\n\\nWant to try it yourself? Head over to [app.tenzir.com](https://app.tenzir.com)\\nwhere you start for free and manage Tenzir nodes and run Tenzir and Fluent Bit\\npipelines.\\n\\n## Tidbits\\n\\nBesides Fluent Bit, the team at Tenzir has been working on some other\\nnoteworthy improvements and features that we would like to share:\\n\\n### JSON Parser Improvements\\n\\nWe\'ve revamped our JSON parser to be a lot faster and more accurate in type\\ninference.\\n\\n![Tenzir v4.3 JSON Improvements](tenzir-v4.3-json-improvements.excalidraw.svg)\\n\\nSchema inference now supports empty records and empty lists. Previously both\\nwere indistinguishable from `null` values. This is best illustrated on an\\nexample:\\n\\n```json\\n{\\"foo\\": [], \\"bar\\": {}, \\"baz\\": \\"127.0.0.1\\"}\\n{\\"foo\\": [null], \\"bar\\": null, \\"baz\\": \\"::1\\"}\\n{\\"foo\\": null, \\"bar\\": {}, \\"baz\\": \\"localhost\\"}\\n```\\n\\nWith Tenzir v4.2, The fields `foo` and `bar` would\'ve been dropped from the\\ninput, and `baz` had the type `string` for all three events.\\n\\nWith Tenzir v4.3, `foo` is of type `list<null>`, `bar` of type `record {}`, and\\nbaz of type `ip` for the first two events, and of type `string` for the third.\\n\\n### YAML Format\\n\\nThe new [`yaml`](/next/formats/yaml) format supports reading and writing YAML\\ndocuments and document streams.\\n\\nFor example, you can now render the configuration of the current node as valid\\nYAML:\\n\\n```\\nshow config | write yaml\\n```\\n\\nThis yields:\\n\\n```yaml\\n---\\ntenzir:\\n  allow-unsafe-pipelines: true\\n  operators:\\n    suricata: \\"shell \'suricata -r /dev/stdin --set outputs.1.eve-log.filename=/dev/stdout --set logging.outputs.0.console.enabled=no\' | read suricata\\\\n\\"\\n    zeek: \\"shell \'eval \\\\\\"$(zkg env)\\\\\\" && zeek -r - LogAscii::output_to_stdout=T JSONStreaming::disable_default_logs=T JSONStreaming::enable_log_rotation=F json-streaming-logs\' | read zeek-json --no-infer\\\\n\\"\\n...\\n```\\n\\nAnother example, perhaps just a party tricks, is converting YAML to JSON:\\n\\n```bash\\ntenzir \'read yaml | write json\' < input.json\\n```\\n\\n### Pipeline Labels\\n\\nNodes now support setting labels for pipelines. This feature isn\'t yet enabled\\nin the app, but will be available soon for all nodes updated to v4.3 or newer."},{"id":"/we-need-to-talk-about-the-cost-of-security-operations-infrastructure","metadata":{"permalink":"/blog/we-need-to-talk-about-the-cost-of-security-operations-infrastructure","source":"@site/blog/we-need-to-talk-about-the-cost-of-security-operations-infrastructure/index.md","title":"We Need to Talk About the Cost of Security Operations Infrastructure","description":"In today\'s digital age, businesses are under immense pressure to bolster their","date":"2023-09-21T00:00:00.000Z","formattedDate":"September 21, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"siem","permalink":"/blog/tags/siem"},{"label":"cost","permalink":"/blog/tags/cost"},{"label":"dataops","permalink":"/blog/tags/dataops"},{"label":"secdataops","permalink":"/blog/tags/secdataops"},{"label":"finops","permalink":"/blog/tags/finops"}],"readingTime":4.15,"hasTruncateMarker":true,"authors":[{"name":"Oliver Rochford","title":"Chief Futurist","url":"https://github.com/oliverrochford","email":"oliver@tenzir.com","imageURL":"https://github.com/oliverrochford.png","key":"oliverrochford"}],"frontMatter":{"authors":"oliverrochford","date":"2023-09-21T00:00:00.000Z","tags":["tenzir","pipelines","siem","cost","dataops","secdataops","finops"],"comments":true},"prevItem":{"title":"Tenzir v4.3","permalink":"/blog/tenzir-v4.3"},"nextItem":{"title":"Tenzir v4.2","permalink":"/blog/tenzir-v4.2"}},"content":"In today\'s digital age, businesses are under immense pressure to bolster their\\ncybersecurity. Understanding the financial implications of security tools is\\nvital to ensure optimal ROI through risk reduction and breach resilience. This\\nis particularly true for consumption-based security solutions like Security\\nInformation and Event Management (SIEM).\\n\\n![capex-vs-opex](capex-vs-opex.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## The SIEM Cost Challenge\\n\\nHaving worked both as a SIEM consultant and an industry analyst, I\'ve observed\\nthat SIEM costs are a recurring point of contention. As a consequence, SIEM\\nsolutions, the cornerstone of many security operations programs, have garnered a\\nreputation for being a financial black hole.\\n\\nAccurately forecasting SIEM costs has remained elusive. Given that a typical\\n[SIEM takes over 6 months to deploy][panther], predicting its data consumption a\\nyear in advance is speculative. Even vendors sometimes fall short in providing\\nrealistic cost estimates.\\n\\n[panther]: https://panther.com/wp-content/uploads/2023/01/State-of-SIEM-2021.pdf\\n\\nSeveral factors contribute to this unpredictability: outdated benchmark data,\\nscope changes, the evolving threat landscape, and rapid digitalization. These\\nvariables often lead to unforeseen price hikes post-deployment, catching many\\nbuyers off guard.\\n\\n## SIEM Pricing: Outdated and Ineffective\\n\\nTraditional SIEM pricing models haven\'t evolved in tandem with the explosion in\\ndata volumes. While security teams now handle data ranging from Megabytes to\\nPetabytes, SIEM licensing remains anchored in a Gigabyte-centric world.\\n\\nMoreover, the true value of the vast amounts of security data remains ambiguous.\\nUntil a breach or intrusion is investigated, it\'s challenging to determine the\\nsignificance of the collected data. While the probability that the data is still\\nrequired decreases over time, we can\u2019t always be sure when it will reach 0.\\n\\n## Quantifying the Value of Security Data\\n\\nThe amount of security data an organization generates doesn\'t always correlate\\nwith its revenue. This discrepancy complicates the task of assessing the\\nbusiness value of security data. Survey data further highlights a disconnect\\nbetween the perceived and actual value of SIEM systems, with many organizations\\nlamenting rising costs and underwhelming features\\n\\nSecurity teams are grappling with [rising costs][techresearch], [underutilized\\nfeatures][panther], and a [lack of comprehensive\\ncoverage][cardinalops], with one study stating that [over 40% think they are\\noverpaying for their SIEM, and more than 50% unhappy with their current SIEM\\nproviders][panther].\\n\\n[techresearch]: https://techresearchonline.com/wp-content/uploads/2022/03/SIEM_Shift_How_the_Cloud_is_Transforming_Security_Operations_US_20211007.pdf\\n[cardinalops]: https://f.hubspotusercontent00.net/hubfs/7289101/CardinalOps%20Quantifying%20the%20Threat%20Coverage%20Gap.pdf\\n\\n## A Shift in Buyer Behavior\\n\\nRather than just endlessly expanding security budgets to combat escalating\\ncosts, organizations are adopting various strategies:\\n\\n- **Limiting Coverage**: Some are narrowing their security monitoring scope,\\n  focusing primarily on compliance. However, this approach can compromise threat\\n  visibility and increase vulnerability.\\n- **Adopting XDR**: Others are transitioning to Extended Detection and Response\\n  (XDR) solutions, prioritizing in-depth analysis over breadth. But as XDR gains\\n  traction, it may inherit SIEM\'s cost challenges.\\n- **Building Security Data Lakes**: These are becoming increasingly popular due\\n  to their cost-effectiveness and advanced analytical capabilities. However,\\n  transitioning to a data lake doesn\'t guarantee reduced consumption. Many\\n  organizations will find they are swapping Capex for Opex. Moreover, while data\\n  lakes offer certain advantages, they can\'t fully replace enterprise SIEMs.\\n\\n## Future-proofing Security Operations for Automation and AI\\n\\nEven with the improved cost efficiencies and economies of scale achieved by\\nusing security data lakes and cloud computing, we are beginning to hit\\naffordability limits again.\\n\\nThe integration of new data-intensive tools and technologies, including machine\\nlearning and AI, like large language models, further intensifies this demand.\\nWhile these advances promise enhanced cybersecurity capabilities, they\\nsimultaneously usher in a new set of financial challenges that the industry will\\nhave to grapple with. Technological advancements have made it feasible to\\nprocess vast data troves, but the question remains: is it economical?\\n\\nFinding the precarious balance between achieving cost efficiencies and\\nmaintaining robust security resilience is the conundrum facing cybersecurity\\nleaders. What they need to be able to make informed decisions is a comprehensive\\nunderstanding of these costs and their implications, so that they can\\nstrategically navigate these challenges.\\n\\n## Security FinOps with Tenzir Security Data Pipelines\\n\\nAt Tenzir, we aim to redefine how organizations manage security operations\\nexpenses. Our security data pipelines address core challenges associated with\\noptimizing SIEM, security data lake, and cloud costs.\\n\\nOur pipelines enhance data flow and processing by normalizing data formats to\\nreduce complexity and redundancy, performing in-stream enrichments, and applying\\npowerful reshaping to optimally prepare the data for consumption. By optimizing\\ndata preprocessing down to the collection point, we curtail unnecessary SIEM\\ningestion and cloud compute costs. We transfer many workloads to the edge that\\nwere previously cost-inefficiently executed centrally. By scaling vertically\\nacross cores and pipelines, and horizontally across nodes, organizations can\\nadapt to variable environments and data loads, ensuring deployment flexibility\\nand cost-efficiency.\\n\\nFurthermore, Tenzir ensures data quality, a vital component for effective\\nDataOps and automation. By filtering out redundant data and prioritizing based\\non significance, you can ensure efficient resource allocation. Tenzir\'s\\ninstrumented data flows provide clear insights into data usage, facilitating\\ntransparent cost benchmarking.\\n\\nDiscover more about our features and benefits in our [solution\\nbrief](https://tenzir.com/solution-brief.pdf) and free whitepaper on\\n[optimizing SIEM, Cloud and data costs using\\nTenzir](https://tenzir.com/whitepaper.pdf).\\n\\nStart using Tenzir right away at [app.tenzir.com](https://app.tenzir.com)."},{"id":"/tenzir-v4.2","metadata":{"permalink":"/blog/tenzir-v4.2","source":"@site/blog/tenzir-v4.2/index.md","title":"Tenzir v4.2","description":"We\'ve just released Tenzir v4.2 that introduces two new connectors: S3 and","date":"2023-09-19T00:00:00.000Z","formattedDate":"September 19, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"connectors","permalink":"/blog/tags/connectors"},{"label":"s3","permalink":"/blog/tags/s-3"},{"label":"gcs","permalink":"/blog/tags/gcs"},{"label":"zmq","permalink":"/blog/tags/zmq"}],"readingTime":6.91,"hasTruncateMarker":true,"authors":[{"name":"Daniel Kostuj","title":"Software Engineer","url":"https://github.com/dakostu","email":"daniel@tenzir.com","imageURL":"https://github.com/dakostu.png","key":"dakostu"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Tenzir v4.2","authors":["dakostu","mavam"],"date":"2023-09-19T00:00:00.000Z","tags":["release","pipelines","connectors","s3","gcs","zmq"]},"prevItem":{"title":"We Need to Talk About the Cost of Security Operations Infrastructure","permalink":"/blog/we-need-to-talk-about-the-cost-of-security-operations-infrastructure"},"nextItem":{"title":"Tenzir v4.1","permalink":"/blog/tenzir-v4.1"}},"content":"We\'ve just released Tenzir v4.2 that introduces two new connectors: [S3][s3] and\\n[GCS][gcs] for interacting with blob storage and [ZeroMQ][zeromq] for writing\\ndistributed multi-hop pipelines. There\'s also a new [`lines`][lines] parser for\\neasier text processing and a bunch of PCAP quality-of-life improvements.\\n\\n[s3]: https://aws.amazon.com/s3/\\n[gcs]: https://cloud.google.com/storage\\n[zeromq]: https://zeromq.org/\\n[lines]: /formats/lines\\n\\n![Tenzir v4.2](tenzir-v4.2.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## S3 Saver & Loader\\n\\nThe new [`s3`](/connectors/s3) connector hooks up Tenzir to the vast data masses\\non [Amazon S3](https://aws.amazon.com/s3/) and S3-compatible object storage\\nsystems. With the `s3` loader, you can access objects on S3 buckets, assuming\\nyou have the proper credentials provided:\\n\\n```bash\\ntenzir \'from s3 s3://bucket/mystuff/file.json\'\\n```\\n\\nInternally, we are using Arrow\'s filesystem abstraction for establishing\\nconnections. This abstraction already handles AWS\'s default credentials provider\\nchain. If you have set up your AWS account in this chain, then you don\'t need to\\nworry about setting it up again in config files or similar formats.\\n\\nS3 buckets can also be public, meaning you don\'t need any specific credentials\\nto access the objects therein. AWS offers tons of such public (read-only)\\nbuckets with scientific data on their [Marketplace][marketplace]. Tenzir can\\nalso consume public read-only data\u2014for example, e.g., some [population density &\\ndemographic estimate data][density]:\\n\\n[marketplace]: https://aws.amazon.com/marketplace/search/results?trk=8384929b-0eb1-4af3-8996-07aa409646bc&sc_channel=el&FULFILLMENT_OPTION_TYPE=DATA_EXCHANGE&CONTRACT_TYPE=OPEN_DATA_LICENSES&DATA_AVAILABLE_THROUGH=S3_OBJECTS&PRICING_MODEL=FREE&filters=FULFILLMENT_OPTION_TYPE%2CCONTRACT_TYPE%2CDATA_AVAILABLE_THROUGH%2CPRICING_MODEL\\n[density]: https://aws.amazon.com/marketplace/pp/prodview-jf2hjpr2mrj4m?sr=0-2&ref_=beagle&applicationId=AWSMPContessa#overview\\n\\n```\\nload s3 dataforgood-fb-data/csv/month=2019-06/country=VGB/type=children_under_five/VGB_children_under_five.csv.gz\\n| decompress gzip\\n| read csv\\n```\\n\\nLet\'s combine this with `aws s3 ls` to receive all [Amazon product Q&A humor\\ndetection data][humor]:\\n\\n[humor]: https://aws.amazon.com/marketplace/pp/prodview-b53zm25dl3jcc?sr=0-3&ref_=beagle&applicationId=AWSMPContessa#overview\\n\\n```bash\\naws s3 ls --no-sign-request --recursive humor-detection-pds/ |\\n  awk \'{print $4}\' |\\n  grep \\"\\\\.csv\\" |\\n  xargs -I {} tenzir \\"from s3 humor-detection-pds/{} read csv\\"\\n```\\n\\nThe original CSV data is a bit unpolished, e.g., there are line breaks\\nand superfluous commas in the middle of some values. Tenzir\'s `csv` parser\\nwill ignore those lines, but the rest of the data is at your fingertips.\\n\\nThe `s3` writer uploads the pipeline output to an object in the bucket:\\n\\n```bash\\ntenzir \\"export | to s3 s3://mybucket/folder/ok.json\\"\\n```\\n\\nYou can provide options to an S3 as [URI query parameters][uri]:\\n\\n> For S3, the options that can be included in the URI as query parameters are\\n> `region`, `scheme`, `endpoint_override`, `access_key`, `secret_key`,\\n> `allow_bucket_creation`, and `allow_bucket_deletion`.\\n\\n[uri]: https://arrow.apache.org/docs/10.0/r/articles/fs.html#uri-options\\n\\nThe most exciting of these options would be `endpoint_override`, as it allows\\nyou to connect to different endpoints of other S3-compatible storage systems:\\n\\n```\\nfrom s3 s3://examplebucket/test.json?endpoint_override=s3.us-west.mycloudservice.com\\n```\\n\\nThe `s3` connector is a huge step for Tenzir\'s capability to interact with blob\\nstorage. Our list of connectors is continuously growing and our modular\\nframework allows for cranking out many more at ease. More connectors, more data,\\nmore information, more value!\\n\\n## Google Cloud Storage (GCS) Connector\\n\\nSimilar to the `s3` connector we added the [`gcs`](/connectors/gcs) connector\\nthat interfaces to [Google Cloud Storage\\n(GCS)](https://cloud.google.com/storage).\\n\\nThe connector tries to retrieve the appropriate credentials using Google\'s\\n[Application Default Credentials](https://google.aip.dev/auth/4110). This means\\nyou can use the connector conveniently to read from or write to a storage\\nbucket:\\n\\n```\\nfrom gcs gs://bucket/path/to/file\\nto gcs gs://bucket/path/to/file\\n```\\n\\nAs with `s3`, you can also use override the default endpoint and other options\\nby passing URI query parameters. Have a look at the [connector\\ndocumentation](/connectors/gcs) for further details.\\n\\n## ZeroMQ Saver & Loader\\n\\nThe new [`zmq`](/connectors/zmq) connector makes it easy to interact with the\\nraw bytes in [ZeroMQ][zeromq] messages. We model the `zmq` *loader* as\\nsubscriber with a `SUB` socket, and the *saver* as a publisher with the `PUB`\\nsocket:\\n\\n![ZeroMQ Connector](zeromq-connector.excalidraw.svg)\\n\\nWhat\'s nice about ZeroMQ is that the directionality of connection establishment\\nis independent of the socket type. So either end can bind or connect. We opted\\nfor the subscriber to connect by default, and the publisher to bind. You can\\noverride this with the `--bind` and `--connect` flags.\\n\\nEven though we\'re using a lossy `PUB`-`SUB` socket pair, we\'ve added a thin\\nlayer of reliability in that a Tenzir pipeline won\'t send or receive ZeroMQ\\nmessages before it has at least one connected socket.\\n\\nWant to exchange and convert events with two single commands? Here\'s how you\\npublish JSON and continue as CSV on the other end:\\n\\n```bash\\n# Publish some data via a ZeroMQ PUB socket:\\ntenzir \'show operators | to zmq write json\'\\n# Subscribe to it in another process\\ntenzir \'from zmq read json | write csv\'\\n```\\n\\nYou can also work with operators that use types. Want to send away chunks of\\nnetwork packets to a remote machine? Here you go:\\n\\n```bash\\n# Publish raw bytes:\\ntenzir \'load nic eth0 | save zmq\'\\n# Tap into the raw feed at the other end and start parsing:\\ntenzir \'load zmq | read pcap | decapsulate\'\\n```\\n\\nNeed to expose the source side of a pipeline as a listening instead of\\nconnecting socket? No problem:\\n\\n```bash\\n# Bind instead of connect with the ZeroMQ SUB socket:\\ntenzir \'from zmq --bind\'\\n```\\n\\nThese examples show the power of composability: Tenzir operators work with both\\nbytes and events, enabling in-flight reshaping, format conversation, or simply\\ndata shipping at ease.\\n\\n## HTTP and FTP Loader\\n\\nWe\'ve added a new round of loaders for HTTP and FTP, named `http`, `https`,\\n`ftp`, and `ftps`. This makes it a lot easier to pull data into a pipeline that\\nlives at a remote web or file server. No more `shell curl` shenanigans!\\n\\nWe modeled the `http` and `https` loaders after [HTTPie](https://httpie.io/),\\nwhich comes with an expressive and intuitive command-line syntax. We recommend\\nto study the [HTTPie documentation](https://httpie.io/docs/cli/examples) to\\nunderstand the full extent of the command-line interface. In many cases, you can\\nperform an *exact* copy of the HTTPie command line and use it drop-in with the\\nHTTP loader, e.g., the invocation\\n\\n```bash\\nhttp PUT pie.dev/put X-API-Token:123 foo=bar\\n```\\n\\nbecomes\\n\\n```\\nfrom http PUT pie.dev/put X-API-Token:123 foo=bar\\n```\\n\\nMore generally, if your HTTPie command line is `http X` then you can write `from\\nhttp X` to obtain an event stream or `load http X` for a byte stream. (Note that\\nwe have only the parts of the HTTPie syntax most relevant to our users.)\\n\\nInternally, we rely on [libcurl](https://curl.se/libcurl/) to perform the actual\\nfile transfer. It is noteworthy that libcurl supports *a lot* of protocols:\\n\\n> libcurl is a free and easy-to-use client-side URL transfer library, supporting\\n> DICT, FILE, FTP, FTPS, GOPHER, GOPHERS, HTTP, HTTPS, IMAP, IMAPS, LDAP, LDAPS,\\n> MQTT, POP3, POP3S, RTMP, RTMPS, RTSP, SCP, SFTP, SMB, SMBS, SMTP, SMTPS,\\n> TELNET and TFTP. libcurl supports SSL certificates, HTTP POST, HTTP PUT, FTP\\n> uploading, HTTP form based upload, proxies, HTTP/2, HTTP/3, cookies,\\n> user+password authentication (Basic, Digest, NTLM, Negotiate, Kerberos), file\\n> transfer resume, http proxy tunneling and more!\\n\\n[Let us know](/discord) if you have use cases for any of these. Let\'s take a\\nlook at some more that you can readily work with.\\n\\nDownload and process a [CSV](/formats/csv) file:\\n\\n```\\nfrom http http://example.org/file.csv read csv\\n```\\n\\nProcess a Zstd-compressed [Zeek TSV](/formats/zeek-tsv) file:\\n\\n```\\nload https https://example.org/gigantic.log.zst\\n| decompress zstd\\n| read zeek-tsv\\n```\\n\\nImport a [CEF](/formats/cef) log from an FTP server into a Tenzir node:\\n\\n```\\nload ftp ftp://example.org/cef.log read cef\\n| import\\n```\\n\\n## Lines Parser\\n\\nThe new [`lines`][lines] parser splits its input at newline characters and\\nproduces events with a single field representing the line. This parser is\\nespecially useful for onboarding line-based text files into pipelines.\\n\\nThe `-s|--skip-empty` flags ignores empty lines. For example, read a text file\\nas follows:\\n\\n```\\nfrom file /tmp/test.txt read lines --skip-empty\\n```\\n\\n## Concatenating PCAPs\\n\\nThe [`pcap`](/formats/pcap) parser can now read concatenated PCAP files,\\nallowing you to easily process large amounts of trace files. This comes\\nespecially handy on the command line:\\n\\n```bash\\ncat *.pcap | tenzir \'read pcap\'\\n```\\n\\nThe [`nic`](/connectors/nic) loader has a new flag `--emit-file-headers` that\\nprepends a PCAP file header for every batch of bytes that it produces, yielding\\na stream of concatenated PCAP files. This gives rise to creative use cases\\ninvolving packet shipping. For example, to ship blocks of packets as \\"micro\\ntraces\\" via 0mq, you could do:\\n\\n```\\nload nic eth0\\n| save zmq\\n```\\n\\nThis creates 0mq PUB socket where subscribes can come and go. Each 0mq message\\nis a self-contained PCAP trace, which avoids painful resynchronization logic.\\nYou can consume this feed with a remote subscriber:\\n\\n```\\nload zmq\\n| read pcap\\n```\\n\\nFinally, we also made it easier to identify available network interfaces when\\nusing the `nic` loader: `show nics` now returns a list of available interfaces."},{"id":"/tenzir-v4.1","metadata":{"permalink":"/blog/tenzir-v4.1","source":"@site/blog/tenzir-v4.1/index.md","title":"Tenzir v4.1","description":"After our successful launch of app.tenzir.com of Tenzir v4.0 at","date":"2023-08-31T00:00:00.000Z","formattedDate":"August 31, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"operators","permalink":"/blog/tags/operators"},{"label":"app","permalink":"/blog/tags/app"}],"readingTime":3.815,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Tenzir v4.1","authors":["dominiklohmann"],"date":"2023-08-31T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","pipelines","operators","app"]},"prevItem":{"title":"Tenzir v4.2","permalink":"/blog/tenzir-v4.2"},"nextItem":{"title":"A First Look at ES|QL","permalink":"/blog/a-first-look-at-esql"}},"content":"After our successful launch of [app.tenzir.com][tenzir-app] of Tenzir v4.0 at\\nBlack Hat, [the new v4.1 release][github-release] continues with several\\nenhancements based on early feedback. We bring to you a (i) new mechanism to\\npause pipelines, (ii) a new operator to match Sigma rules, (iii) new operators\\nfor in-pipeline (de)compression, and (iv) a revamp of the `show` operator.\\n\\n[github-release]: https://github.com/tenzir/vast/releases/tag/v4.1.0\\n[tenzir-app]: https://app.tenzir.com\\n\\n![Tenzir v4.1](tenzir-v4.1.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Pausing Pipelines\\n\\nTenzir now supports pausing pipelines. Previously, users were able to deploy and\\nstart pipelines, and to let them run until they completed (or failed) or\\nmanually stopped. With new paused state, we now have the following five possible\\nstates of a pipeline:\\n\\n- *Created*: The pipeline was created, but never started\\n- *Running*: The pipeline is processing data\\n- *Paused*: A running pipeline has been suspended but still retains its\\n  in-memory state\\n- *Stopped*: A pipeline is not running and has no in-memory state\\n- *Failed*: The pipeline stopped unexpectedly\\n\\nPausing suspends a pipeline in an instant, causing its execution to stop. When a\\npaused pipeline is started, it resumes right where it left off as opposed to\\nrestarting all the way from the beginning.\\n\\nWe\'re also making use of this feature in the app with a new pause action button.\\nA new tab bar at the top makes it easy to see all pipeline states at a glance.\\nThese features will be enabled in the coming days in the app. Here\'s a sneak\\npreview of a mock from our designer:\\n\\n![Pause Feature Mock](pause-mock.png)\\n\\nYou can also pause a pipeline through the API. Use the [/pipeline/update\\nendpoint][update-endpoint] with the new `pause` action to suspend a pipeline.\\nThe `start` action resumes a pipeline that is currently paused.\\n\\n[update-endpoint]: https://docs.tenzir.com/api#/paths/~1pipeline~1update/post\\n\\n## Sigma Operator\\n\\nThe experimental [`sigma` operator](/next/operators/sigma) executes [Sigma\\nrules][sigma-github] on its input and outputs matching events. The operator can\\nwork both on files and on directories of rules. Rule directories may be updated\\nwhile the operator is running, so that adding a new rule to an already deployed\\npipeline is as simple as dropping a Sigma rule into the configured directory.\\n\\nConsider the `sigma` operator as one concrete instance of security content\\nexecution that we enable live and retrospectively. For example, you can perform\\nhistorical matching via `export | sigma` and streaming execution over a Kafka\\nsource via `from kafka --topic events | sigma`. Now that we have the capability\\nin place, we are working on a unified interface to live and retro matching.\\n\\n[sigma-github]: https://github.com/SigmaHQ/sigma\\n\\n## Show Operator\\n\\nThe experimental [`show` operator](/next/operators/show) supersedes the\\n`version` operator. Use `show <aspect>` to show various aspects of a Tenzir\\nnode. The following aspects are currently available:\\n\\n- `build`: shows compile-time build information.\\n- `config`: shows all current configuration options.\\n- `connectors`: shows all available [connectors][connectors-docs].\\n- `dependencies`: shows information about build-time dependencies.\\n- `fields`: shows all fields of existing tables at a remote node.\\n- `formats`: shows all available [formats][formats-docs].\\n- `operators`: shows all available [operators][operators-docs].\\n- `partitions`: shows all table partitions of a remote node.\\n- `pipelines`: shows all managed pipelines of a remote node.\\n- `plugins`: shows all loaded plugins.\\n- `types`: shows all known types at a remote node.\\n- `version`: shows the Tenzir version.\\n\\nThis enables powerful introspection use-cases. Here are some examples that we\\nfound useful at Tenzir.\\n\\nShow all running pipelines with an ingress of over 10 MiB:\\n\\n```\\nshow pipelines\\n| where total.ingress.num_approx_bytes > 10 Mi\\n```\\n\\nShow the memory usage of the node\'s catalog by schema in descending order:\\n\\n```\\nshow partitions\\n| summarize memory_usage=sum(memory_usage) by schema\\n| sort memory_usage desc\\n```\\n\\nPrint the configured endpoint of the node (returns `null` if not customized):\\n\\n```\\nshow config\\n| put tenzir.endpoint\\n```\\n\\n## Compress and Decompress Operators\\n\\nThe [`compress`][compress-docs] and [`decompress`][decompress-docs] operators\\nmake it easy to read and write compressed data. The operator uses Apache Arrow\'s\\ncompression utilities under the hood, and transparently supports all options\\nthat Apache Arrow supports for streaming compression. The currently supported\\ncodecs are `brotli`, `bz2`, `gzip`, `lz4`, and `zstd`.\\n\\nFor example, the following pipeline creates a Gzip-compressed NDJSON file that\\ncontains all events that were previously imported at the node:\\n\\n```\\nexport\\n| write json --compact-output\\n| compress gzip\\n| save file /tmp/backup.json.gz\\n```\\n\\n[compress-docs]: /next/operators/compress\\n[decompress-docs]: /next/operators/decompress\\n\\n## Small Things\\n\\nWe\'re constantly polishing the node\'s pipeline execution engine, and improving\\nthe app\'s usability. Since the last release, we\'ve improved pipeline execution\\nto make slow pipelines return their first results faster, and have improved the\\nrendering of the Explorer\'s results table for small result sets. The inline help\\nin the Explorer\'s pipeline editor is now more reliable."},{"id":"/a-first-look-at-esql","metadata":{"permalink":"/blog/a-first-look-at-esql","source":"@site/blog/a-first-look-at-esql/index.md","title":"A First Look at ES|QL","description":"Elastic just released their new pipeline query language called","date":"2023-08-29T00:00:00.000Z","formattedDate":"August 29, 2023","tags":[{"label":"esql","permalink":"/blog/tags/esql"},{"label":"elastic","permalink":"/blog/tags/elastic"},{"label":"tql","permalink":"/blog/tags/tql"},{"label":"kusto","permalink":"/blog/tags/kusto"},{"label":"splunk","permalink":"/blog/tags/splunk"},{"label":"spl","permalink":"/blog/tags/spl"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"language","permalink":"/blog/tags/language"}],"readingTime":7.35,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"A First Look at ES|QL","authors":"mavam","date":"2023-08-29T00:00:00.000Z","last_update":"2023-12-12T00:00:00.000Z","tags":["esql","elastic","tql","kusto","splunk","spl","pipelines","language"],"comments":true},"prevItem":{"title":"Tenzir v4.1","permalink":"/blog/tenzir-v4.1"},"nextItem":{"title":"Slash Your SIEM, Cloud, and Data Costs with Tenzir Security Data Pipelines","permalink":"/blog/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines"}},"content":"Elastic [just released][esql-blog] their new pipeline query language called\\n**ES|QL**. This is a conscious attempt to consolidate the language zoo in the\\nElastic ecosystem\\n([queryDSL](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html),\\n[EQL](https://www.elastic.co/guide/en/elasticsearch/reference/current/eql.html),\\n[KQL](https://www.elastic.co/guide/en/kibana/current/kuery-query.html),\\n[SQL](https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-sql.html),\\n[Painless](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html),\\n[Canvas/Timelion](https://www.elastic.co/guide/en/kibana/current/timelion.html)).\\nElastic said that they worked on this effort for over a year. The\\n[documentation][esql-docs] is still sparse, but we still tried to read between\\nthe lines to understand what this new pipeline language has to offer.\\n\\n[esql-blog]: https://www.elastic.co/blog/elasticsearch-query-language-esql\\n[esql-docs]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql.html\\n\\n![ESQL](esql.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\nThe reason why we are excited about this announcement is because we have *also*\\ndesigned and implemented a pipeline language over the past 8 months that we just\\n[launched at BlackHat](/blog/introducing-tenzir-security-data-pipelines). First,\\nwe see the release of ES|QL as a confirmation for pipeline model. In our [blog\\npost about Splunk\'s Search Processing Language\\n(SPL)](/blog/tenzir-for-splunk-users), we briefly mentioned why SQL might not be\\nthe best choice for analysts, and why thinking about one operator at a time\\nprovides an easier user experience. Second, we\'d like to look under the hood of\\nES|QL to compare and reflect on our own [Tenzir Query Language\\n(TQL)](/language).\\n\\n## Walk-through by Example\\n\\nSo, ES|QL, how does it feel?\\n\\n```\\n  FROM employees\\n| EVAL hired_year = TO_INTEGER(DATE_FORMAT(hire_date, \\"YYYY\\"))\\n| WHERE hired_year > 1984\\n| STATS avg_salary = AVG(salary) BY languages\\n| EVAL avg_salary = ROUND(avg_salary)\\n| EVAL lang_code = TO_STRING(languages)\\n| ENRICH languages_policy ON lang_code WITH lang = language_name\\n| WHERE NOT IS_NULL(lang)\\n| KEEP avg_salary, lang\\n| SORT avg_salary ASC\\n| LIMIT 3\\n```\\n\\nThis syntax reads very straight-forward. Splunk users will immediately grasp\\nwhat it does, as there is a remarkable similarity in operator naming. Let\'s go\\nthrough each pipeline operator individually:\\n\\n- [`FROM`][esql-from] generates a table with up to 10k rows from a data stream,\\n  index, or alias. We asked ourselves why there is a hard-baked 10k limit?\\n  Shouldn\'t that be the job of [`LIMIT`][esql-limit]? The limit feels a\\n  technical limitation rather than a conscious design decision. In TQL, we have\\n  unbounded streams but also follow the single responsibility principle: one\\n  operator has exactly one job.\\n- [`EVAL`][esql-eval] appends new or replaces existing columns. We named this\\n  operator [`extend`](/next/operators/extend) because we found the\\n  Splunk-inspired command name \\"eval\\" too generic for this use case.[^1]\\n- [`WHERE`][esql-where] filters the input with an expression. We have the same\\n  [`where`](/next/operators/where) in TQL.\\n- [`STATS`][esql-stats] groups its input via `BY` and applies aggregation\\n  functions on select fields of each group.  Elastic went with Splunk\\n  nomenclature for this central operation, perhaps also to make the transition\\n  from Splunk to Elastic as easy as possible.\\n- [`ENRICH`][esql-enrich] adds data from existing indexes. It\'s effectively a\\n  join operation, and the `ON` keywords makes it possible to select the join\\n  field. Interestingly, the word \\"join\\" doesn\'t appear on the documentation. We\\n  hypothesize that this was a conscious choice, as a database join may feel\\n  intimidating for beginning and intermediate users.\\n- [`KEEP`][esql-keep] selects a set of columns from the input and drops all\\n  others. It is the inverse of [`DROP`][esql-drop]. In TQL, we call these\\n  projection operators [`select`](/next/operators/select) and also\\n  [`drop`](/next/operators/drop).\\n- [`SORT`][esql-sort]\\n  sorts rows by one or more fields. `SORT height DESC, first_name ASC` sorts the\\n  field `height` in descending order and the field `first_name` in ascending\\n  order. The syntax of our [`sort`](/next/operators/sort) is identical.\\n  Controlling the position of null values works with `NULLS FIRST` and `NULLS\\n  LAST`. In TQL, we went Kusto-like with `nulls-first` and `nulls-last`.\\n- [`LIMIT`][esql-limit] restricts the number of output rows. In TQL, we have\\n  [`head`](/next/operators/head) and [`tail`](/next/operators/tail) for this\\n  purpose.\\n\\n[esql-from]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-from.html\\n[esql-eval]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-eval.html\\n[esql-where]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-where.html\\n[esql-stats]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-stats-by.html\\n[esql-enrich]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-enrich.html\\n[esql-keep]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-keep.html\\n[esql-drop]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-drop.html\\n[esql-sort]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-sort.html\\n[esql-limit]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-limit.html\\n[^1]: We took the name `extend` from Kusto. In general, we find that Kusto has\\n    very self-descriptive operator names. During the design of TQL, we compared\\n    many different languages and often favored Kusto\'s choice of name.\\n\\n## Sources, Transformations, ... but Sinks?\\n\\nES|QL differentiates two types of commands (which we call *operators* in TQL):\\n\\n1. [Source commands](https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-source-commands.html)\\n2. [Processing commands](https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-processing-commands.html)\\n\\nIn TQL, an operator is a *source*, a *transformation*, or a *sink*. Some\\noperators can be of multiple categories, like [`shell`](/next/operators/shell).\\n\\nMaybe this is still coming, but ES|QL doesn\'t appear to offer sinks. We\\nhypothesize that users should consume pipeline output uniformly as JSON through\\na REST API.\\n\\n## Syntax\\n\\nSyntactically, the [ES|QL language][esql-syntax] is similar to TQL. The\\nfollowing points stood out:\\n\\n[esql-syntax]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-syntax.html\\n\\n- The `|` (pipe) symbol separates commands that describe the dataflow.\\n- Comments work as in C++: `//` for single line and `/*` and `*/` for multi-line\\n  comments.\\n- Expressions can occur in `WHERE`, `STATS`, and other commands. The following\\n  relational operators exist:\\n  - Arithmetic comparisons via `<`, `<=`, `==`, `>=`, `>`\\n  - Set membership via `IN`\\n  - Glob-like wildcard search via `LIKE`\\n  - Regular expressions via `RLIKE`\\n- Date-time literals make it easier to express dates (`seconds`, `hours`, etc.)\\n  and timespans (e.g., `1 day`). We found that expressing numeric values across\\n  multiple orders of magnitude is common, e.g., when dealing with GBs. This is\\n  why we also offer SI literals in TQL, allowing you to write large numbers as\\n  `1 Mi` or `1 M`.\\n- ES|QL features multiple scalar [functions][esql-funcs].\\n  that perform value-to-value transformations. Functions can occur in `ROW`,\\n  `EVAL`, and `WHERE`.\\n- Similarly, [aggregation functions][esql-agg-funcs] perform a vector-to-scalar\\n  transformation per group in `STATS`.\\n\\n[esql-funcs]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-functions.html\\n[esql-agg-funcs]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-agg-functions.html\\n\\n## Engine\\n\\nES|QL comes with its own executor, i.e., it\'s not transpiled into any of the\\nexisting engines. A running pipelines is a *task* and there exists an\\n[API][esql-api] for querying their state, which may return something like:\\n\\n[esql-api]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-task-management.html\\n\\n```json\\n{\\n  \\"node\\" : \\"2j8UKw1bRO283PMwDugNNg\\",\\n  \\"id\\" : 5326,\\n  \\"type\\" : \\"transport\\",\\n  \\"action\\" : \\"indices:data/read/esql\\",\\n  \\"description\\" : \\"FROM test | STATS MAX(d) by a, b\\",\\n  \\"start_time\\" : \\"2023-07-31T15:46:32.328Z\\",\\n  \\"start_time_in_millis\\" : 1690818392328,\\n  \\"running_time\\" : \\"41.7ms\\",\\n  \\"running_time_in_nanos\\" : 41770830,\\n  \\"cancellable\\" : true,\\n  \\"cancelled\\" : false,\\n  \\"headers\\" : { }\\n}\\n```\\n\\n## Data Model\\n\\nThe concept of [multi-valued fields][esql-mv-fields] exists to bridge the world\\nbetween JSON records and 2D tables. This shows the heritage of the type system,\\nwhich evolved from document stores as opposed to structured data stores. In\\ndocument land, every record may have a different shape (or schema). The term\\n*multi-valued* effectively means *list*, e.g., `[1, 2, 3]`.\\n\\n[esql-mv-fields]: https://esql-latest.docs-preview.app.elstc.co/guide/en/elasticsearch/reference/master/esql-multivalued-fields.html\\n\\nNoteworthy:\\n\\n- The order of multi-valued fields is undefined.\\n- It\'s possible to impose set semantics by using the `keyword` type. Specifying\\n  this type causes duplicate removal on ingest.\\n- Other types, like `long`, do not cause removal of duplicates on ingest.\\n\\nThe output is still semi-structured in that listness is something dynamic on a\\nper-value basis. Consider this output:\\n\\n```json\\n{\\n  \\"columns\\": [\\n    { \\"name\\": \\"a\\", \\"type\\": \\"long\\"},\\n    { \\"name\\": \\"b\\", \\"type\\": \\"long\\"}\\n  ],\\n  \\"values\\": [\\n    [1, [1, 2]],\\n    [2,      3]\\n  ]\\n}\\n```\\n\\nThe column `b` has the list value `[1, 2]` in the first row and `3` in the\\nsecond. In a strict type system (like TQL), the type of `b` could be\\n`list<long>` but then the second row would have value `[3]` instead of `3`. Sum\\ntypes (called `union` or `variant` in many languages) are another way to\\nrepresent heterogeneous data as in the above example. If we described `b` with\\nthe type `union<long, list<long>>` instead of `long`, then it would be perfectly\\nfine for `b` to take one value `[1, 2]` in one row and `3` in another.\\n\\nFor TQL, we built our data model on top of data frames. We express structure in\\nterms of *records* and *lists*, and arbitrarily nested combinations of them. It\\nwould be up the user to define set semantics that ensures unique values. We\\nconsider adding such a set type in the future (possible as type constraint or\\nattribute) as we gain more complete support of the underlying Arrow type system.\\nSimilarly, we plan on adding sum types in the future.\\n\\n## Summary\\n\\nThe release of ES|QL witnesses a current trend of convergence in terms of query\\nlanguages. The pipeline concept now exists for several decades. Splunk was the\\nfirst company to successfully commercialize this interface with SPL, but today\\nthere are many players in the market that have a similar language. Microsoft\\nopen-sourced their Kusto language, and we see other vendors embedding it into\\ntheir products, such as Cribl Search. Most SIEM vendors also have their own\\ninhouse pipeline language.\\n\\nThe data ecosystem has numerous languages for advanced users to offer, such as\\n[dplyr](https://dplyr.tidyverse.org/), [jq](https://stedolan.github.io/jq/),\\n[pandas](https://pandas.pydata.org/), and [polars](https://www.pola.rs/). And\\nnew ones are mushrooming everywhere, e.g., [PRQL](https://prql-lang.org/),\\n[Zed](https://zed.brimdata.io/).\\n\\nWith our own TQL, we seek to bridge the data and security analytics world, by\\noffering an intuitive language that is easy to grasp, but that internally maps\\nto vectorized execution on top of data frames that can be easily shared with other\\nruntimes.\\n\\nIf you want to look deeper at ES|QL, check out the branch\\n[`feature/esql`][esql-branch]. Find something interesting about pipelines to\\ndiscuss? Swing by our [Discord](/discord) and start a conversation.\\n\\n[esql-branch]: https://github.com/elastic/elasticsearch/tree/feature/esql/x-pack/plugin/esql"},{"id":"/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines","metadata":{"permalink":"/blog/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines","source":"@site/blog/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines/index.md","title":"Slash Your SIEM, Cloud, and Data Costs with Tenzir Security Data Pipelines","description":"Staying ahead in the realm of cybersecurity means relentlessly navigating an","date":"2023-08-17T00:00:00.000Z","formattedDate":"August 17, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"siem","permalink":"/blog/tags/siem"},{"label":"cost","permalink":"/blog/tags/cost"}],"readingTime":1.105,"hasTruncateMarker":true,"authors":[{"name":"Oliver Rochford","title":"Chief Futurist","url":"https://github.com/oliverrochford","email":"oliver@tenzir.com","imageURL":"https://github.com/oliverrochford.png","key":"oliverrochford"}],"frontMatter":{"authors":"oliverrochford","date":"2023-08-17T00:00:00.000Z","tags":["tenzir","pipelines","siem","cost"],"comments":true},"prevItem":{"title":"A First Look at ES|QL","permalink":"/blog/a-first-look-at-esql"},"nextItem":{"title":"Introducing Tenzir Security Data Pipelines","permalink":"/blog/introducing-tenzir-security-data-pipelines"}},"content":"Staying ahead in the realm of cybersecurity means relentlessly navigating an\\nendless sea of emerging threats and ever-increasing data volumes. The battle to\\nstay one step ahead can often feel overwhelming, especially when your\\norganization\'s data costs are skyrocketing.\\n\\n![Slash your Costs](slash-your-costs.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n**Imagine you can simultaneously cut costs, improve security, and optimize your\\ndetection and response infrastructure.**\\n\\nIntroducing our latest whitepaper: *Slashing SIEM, Cloud, and Data Costs with\\nTenzir*\\n\\nBy reading this whitepaper, you will learn how Tenzir security data pipelines\\nmake your cybersecurity infrastructure more cost-effective and robust, and why\\nyou should consider leveraging them to future proof your security data\\noperations.\\n\\n**What you\'ll find inside:**\\n* A clear breakdown of the challenges around skyrocketing SIEM, cloud and data\\n  costs (and why they\'re eating into your security budget).\\n* An expert\'s take on why solid security data management is your secret weapon\\n  against evolving cyber threats.\\n* A sneak peek at [Security Data Operations\\n  (SecDataOps)](https://tenzir.com/secdataops?utm_source=Blog)\u2014a fresh approach\\n  to reduce data costs and data wrangling, while maximizing data utility.\\n* A detailed look at how Tenzir\'s clever security data pipelines make data\\n  filtering, reduction, and deduplication a breeze.\\n\\nEquip yourself with the knowledge to transform your security data operations.\\nDownload Tenzir\'s whitepaper today and step into the future of cybersecurity\\nwith confidence.\\n\\n<div align=\\"center\\">\\n  <a class=\\"button button--primary\\" href=\\"https://tenzir.com/whitepaper.pdf\\">Download Now</a>\\n</div>"},{"id":"/introducing-tenzir-security-data-pipelines","metadata":{"permalink":"/blog/introducing-tenzir-security-data-pipelines","source":"@site/blog/introducing-tenzir-security-data-pipelines/index.md","title":"Introducing Tenzir Security Data Pipelines","description":"We\'re overjoyed to announce our highly-anticipated security data pipeline","date":"2023-08-09T00:00:00.000Z","formattedDate":"August 9, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":4.3,"hasTruncateMarker":true,"authors":[{"name":"Oliver Rochford","title":"Chief Futurist","url":"https://github.com/oliverrochford","email":"oliver@tenzir.com","imageURL":"https://github.com/oliverrochford.png","key":"oliverrochford"}],"frontMatter":{"title":"Introducing Tenzir Security Data Pipelines","authors":"oliverrochford","date":"2023-08-09T00:00:00.000Z","tags":["tenzir","pipelines"],"comments":true},"prevItem":{"title":"Slash Your SIEM, Cloud, and Data Costs with Tenzir Security Data Pipelines","permalink":"/blog/slash-your-siem-cloud-and-data-costs-with-tenzir-security-data-pipelines"},"nextItem":{"title":"Tenzir for Splunk Users","permalink":"/blog/tenzir-for-splunk-users"}},"content":"We\'re overjoyed to [announce][pr] our highly-anticipated security data pipeline\\nplatform at the renowned BlackHat conference in Las Vegas. The launch marks a\\nmilestone in our journey to bring simplicity to data engineering for\\ncybersecurity operations, and to bring a cost-efficient way to tackle the\\nincreasingly complex data engineering challenges that security teams confront\\ndaily.\\n\\n[pr]: https://tenzir.com/press/tenzir-launches-security-data-pipeline-platform?utm_source=Blog&utm_campaign=launch\\n\\n![Tenzir Launch](tenzir-launch.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Security Data Operations for the Automation Age\\n\\nThe volume of data that needs to be collected, analyzed, and stored by security\\nteams has skyrocketed. Traditional security operations tools are increasingly\\noverwhelmed, leading to an urgent need for more efficient and effective\\nsolutions. Tenzir addresses this challenge head-on, simplifying data management\\nso that security teams can focus more on identifying and mitigating threats.\\n\\nIn the words of our CEO and founder, [Matthias\\nVallentin](https://www.linkedin.com/in/matthias-vallentin/), \\"To survive in\\ntoday\'s unforgiving threat landscape you need fast, near real-time *and*\\nextensive historical data. Tenzir pipelines help security teams speed up and\\nsimplify managing the data they need, so that they can spend more time doing\\nwhat is most crucial\u2014hunting threats.\\"\\n\\n## Why Data Pipelines for Security?\\n\\nCybersecurity has become an increasingly data-driven field. From network traffic\\nto cloud telemetry, the amount of information that security teams need to\\nanalyze is staggering. A single security incident can generate billions of data\\npoints that need to be reviewed, analyzed, and actioned. Traditional methods of\\ncollecting, aggregating, and analyzing this data are not just insufficient, they\\nare obsolete, leading to security gaps and cost inefficiencies.\\n\\nNavigating the modern security data stack is no small feat. Today\'s security\\nteams are faced not just with the management of a horde of advanced security\\nsolutions including SIEM, SOAR, UEBA, and threat intelligence platforms, but\\nalso the challenge of integrating these systems with diverse data technologies,\\nsuch as databases, data lakes, and data warehouses. Compounding this complexity\\nis a growing reliance on cloud microservices and increasingly AI services. This\\nhas made security operations less of a routine process and more a strategic\\nexercise in continuously mastering emergent complexities and optimizing\\nperformance.\\n\\nThis is where Tenzir\'s security data pipelines come in. Our unique platform\\ninstigates a shift from centralized security information and event management to\\na more adaptive and decentralized operating model more aligned with DevOps and\\ndata engineering principles: security data operations\\n([SecDataOps](https://tenzir.com/secdataops?utm_source=Blog)). It transcends\\nmere collection of events and logs, instead building resilient and robust data\\nflows that optimize data for further use, whether for detection and correlation,\\nthreat hunting, or machine learning. Data pipelines are already common in data\\nengineering and DevOps. They are designed to provide a seamless, efficient, and\\nflexible way to manage and move data. But there are a number of reasons why data\\npipelines are also the ideal solution for today\'s cybersecurity challenges.\\n\\n- Firstly, security data pipelines optimize and formalize data management. They\\n  allow for the standardized collection, shaping, enrichment, and routing of\\n  data between any security and data technology. They also provide a measurable,\\n  repeatable and more cost-effective approach to solve the growing data\\n  engineering challenges typically faced by security teams.  As they are\\n  designed specifically for security use-cases, they also allow security teams\\n  to meet their own data needs.\\n- Secondly, and in today\'s economic climate more crucially, security data\\n  pipelines reduce consumption-based costs. By moving only the right data to the\\n  right place at the right time in the most efficient way, and by pushing\\n  detection and enrichment workloads to the network edge, businesses can\\n  drastically reduce their SIEM, cloud, and other data costs. Security\\n  operations become more efficient and cost-effective, ultimately allowing more\\n  data to be collected, and scarce money to be reallocated.\\n- Thirdly, security data pipelines help avoid vendor lock-in. Tenzir is built on\\n  open data and security standards, making data exchange between different\\n  technologies trivial. Pipelines also connect diverse tools and solutions as\\n  needed, enabling organizations to choose whatever solutions fit best for them,\\n  and to better adapt to evolving.\\n- Finally, the flexibility and scalability of security data pipelines are\\n  unmatched. They can scale up or down according to need. They also make it easy\\n  to support new data types and security scenarios, helping to future-proof your\\n  security architecture, and providing operational plasticity and resilience.\\n\\nSecurity data pipelines are transforming the security operations landscape by\\nproviding a more effective and efficient way to manage the ever-growing volumes\\nof security data. As the volume, variety, and velocity of security data continue\\nto increase, the need for more effective data management and analysis tools will\\nonly grow as well.\\n\\nAt Tenzir, we are leading this transformation, building an open platform that\\nempowers security teams to build and deploy efficient security data pipelines\\nusing plug-and-play building blocks. Our goal is simple\u2014more time for threat\\nhunting, less time and money on data engineering, and a more robust\\ncybersecurity posture overall.\\n\\nIn today\'s complex cybersecurity landscape, data pipelines are not just for data\\nengineers anymore. They have become indispensable for security teams. The era\\nfor security data pipelines isn\'t on the horizon, it\'s already here.\\n\\nJoin us on this exciting journey to revamp cybersecurity operations.\\n\\nStart using Tenzir by visiting our website at\\n[https://tenzir.com](https://tenzir.com?utm_source=Blog), or get in touch with\\nus at [info@tenzir.com](mailto:info@tenzir.com)."},{"id":"/tenzir-for-splunk-users","metadata":{"permalink":"/blog/tenzir-for-splunk-users","source":"@site/blog/tenzir-for-splunk-users/index.md","title":"Tenzir for Splunk Users","description":"Our Tenzir Query Language (TQL) is a pipeline language that works","date":"2023-08-03T00:00:00.000Z","formattedDate":"August 3, 2023","tags":[{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"threat hunting","permalink":"/blog/tags/threat-hunting"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"tql","permalink":"/blog/tags/tql"},{"label":"splunk","permalink":"/blog/tags/splunk"},{"label":"spl","permalink":"/blog/tags/spl"}],"readingTime":8.06,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Tenzir for Splunk Users","authors":"mavam","date":"2023-08-03T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["zeek","threat hunting","pipelines","tql","splunk","spl"],"comments":true},"prevItem":{"title":"Introducing Tenzir Security Data Pipelines","permalink":"/blog/introducing-tenzir-security-data-pipelines"},"nextItem":{"title":"Native Zeek Log Rotation & Shipping","permalink":"/blog/native-zeek-log-rotation-and-shipping"}},"content":"Our [Tenzir Query Language (TQL)](/language) is a pipeline language that works\\nby chaining operators into data flows. When we designed TQL, we specifically\\nstudied Splunk\'s [Search Processing Language (SPL)][spl], as it generally leaves\\na positive impression for security analysts that are not data engineers. Our\\ngoal was to take all the good things of SPL, but provide a more powerful\\nlanguage without compromising simplicity. In this blog post, we explain how the\\ntwo languages differ using concrete threat hunting examples.\\n\\n[spl]: https://docs.splunk.com/Documentation/SplunkCloud/latest/Search/Aboutthesearchlanguage\\n\\n![SPL versus TQL](spl-vs-tql.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Why not SQL?\\n\\nSplunk was the first tool that provided an integrated solution from interactive\\ndata exploration to management-grade dashboards\u2014all powered by dataflow\\npipelines. The success of Splunk is not only resulting from their first-mover\\nadvantage in the market, but also because their likable user experience: it is\\n*easy* to get things done.\\n\\nAt Tenzir, we have a very clear target audience: security practitioners. They\\nare not necessarily data engineers and fluent in SQL and low-level data tools,\\nbut rather identify as blue teamers, incident responders, threat hunters,\\ndetection engineers, threat intelligence analysts, and other domain experts. Our\\ngoal is cater to these folks, without requiring them to have deep understanding\\nof relational algebra.\\n\\nWe opted for a dataflow language because it simplifies reasoning\u2014one step at a\\ntime. At least conceptually, because a smart system optimizes the execution\\nunder the hood. As long as the observable behavior remains the same, the\\nunderlying implementation can optimize the actual computation at will. This is\\nespecially noticeable with declarative languages, such as SQL, where the user\\ndescribes the *what* instead of the *how*. A dataflow language is a bit more\\nconcrete in that it\'s closer to the *how*, but that\'s precisely the trade-off\\nthat simplifies the reasoning: the focus is on a single operation at a time as\\nopposed to an entire large expression.\\n\\nThis dataflow pipeline style is becoming more and more popular. Most SIEMs have\\na language of their own, like Splunk. [Kusto][kusto] is another great example\\nwith a wide user base in security. Even in the data space,\\n[PRQL](https://prql-lang.org) witnesses a strong support for this way of\\nthinking.\\n\\n[kusto]: https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/\\n\\nIn fact, for a given dataflow pipeline there\'s often an equivalent SQL\\nexpression, because the underlying engines frequently map to the same execution\\nmodel. This gives rise to [transpiling dataflow languages to other execution\\nplatforms][splunk-transpiler]. Ultimately, our goal is that security\\npractitioners do not have to think about *any* of this and stay in their happy\\nplace, which means avoiding context switches to lower-level data primitives.\\n\\n[splunk-transpiler]: https://www.databricks.com/blog/2022/12/16/accelerating-siem-migrations-spl-pyspark-transpiler.html\\n\\nNow that we got the SQL topic out of the way, let\'s dive into some hands-on\\nexamples that illustrate the similarities and differences between SPL and TQL.\\n\\n## Examples\\n\\nBack in 2020, Eric Ooi wrote about [threat hunting with\\nZeek](https://www.ericooi.com/zeekurity-zen-part-iv-threat-hunting-with-zeek/),\\nproviding a set of Splunk queries that are corner stones for threat hunting.\\n\\n### Connections to destination port > 1024\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn id.resp_p > 1024\\n| chart count over service by id.resp_p\\n```\\n\\nTenzir:\\n\\n```\\nexport\\n| where #schema == \\"zeek.conn\\" && id.resp_p > 1024\\n| summarize count(.) by service, id.resp_p\\n```\\n\\nAnalysis:\\n\\n- In SPL, you typically start with an `index=X` to specify your dataset. In\\n  TQL, you start with a source operator. To run a query over historical data, we\\n  use the [`export`](/next/operators/export) operator.\\n\\n- The subsequent [`where`](/next/operators/where) operator is a\\n  transformation to filter the stream of events with the\\n  [expression](/language/expressions) `#schema == \\"zeek.conn\\" && id.resp_p >\\n  1024`. In SPL, you write that expression directly into `index`. In TQL, we\\n  logically separate this because one operator should have exactly one purpose.\\n  Under the hood, the TQL optimizer does predicate pushdown to avoid first\\n  exporting the entire database and only then applying the filter.\\n\\n  Why does this single responsibility principle matter? Because it\'s critical\\n  for *composition*: we can now replace `export` with another data source, like\\n  [`from`](/next/operators/from) [`kafka`](/connectors/kafka), and the rest\\n  of the pipeline stays the same.\\n\\n- TQL\'s `#schema` is an expression that is responsible for filtering the data\\n  sources. This is because all TQL pipelines are *multi-schema*, i.e., they can\\n  process more than a single type of data. The ability to specify a regular\\n  expression makes for a powerful way to select the desired input.\\n\\n- SPL\'s [`chart X by Y, Z`][chart] (or equivalently `chart X over Y by Z`)\\n  performs an implicit\\n  [pivot-wider](https://epirhandbook.com/en/pivoting-data.html) operation on\\n  `Z`. This different tabular format has the same underlying data produced by\\n  `summarize X by Y, Z`, which is why we are replacing it accordingly in our\\n  examples.\\n\\n[chart]: https://www.splunk.com/en_us/blog/tips-and-tricks/search-commands-stats-chart-and-timechart.html\\n\\n### Top 10 sources by number of connections\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn\\n| top id.orig_h\\n| head 10\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.conn\\"\\n| top id.orig_h\\n| head 10\\n```\\n\\nNote the similarity. We opted to add [`top`](/next/operators/top) and\\n[`rare`](/next/operators/rare) to make SPL users feel at home.\\n\\n### Top 10 sources by bytes sent\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_conn\\n| stats values(service) as Services sum(orig_bytes) as B by id.orig_h\\n| sort -B\\n| head 10\\n| eval MB = round(B/1024/1024,2)\\n| eval GB = round(MB/1024,2)\\n| rename id.orig_h as Source\\n| fields Source B MB GB Services\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.conn\\"\\n| summarize Services=distinct(service), B=sum(orig_bytes) by id.orig_h\\n| sort B desc\\n| head 10\\n| extend MB=round(B/1024/1024,2)\\n| extend GB=round(MB/1024,2)\\n| put Source=id.orig_h, B, MB, GB, Services\\n```\\n\\nAnalysis:\\n\\n- We opted for Kusto\'s syntax of sorting (for technical reasons), by appending\\n  an `asc` or `desc` qualifier after the field name. `sort -B` translates into\\n  `sort B desc`, whereas `sort B` into `sort B asc`. However, we want to adopt\\n  the SPL syntax in the future.\\n\\n- SPL\'s `eval` maps to [`extend`](/next/operators/extend).\\n\\n- The difference between `extend` and [`put`](/next/operators/put) is\\n  that `extend` keeps all fields as is, whereas `put` reorders fields and\\n  performs an explicit projection with the provided fields.\\n\\n- We don\'t have functions in TQL. *Yet*. It\'s one of our most important roadmap\\n  items at the time of writing, so stay tuned.\\n\\n### Bytes transferred over time by service\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=\\"zeek_conn\\" OR sourcetype=\\"zeek_conn_long\\"\\n| eval orig_megabytes = round(orig_bytes/1024/1024,2)\\n| eval resp_megabytes = round(resp_bytes/1024/1024,2)\\n| eval orig_gigabytes = round(orig_megabytes/1024,2)\\n| eval resp_gigabytes = round(resp_megabytes/1024,2)\\n| timechart sum(orig_gigabytes) AS \'Outgoing\',sum(resp_gigabytes) AS \'Incoming\' by service span=1h\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == /zeek\\\\.conn.*/\\n| extend orig_megabytes=round(orig_bytes/1024/1024, 2)\\n| extend resp_megabytes=round(orig_bytes/1024/1024, 2)\\n| extend orig_gigabytes=round(orig_megabytes/1024, 2)\\n| extend resp_gigabytes=round(orig_megabytes/1024, 2)\\n| summarize Outgoing=sum(orig_gigabytes), Incoming=sum(resp_gigabytes) by ts, service resolution 1h\\n```\\n\\nAnalysis:\\n\\n- SPL\'s `timechart` does an implicit group by timestamp. As we use TQL\'s\\n  `summarize` operator, we need to explicitly provide the grouping field `ts`.\\n  In the future, you will be able to use `:timestamp` in a grouping expression,\\n  i.e., group by the field with the type named `timestamp`.\\n\\n- This query spreads over two data sources: the event `zeek.conn` and\\n  `zeek.conn_long`. The latter tracks long-running connections and is available\\n  as [separate package](https://github.com/corelight/zeek-long-connections).\\n\\n### Rare JA3 hashes\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_ssl\\n| rare ja3\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.ssl\\"\\n| rare ja3\\n| head 10\\n```\\n\\nAnalysis:\\n\\n- This example shows again how to select a specific data source and perform\\n  \\"stack counting\\". Unlike SPL, our version of `rare` does not limit the output\\n  to 10 events by default, which is why add `head 10`. This goes back to the\\n  single responsibility principle: one operator should do exactly one thing. The\\n  act of limiting the output should always be associated with\\n  [`head`](/next/operators/head).\\n\\n### Expired certificates\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_x509\\n| convert num(certificate.not_valid_after) AS cert_expire\\n| eval current_time = now(), cert_expire_readable = strftime(cert_expire,\\"%Y-%m-%dT%H:%M:%S.%Q\\"), current_time_readable=strftime(current_time,\\"%Y-%m-%dT%H:%M:%S.%Q\\")\\n| where current_time > cert_expire\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where certificate.not_valid_after > now()\\n```\\n\\nAnalysis:\\n\\n- This example shows the benefit of native time types (and Tenzir\'s rich type\\n  system in general).\\n\\n- TQL\'s [type system](/data-model/type-system) has first-class support for times\\n  and durations.\\n\\n- TQL\'s [`zeek-tsv`](/formats/zeek-tsv) parser preserves `time` types natively,\\n  so you don\'t have to massage strings at query-time.\\n\\n### Large DNS queries\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_dns\\n| eval query_length = len(query)\\n| where query_length > 75\\n| table _time id.orig_h id.resp_h proto query query_length answer\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.dns\\"\\n| extend query_length = length(query)\\n| where query_length > 75\\n| select :timestamp, id.orig_h, id.resp_h, proto, query, query_length, answer\\n```\\n\\nAnalysis:\\n\\n- As mentioned above, we don\'t have functions in TQL yet. Once we have them,\\n  SPL\'s `len` will map to `length` in TQL.\\n\\n- The SPL-generated `_time` field maps to the `:timestamp` type extractor in\\n  TQL.\\n\\n### Query responses with NXDOMAIN\\n\\nSplunk:\\n\\n```splunk-spl\\nindex=zeek sourcetype=zeek_dns rcode_name=NXDOMAIN\\n| table _time id.orig_h id.resp_h proto query\\n```\\n\\nTenzir:\\n\\n```splunk-spl\\nexport\\n| where #schema == \\"zeek.dns\\" && rcode_name == \\"NXDOMAIN\\"\\n| select :timestamp, id.orig_h, id.resp_h, proto, query\\n```\\n\\nAnalysis:\\n\\n- The `table` operator in splunk outputs the data in tabular form. This is the\\n  default for our [app](https://app.tenzir.com).\\n\\n- There\'s also an [upcoming](https://github.com/tenzir/tenzir/pull/3113) `write\\n  table` format to generate a tabular representation outside the app.\\n\\n## Summary\\n\\nIn this blog post we\'ve juxtaposed the languages of Splunk (SPL) and Tenzir\\n(TQL). They are remarkably similar\u2014and that\'s not accidental. When we talked to\\nsecurity analysts we often heard that Splunk has a great UX. Even our own\\nengineers that live on the command line find this mindset natural. But Splunk\\nwas not our only inspiration, we also drew inspiration from Kusto and others.\\n\\nAs we created TQL, we wanted to learn from missed opportunities while doubling\\ndown on SPL\'s great user experience.\\n\\nIf you\'d like to give Tenzir a spin, [try our community edition](/get-started)\\nfor free. A demo node with example pipelines is waiting for you. For more\\ndetails about TQL, head over to the [language documentation](/pipelines)."},{"id":"/native-zeek-log-rotation-and-shipping","metadata":{"permalink":"/blog/native-zeek-log-rotation-and-shipping","source":"@site/blog/native-zeek-log-rotation-and-shipping/index.md","title":"Native Zeek Log Rotation & Shipping","description":"Did you know that Zeek supports log rotation triggers, so","date":"2023-07-27T00:00:00.000Z","formattedDate":"July 27, 2023","tags":[{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"logs","permalink":"/blog/tags/logs"},{"label":"shipping","permalink":"/blog/tags/shipping"},{"label":"rotation","permalink":"/blog/tags/rotation"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":4.875,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Native Zeek Log Rotation & Shipping","authors":"mavam","date":"2023-07-27T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["zeek","logs","shipping","rotation","pipelines"],"comments":true},"prevItem":{"title":"Tenzir for Splunk Users","permalink":"/blog/tenzir-for-splunk-users"},"nextItem":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","permalink":"/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir"}},"content":"Did you know that [Zeek](http://zeek.org) supports log rotation triggers, so\\nthat you can do anything you want with a newly rotated batch of logs?\\n\\n![Zeek Log Rotation](zeek-log-rotation.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nThis blog post shows you how to use Zeek\'s native log rotation feature to\\nconveniently invoke any post-processor, such as a log shipper. In our examples\\nwe show how to to ingest data into Tenzir, but you can plug in any downstream\\ntooling.\\n\\n## External Log Shipping (pull)\\n\\nIn case you\'re not using Zeek\'s native log rotation trigger, you may observe a\\ndirectory to which Zeek periodically writes files. For example, the utility\\n[zeek-archiver](https://github.com/zeek/zeek-archiver) does that.\\n\\nGeneric log shippers can take care of that as well. Your mileage may vary. For\\nexample, [Filebeat][filebeat] works for stock Zeek only. The parsing logic is\\nhard-coded for every log type. If you have custom scripts or extend some logs,\\nyou\'re left alone. Filebeat also uses the stock Zeek JSON output, which has no\\ntype information. Filebeat then brings the typing back manually later as it\\nconverts the logs to the Elastic Common Schema (ECS).\\n\\n[filebeat]: https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-zeek.html\\n\\n## Native Log Shipping (push)\\n\\nThere\'s also a lesser known, push-based option using [Zeek\'s logging\\nframework](https://docs.zeek.org/en/master/frameworks/logging.html). You can\\nprovide a shell script that Zeek invokes *whenever it rotates a file*. The shell\\nscript receives the filename of the rotated file plus some additional metadata\\nas arguments.\\n\\nFirst, to activate log rotation, you need to set\\n`Log::default_rotation_interval` to a non-zero value. The default of `0 secs`\\nmeans that log rotation is disabled.\\n\\nSecond, to customize what\'s happening on rotation you can redefine\\n[`Log::default_rotation_postprocessor_cmd`](https://docs.zeek.org/en/master/scripts/base/frameworks/logging/main.zeek.html#id-Log::default_rotation_postprocessor_cmd)\\nto point to a shell script.\\n\\nFor example, to rotate all log files every 10 minutes with a custom `ingest`\\nscript, you can invoke Zeek as follows:\\n\\n```bash\\nzeek -r trace.pcap \\\\\\n  Log::default_rotation_postprocessor_cmd=ingest \\\\\\n  Log::default_rotation_interval=10mins\\n```\\n\\nLet\'s take a look at this `ingest` shell script in more detail. Zeek always\\npasses 6 arguments to the post-processing script:\\n\\n1. The filename of the log, e.g., `/path/to/conn.log`\\n2. The type of the log (aka. `path`), such as `conn` or `http`\\n3. Timestamp when Zeek opened the log file\\n4. Timestamp when Zeek closed (= rotated) the log file\\n5. A flag that is true when rotation occurred due to Zeek terminating\\n6. The format of the log, which is either `ascii` (=\\n   [`zeek-tsv`](/formats/zeek-tsv)) or [`json`](/formats/json)\\n\\nHere\'s a complete example that uses (1), (2), and (6):\\n\\n```bash title=\\"ingest\\"\\n#!/bin/sh\\n\\nfile_name=\\"$1\\"\\nbase_name=\\"$2\\"\\nfrom=\\"$3\\"\\nto=\\"$4\\"\\nterminating=\\"$5\\"\\nwriter=\\"$6\\"\\n\\nif [ \\"$writer\\" = \\"ascii\\" ]; then\\n  format=\\"zeek-tsv\\"\\nelif [ \\"$writer\\" = \\"json\\" ]; then\\n  format=\\"json --schema zeek.$base_name\\"\\nelse\\n  echo \\"unsupported Zeek writer: $writer\\"\\n  exit 1\\nfi\\n\\npipeline=\\"from file $file_name read $format | import\\"\\n\\ntenzir \\"$pipeline\\"\\n```\\n\\n### Post-processing with Tenzir pipelines\\n\\nWhen you run Zeek as above, the `ingest` script dynamically constructs an\\ningestion pipeline based on the type of the Zeek log at hand. Given your logging\\nformat (TSV or JSON), the pipelines for a rotated `conn.log` file may look like\\nthis:\\n\\n```\\nfrom file /path/to/conn.log read zeek-tsv | import\\nfrom file /path/to/conn.log read json --schema zeek.conn | import\\n```\\n\\nThis pipeline reads the Zeek log and pipes it to the\\n[`import`](/next/operators/import) operator, which stores all your logs at a\\nrunning Tenzir node. You could also use the\\n[`extend`](/next/operators/extend) operator to include the filename in the data:\\n\\n```bash\\npipeline=\\"from file $file_name read $format \\\\\\n          | extend filename=$file_name \\\\\\n          | import\\"\\n```\\n\\nTake a look at the [list of operators](/next/operators) for further inspiration\\non things you can do, or check out the [user guides](/user-guides) for concrete\\nideas.\\n\\n### Zeek package\\n\\nIf you want post-processing with Tenzir pipelines out of the box, use our\\nofficial [Zeek package](https://github.com/tenzir/zeek-tenzir):\\n\\n```bash\\nzkg install zeek-tenzir\\n```\\n\\nAfter installing the package, you have two options to run pipelines on rotated\\nZeek logs:\\n\\n1. Load the `tenzir-import` Zeek script to ship logs to a local Tenzir node\\n\\n   ```bash\\n   # Start a node.\\n   tenzir-node\\n   # Ship logs to it and delete the original files.\\n   zeek -r trace.pcap tenzir/import\\n   ```\\n\\n  Pass `Tenzir::delete_after_postprocesing=F` to `zeek` to keep the original\\n  logs.\\n\\n2. Write Zeek scripts to register pipelines manually:\\n\\n   ```zeek\\n   # Activate log rotation by setting a non-zero value.\\n   redef Log::default_rotation_interval = 10 mins;\\n \\n   event zeek_init()\\n     {\\n     Tenzir::postprocess(\\"import\\");\\n     Tenzir::postprocess(\\"to directory /tmp/logs write parquet\\");\\n     }\\n   ```\\n\\n   The above Zeek script hooks up two pipelines via the function\\n   `Tenzir::postprocess`. Each pipeline executes upon log rotation and receives\\n   the Zeek log file as input. The first imports all data via\\n   [`import`](/next/operators/import) and the second writes the logs as\\n   [`parquet`](/formats/parquet) files using [`to`](/next/operators/to).\\n\\n## Reliability\\n\\nZeek implements the log rotation logic by spawning a separate child process.\\nWhen the (parent) Zeek process dies, the children become orphaned and keep\\nrunning until completion.\\n\\nThe implication is that Zeek cannot re-trigger a failed post-processing command.\\nSo you have exactly one shot. This may not be a problem for trace file analysis,\\nbut live deployments may require higher reliability guarantees. For such\\nscenarios, we recommend to use the post-processing script as a notifier, e.g.,\\nto signal another tool that it can now process a file.\\n\\nFor ultimate control over logging, you can always develop your own [writer\\nplugin](/blog/mobilizing-zeek-logs#writer-plugin) that immediately ship logs\\ninstead of going through the file system.\\n\\n## Conclusion\\n\\nThis blog post shows how you can use Zeek\'s native log rotation feature to\\ninvoke an arbitrary command as soon as a log file gets rotated. This approach\\nprovides an attractive alternative that turns pull-based file monitoring into\\nmore flexible push-based delivery.\\n\\n|              |   Push   |     Pull     |\\n| ------------ |:--------:|:------------:|\\n| Trigger      | rotation | new file/dir |\\n| Complexity   |   low    |    medium    |\\n| Reliability  |   low    |    high      |\\n\\nIf you are looking for an efficient way to get your Zeek logs flowing, [give\\nTenzir a try](/get-started). [Our Zeek\\npackage](https://github.com/tenzir/zeek-tenzir) makes it easy to launch\\npost-processing pipelines natively from Zeek. And don\'t forget to check out our\\n[other Zeek blogs](/blog/tags/zeek)."},{"id":"/shell-yeah-supercharging-zeek-and-suricata-with-tenzir","metadata":{"permalink":"/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir","source":"@site/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir/index.md","title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","description":"As an incident responder, threat hunter, or detection engineer, getting quickly","date":"2023-07-20T00:00:00.000Z","formattedDate":"July 20, 2023","tags":[{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"suricata","permalink":"/blog/tags/suricata"},{"label":"logs","permalink":"/blog/tags/logs"},{"label":"shell","permalink":"/blog/tags/shell"}],"readingTime":4.125,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","authors":"mavam","date":"2023-07-20T00:00:00.000Z","last_updated":"2024-02-23T00:00:00.000Z","tags":["zeek","suricata","logs","shell"],"comments":true},"prevItem":{"title":"Native Zeek Log Rotation & Shipping","permalink":"/blog/native-zeek-log-rotation-and-shipping"},"nextItem":{"title":"Zeek and Ye Shall Pipe","permalink":"/blog/zeek-and-ye-shall-pipe"}},"content":"As an incident responder, threat hunter, or detection engineer, getting quickly\\nto your analytics is key for productivity. For network-based visibility and\\ndetection, [Zeek](https://zeek.org) and [Suricata](https://suricata.io) are the\\nbedrock for many security teams. But operationalizing these tools can take a\\ngood chunk of time.\\n\\nSo we asked ourselves: **How can we make it super easy to work with Zeek and\\nSuricata logs?**\\n\\n![Shell Operator](shell-operator.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Purring like a Suricat\\n\\nIn our [previous blog post](/blog/zeek-and-ye-shall-pipe/) we adapted Zeek to\\nbehave like a good ol\' Unix tool, taking input via stdin and producing output\\nvia stdout. Turns out you can do the same fudgery with Suricata:[^1]\\n\\n[^1]: Suricata outputs [EVE\\nJSON](https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html)\\nby default, which is equivalent to Zeek\'s streaming JSON output, with the\\ndifference being that Zeek\'s `_path` field is called `event_type` in Suricata\\nlogs.\\n\\n```bash title=suricatify\\n#!/bin/sh\\nsuricata -r /dev/stdin \\\\\\n  --set outputs.1.eve-log.filename=/dev/stdout \\\\\\n  --set logging.outputs.0.console.enabled=no\\n```\\n\\nLet\'s break this down:\\n\\n- The `--set` option take a `name=value` parameter that overrides the settings\\n  in your `suricata.yaml` config file.\\n- The key `outputs.1.eve-log.filename` refers to the `outputs` array, takes\\n  element at index `1`, treats that as object and goes to the nested field\\n  `eve-log.filename`. Setting `/dev/stdout` as filename makes Suricata write to\\n  stdout.\\n- We must set `logging.outputs.0.console.enabled` to `no` because Suricata\\n  writes startup log messages to stdout. Since they are not valid JSON, we\\n  would otherwise create an invalid JSON output stream.\\n\\n## User-defined Operators\\n\\nNow that we have both Zeek and Suricata at our fingertips, how can we work with\\ntheir output more easily? This is where Tenzir comes into play\u2014easy\\n[pipelines](/pipelines) for security teams to acquire,\\n[shape](/next/user-guides/shape-data), and route event data.\\n\\nHere are two examples that count the number of unique source IP addresses per\\ndestination IP address, on both Zeek and Suricata data:\\n\\n```bash\\n# Zeek\\nzcat pcap.gz | zeekify | tenzir \\\\\\n  \'read zeek-json\\n   | where #schema == \\"zeek.conn\\"\\n   | summarize n=count_distinct(id.orig_h) by id.resp_h\\n   | sort n desc\'\\n# Suricata\\nzcat pcap.gz | suricatify | tenzir \\\\\\n  \'read suricata\\n   | where #schema == \\"suricata.flow\\"\\n   | summarize n=count_distinct(src_ip) by dst_ip\\n   | sort n desc\'\\n```\\n\\nIt\'s a bit unwieldy to write such a command line that requires an external shell\\nscript to work. This is where [user-defined\\noperators](/next/language/user-defined-operators) come into play. In combination\\nwith the [`shell`](/next/operators/shell) operator, you can write a custom\\n`zeek` and `suricata` operator and ditch the shell script:\\n\\n```yaml title=\\"tenzir.yaml\\"\\ntenzir:\\n  operators:\\n    zeek:\\n     shell \\"zeek -r - LogAscii::output_to_stdout=T\\n            JSONStreaming::disable_default_logs=T\\n            JSONStreaming::enable_log_rotation=F\\n            json-streaming-logs\\"\\n     | read zeek-json\\n    suricata:\\n     shell \\"suricata -r /dev/stdin\\n            --set outputs.1.eve-log.filename=/dev/stdout\\n            --set logging.outputs.0.console.enabled=no\\"\\n     | read suricata\\n```\\n\\nThe difference stands out when you look now at the pipeline definition:\\n\\n```text title=Zeek\\nzeek\\n| where #schema == \\"zeek.conn\\"\\n| summarize n=count_distinct(id.orig_h) by id.resp_h\\n| sort n desc\\n```\\n\\n```text bash title=Suricata\\nsuricata\\n| where #schema == \\"suricata.flow\\"\\n| summarize n=count_distinct(src_ip) by dst_ip\\n| sort n desc\\n```\\n\\nIt\'s pretty convenient to drop packets into a Tenzir pipeline, process them with\\nour favorite tools, and then perform fast in-situ analytics on them. The nice\\nthing is that operators compose: a new operator automatically works with all\\nexisting ones.\\n\\n## How does it work?\\n\\nFirst, let\'s take a look at the standard approach where one process pipes the\\noutput into the next:\\n\\n![Piping Zeek to Tenzir](zeek-to-tenzir-pipe.excalidraw.svg)\\n\\nWhen using the `shell` operator, the `tenzir` process spawns `zeek` or\\n`suricata` as child process. The operator then forwards the bytes from stdin of\\nthe `tenzir` process to the child\'s stdin, and uses the child\'s stdout as input\\nto the subsequent [`read`](/next/operators/read) operator.\\n\\n![Shelling out to Zeek](zeek-to-tenzir-shell.excalidraw.svg)\\n\\nIn the above example, `shell` acts as a *source* operator, i.e., it does not\\nconsume input and only produces output. The `shell` operator can also act as\\n*transformation*, i.e., additionally accept input. This makes it possible to use\\nit more flexibly in combination with other operators, e.g., the\\n[`load`](/next/operators/load) operator emitting bytes from a\\n[loader](/connectors):\\n\\n```\\nload file trace.pcap\\n| zeek\\n| where 6.6.6.6\\n| write json\\n```\\n\\nGot a PCAP trace via Kafka? Just exchange the `file` loader with the\\n[`kafka`](/connectors/kafka) loader:\\n\\n```\\nload kafka -t artifact\\n| zeek\\n| where 6.6.6.6\\n| write json\\n```\\n\\nYou may not always sit in front of a command line and are able to pipe data from\\na Unix tool into a Tenzir pipeline. For example, when you use our\\n[app](https://app.tenzir.com) or the [REST API](/rest-api). This is where the\\n`shell` operator shines. The diagram above shows how `shell` shifts the entry\\npoint of data from a tool to the Tenzir process. You can consider `shell` your\\nescape hatch to reach deeper into a specific Tenzir node, as if you had a native\\nshell.\\n\\n## Conclusion\\n\\nIn this blog post we showed you the [`shell`](/next/operators/shell) operator\\nand how you can use it to integrate third-party tooling into a Tenzir pipeline\\nwhen coupled with [user-defined\\noperators](/next/language/user-defined-operators).\\n\\nUsing Zeek or Suricata? Tenzir makes \'em fun to work with. Check out our other\\nblogs tagged with [`#zeek`](/blog/tags/zeek) and\\n[`#suricata`](/blog/tags/suricata), and [give it a shot](/get-started) yourself."},{"id":"/zeek-and-ye-shall-pipe","metadata":{"permalink":"/blog/zeek-and-ye-shall-pipe","source":"@site/blog/zeek-and-ye-shall-pipe/index.md","title":"Zeek and Ye Shall Pipe","description":"Zeek turns packets into structured logs. By default, Zeek","date":"2023-07-13T00:00:00.000Z","formattedDate":"July 13, 2023","tags":[{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"logs","permalink":"/blog/tags/logs"},{"label":"json","permalink":"/blog/tags/json"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":2.46,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Zeek and Ye Shall Pipe","authors":"mavam","date":"2023-07-13T00:00:00.000Z","tags":["zeek","logs","json","pipelines"],"comments":true},"prevItem":{"title":"Shell Yeah! Supercharging Zeek and Suricata with Tenzir","permalink":"/blog/shell-yeah-supercharging-zeek-and-suricata-with-tenzir"},"nextItem":{"title":"Mobilizing Zeek Logs","permalink":"/blog/mobilizing-zeek-logs"}},"content":"[Zeek](https://zeek.org) turns packets into structured logs. By default, Zeek\\ngenerates one file per log type and per rotation timeframe. If you don\'t want to\\nwrangle files and directly process the output, this short blog post is for you.\\n\\n![Zeek as Pipeline](zeek-as-pipeline.excalidraw.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\nZeek requires a bit of adaptation to fit in the Unix pipeline model, by which we\\nmean *take your input on stdin and produce your output to stdout*:\\n\\n```\\n<upstream> | zeek | <downstream>\\n```\\n\\nIn this example, `<upstream>` produces packets in PCAP format and `<downstream>`\\nprocesses the Zeek logs. Let\'s work towards this.\\n\\nSolving the upstream part is easy: just use `zeek -r -` to read from stdin. So\\nlet\'s focus on the logs downstream. [Our last blog](/blog/mobilizing-zeek-logs)\\nintroduced the various logging formats, such as tab-separated values (TSV),\\nJSON, and Streaming JSON with an extra `_path` discriminator field. The only\\nformat conducive to multiplexing different log types is Streaming JSON.\\n\\nLet\'s see what we get:\\n\\n```bash\\nzcat < trace.pcap | zeek -r - json-streaming-logs\\n```\\n\\n```\\n\u276f ls\\njson_streaming_analyzer.1.log       json_streaming_packet_filter.1.log\\njson_streaming_conn.1.log           json_streaming_pe.1.log\\njson_streaming_dce_rpc.1.log        json_streaming_reporter.1.log\\njson_streaming_dhcp.1.log           json_streaming_sip.1.log\\njson_streaming_dns.1.log            json_streaming_smb_files.1.log\\njson_streaming_dpd.1.log            json_streaming_smb_mapping.1.log\\njson_streaming_files.1.log          json_streaming_snmp.1.log\\njson_streaming_http.1.log           json_streaming_ssl.1.log\\njson_streaming_kerberos.1.log       json_streaming_tunnel.1.log\\njson_streaming_ntlm.1.log           json_streaming_weird.1.log\\njson_streaming_ntp.1.log            json_streaming_x509.1.log\\njson_streaming_ocsp.1.log\\n```\\n\\nThe `json-streaming-package` prepends a distinguishing prefix to the filename.\\nThe `*.N.log` suffix counts the rotations, e.g., `*.1.log` means the logs from\\nthe first batch.\\n\\nLet\'s try to avoid the files altogether and send the contents of these file to\\nstdout. This requires a bit of option fiddling to achieve the desired result:\\n\\n```bash\\nzcat < trace.pcap |\\n  zeek -r - \\\\\\n    LogAscii::output_to_stdout=T \\\\\\n    JSONStreaming::disable_default_logs=T \\\\\\n    JSONStreaming::enable_log_rotation=F \\\\\\n    json-streaming-logs\\n```\\n\\nThis requires a bit explanation:\\n\\n- `LogAscii::output_to_stdout=T` redirects the log output to stdout.\\n- `JSONStreaming::disable_default_logs=T` disables the default TSV logs.\\n  Without this option, Zeek will print *both* TSV and NDJSON to stdout.\\n- `JSONStreaming::enable_log_rotation=F` disables log rotation. This is needed\\n  because the option `output_to_stdout=T` sets the internal filenames to\\n  `/dev/stdout`, which Zeek then tries to rotate away. Better not.\\n\\nHere\'s the result you\'d expect, which is basically a `cat *.log`:\\n\\n```json\\n{\\"_path\\":\\"files\\",\\"_write_ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"fuid\\":\\"FhEFqzHx1hVpkhWci\\",\\"uid\\":\\"CHhfpE1dTbPgBTR24\\",\\"id.orig_h\\":\\"128.14.134.170\\",\\"id.orig_p\\":57468,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":80,\\"source\\":\\"HTTP\\",\\"depth\\":0,\\"analyzers\\":[],\\"mime_type\\":\\"text/html\\",\\"duration\\":0.0,\\"is_orig\\":false,\\"seen_bytes\\":51,\\"total_bytes\\":51,\\"missing_bytes\\":0,\\"overflow_bytes\\":0,\\"timedout\\":false}\\n{\\"_path\\":\\"http\\",\\"_write_ts\\":\\"2021-11-17T13:32:43.250616Z\\",\\"ts\\":\\"2021-11-17T13:32:43.249475Z\\",\\"uid\\":\\"CHhfpE1dTbPgBTR24\\",\\"id.orig_h\\":\\"128.14.134.170\\",\\"id.orig_p\\":57468,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"GET\\",\\"host\\":\\"198.71.247.91\\",\\"uri\\":\\"/\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36 \\",\\"request_body_len\\":0,\\"response_body_len\\":51,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[],\\"resp_fuids\\":[\\"FhEFqzHx1hVpkhWci\\"],\\"resp_mime_types\\":[\\"text/html\\"]}\\n{\\"_path\\":\\"packet_filter\\",\\"_write_ts\\":\\"1970-01-01T00:00:00.000000Z\\",\\"ts\\":\\"2023-07-11T03:30:17.189787Z\\",\\"node\\":\\"zeek\\",\\"filter\\":\\"ip or not ip\\",\\"init\\":true,\\"success\\":true}\\n{\\"_path\\":\\"conn\\",\\"_write_ts\\":\\"2021-11-17T13:33:01.457108Z\\",\\"ts\\":\\"2021-11-17T13:32:46.565338Z\\",\\"uid\\":\\"CD868huwhDP636oT\\",\\"id.orig_h\\":\\"89.248.165.145\\",\\"id.orig_p\\":43831,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":52806,\\"proto\\":\\"tcp\\",\\"conn_state\\":\\"S0\\",\\"missed_bytes\\":0,\\"history\\":\\"S\\",\\"orig_pkts\\":1,\\"orig_ip_bytes\\":40,\\"resp_pkts\\":0,\\"resp_ip_bytes\\":0}\\n{\\"_path\\":\\"tunnel\\",\\"_write_ts\\":\\"2021-11-17T13:40:34.891453Z\\",\\"ts\\":\\"2021-11-17T13:40:34.891453Z\\",\\"uid\\":\\"CsqzCG2F8VDR4gM3a8\\",\\"id.orig_h\\":\\"49.213.162.198\\",\\"id.orig_p\\":0,\\"id.resp_h\\":\\"198.71.247.91\\",\\"id.resp_p\\":0,\\"tunnel_type\\":\\"Tunnel::GRE\\",\\"action\\":\\"Tunnel::DISCOVER\\"}\\n```\\n\\nNobody can remember this invocation. Especially during firefighting when you\\nquickly need to plow through a trace to understand it. So we want to wrap this\\nsomehow:\\n\\n```bash title=zeekify\\n#!/bin/sh\\nzeek -r - \\\\\\n  LogAscii::output_to_stdout=T \\\\\\n  JSONStreaming::disable_default_logs=T \\\\\\n  JSONStreaming::enable_log_rotation=F \\\\\\n  json-streaming-logs \\\\\\n  \\"$@\\"\\n```\\n\\nNow we\'re in pipeline land:\\n\\n```bash\\nzcat pcap.gz | zeekify | head | jq -r ._path\\n```\\n\\n```\\npacket_filter\\nfiles\\nntp\\ntunnel\\nconn\\nntp\\nhttp\\nconn\\nntp\\nconn\\n```\\n\\nOkay, we got Zeek as a Unix pipe. But now you have to wrangle the JSON with\\n`jq`. Unless you\'re a die-hard fan, even simple analytics, like filtering or\\naggregating, have a steep learning curve. In the next blog post, we\'ll double\\ndown on the elegant principle of pipelines and show how you can take do easy\\nin-situ analytics with Tenzir."},{"id":"/mobilizing-zeek-logs","metadata":{"permalink":"/blog/mobilizing-zeek-logs","source":"@site/blog/mobilizing-zeek-logs/index.md","title":"Mobilizing Zeek Logs","description":"Zeek offers many ways to produce and consume logs. In this","date":"2023-07-06T00:00:00.000Z","formattedDate":"July 6, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"zeek","permalink":"/blog/tags/zeek"},{"label":"logs","permalink":"/blog/tags/logs"},{"label":"json","permalink":"/blog/tags/json"},{"label":"kafka","permalink":"/blog/tags/kafka"}],"readingTime":7.695,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Mobilizing Zeek Logs","authors":"mavam","date":"2023-07-06T00:00:00.000Z","last_updated":"2023-12-17T00:00:00.000Z","tags":["tenzir","zeek","logs","json","kafka"],"comments":true},"prevItem":{"title":"Zeek and Ye Shall Pipe","permalink":"/blog/zeek-and-ye-shall-pipe"},"nextItem":{"title":"Migrating from VAST to Tenzir","permalink":"/blog/migrating-from-vast-to-tenzir"}},"content":"[Zeek](https://zeek.org) offers many ways to produce and consume logs. In this\\nblog, we explain the various Zeek logging formats and show how you can get the\\nmost out of Zeek with Tenzir. We conclude with recommendations for when to use\\nwhat Zeek format based on your use case.\\n\\n![Packet to Logs](zeek-packets-to-logs.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## Zeek Logging 101\\n\\nZeek\'s main function is turning live network traffic or trace files into\\nstructured logs.[^1] Zeek logs span the entire network stack, including\\nlink-layer analytics with MAC address to application-layer fingerprinting of\\napplications. These rich logs are invaluable for any network-based detection and\\nresponse activities. Many users also simply appreciate the myriad of protocol\\nanalyzers, each of which generates a dedicated log file, like `smb.log`,\\n`http.log`, `x509.log`, and others.\\n\\n[^1]: Zeek also comes with a Turing-complete scripting language for executing\\narbitrary logic. The event-based language resembles Javascript and is\\nespecially useful for performing in-depth protocol analysis and engineering\\ndetections.\\n\\nIn the default configuration, Zeek writes logs into the current directory, one\\nfile per log type. There are various file formats to choose from, such as TSV,\\nJSON, and others. Let\'s take a look.\\n\\n### Tab-Separated Values (TSV)\\n\\nZeek\'s custom tab-separated value (TSV) format is variant of CSV with additional\\nmetadata, similar to a data frame.\\n\\nHere\'s how you create TSV logs from a trace:\\n\\n```bash\\nzeek -C -r trace.pcap [scripts]\\n```\\n\\n:::info disable checksumming\\nWe add `-C` to disable checksumming, telling Zeek to ignore mismatches and use\\nall packets in the trace. This is good practice to process all packets in a\\ntrace, as some capturing setups may perturb the checksums.\\n:::\\n\\nAnd here\'s a snippet of the corresponding `conn.log`:\\n\\n```\\n#separator \\\\x09\\n#set_separator\\t,\\n#empty_field\\t(empty)\\n#unset_field\\t-\\n#path\\tconn\\n#open\\t2019-06-07-14-30-44\\n#fields\\tts\\tuid\\tid.orig_h\\tid.orig_p\\tid.resp_h\\tid.resp_p\\tproto\\tservice\\tduration\\torig_bytes\\tresp_bytes\\tconn_state\\tlocal_orig\\tlocal_resp\\tmissed_bytes\\thistory\\torig_pkts\\torig_ip_bytes\\tresp_pkts\\tresp_ip_bytes\\ttunnel_parents\\tcommunity_id\\n#types\\ttime\\tstring\\taddr\\tport\\taddr\\tport\\tenum\\tstring\\tinterval\\tcount\\tcount\\tstring\\tbool\\tbool\\tcount\\tstring\\tcount\\tcount\\tcount\\tcount\\tset[string]\\tstring\\n1258531221.486539\\tCz8F3O3rmUNrd0OxS5\\t192.168.1.102\\t68\\t192.168.1.6\\t7\\tudp\\tdhcp\\t0.163820\\t301\\t300\\tSF\\t-\\t-\\t0\\tDd\\t1\\t329\\t1\\t328\\t-\\t1:aWZfLIquYlCxKGuJ62fQGlgFzAI=\\n1258531680.237254\\tCeJFOE1CNssyQjfJo1\\t192.168.1.103\\t137\\t192.168.1.255\\t137\\tudp\\tdns\\t3.780125\\t350\\t0\\tS0\\t-\\t-\\t546\\t0\\t0\\t-\\t1:fLbpXGtS1VgDhqUW+WYaP0v+NuA=\\n```\\n\\nAnd here\'s a `http.log` with a different header:\\n\\n```\\n#separator \\\\x09\\n#set_separator\\t,\\n#empty_field\\t(empty)\\n#unset_field\\t-\\n#path\\thttp\\n#open\\t2019-06-07-14-30-44\\n#fields\\tts\\tuid\\tid.orig_h\\tid.orig_p\\tid.resp_h\\tid.resp_p\\ttrans_depth\\tmethod\\thost\\turi\\treferrer\\tversion\\tuser_agent\\trequest_body_len\\tresponse_body_len\\tstatus_code\\tstatus_msg\\tinfo_code\\tinfo_msg\\ttags\\tusername\\tpassword\\tproxied\\torig_fuids\\torig_filenames\\torig_mime_types\\tresp_fuids\\tresp_filenames\\tresp_mime_types\\n#types\\ttime\\tstring\\taddr\\tport\\taddr\\tport\\tcount\\tstring\\tstring\\tstring\\tstring\\tstring\\tstring\\tcount\\tcount\\tcount\\tstring\\tcount\\tstring\\tset[enum]\\tstring\\tstring\\tset[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\tvector[string]\\n1258535653.087137\\tCUk3vSsgfU9oCghL4\\t192.168.1.104\\t1191\\t65.54.95.680\\t1\\tHEAD\\tdownload.windowsupdate.com\\t/v9/windowsupdate/redir/muv4wuredir.cab?0911180916\\t-\\t1.1\\tWindows-Update-Agent\\t0\\t0\\t20OK\\t-\\t-\\t(empty)\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\n1258535655.525107\\tCc6Alh3FtTOAqNSIx2\\t192.168.1.104\\t1192\\t65.55.184.16\\t80\\t1\\tHEAD\\twww.update.microsoft.com\\t/v9/windowsupdate/selfupdate/wuident.cab?0911180916\\t-\\t1.1\\tWindows-Update-Agent\\t0\\t200\\tOK\\t-\\t-\\t(empty)\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\n```\\n\\nMany Zeek users would now resort to their downstream log management tool,\\nassuming it supports the custom TSV format. Zeek also comes with small helper\\nutility `zeek-cut` for light-weight reshaping of this TSV format. For example:\\n\\n```bash\\nzeek-cut id.orig_h id.resp_h < conn.log\\n```\\n\\nThis selects the columns `id.orig_h` and `id.resp_h`. Back in the days, many\\nfolks used `awk` to extract fields by their position, e.g., with `$4`, `$7`,\\n`$9`. This is not only difficult to understand, but also brittle, since Zeek\\nschemas can change based on configuration. With `zeek-cut`, it\'s at least a bit\\nmore robust.\\n\\nTenzir\'s data pipelines make it easy to process Zeek logs. The native\\n[`zeek-tsv`](/next/formats/zeek-tsv) parser converts them into data frames, so\\nthat you can process them with a wide range of [operators](/next/operators):\\n\\n```bash\\ncat *.log | tenzir \'read zeek-tsv | select id.orig_h, id.resp_h\'\\n```\\n\\nTenzir takes care of parsing the type information properly and keeps IP\\naddresses and timestamps as native data types. You can also see in the examples\\nthat Tenzir handles multiple concatenated TSV logs of different schemas as you\'d\\nexpect.\\n\\nNow that Zeek logs are flowing, you can do a lot more than selecting specific\\ncolumns. Check out the [shaping guide](/next/user-guides/shape-data) for\\nfiltering rows, performing aggregations, and routing them elsewhere. Or [store\\nthe logs](/next/user-guides/import-into-a-node) locally at a Tenzir node in\\n[Parquet](https://parquet.apache.org) to process them with other data tools.\\n\\n### JSON\\n\\nZeek can also render logs as JSON by setting\\n[`LogAscii::use_json=T`](https://docs.zeek.org/en/master/frameworks/logging.html):\\n\\n```bash\\nzeek -r trace.pcap LogAscii::use_json=T\\n```\\n\\nAs with TSV, this generates one file per log type containing the NDJSON records.\\nHere are the same two entries from above:\\n\\n```json\\n{\\"ts\\":1258531221.486539,\\"uid\\":\\"C8b0xF1gjm7rOZXemg\\",\\"id.orig_h\\":\\"192.168.1.102\\",\\"id.orig_p\\":68,\\"id.resp_h\\":\\"192.168.1.1\\",\\"id.resp_p\\":67,\\"proto\\":\\"udp\\",\\"service\\":\\"dhcp\\",\\"duration\\":0.1638200283050537,\\"orig_bytes\\":301,\\"resp_bytes\\":300,\\"conn_state\\":\\"SF\\",\\"missed_bytes\\":0,\\"history\\":\\"Dd\\",\\"orig_pkts\\":1,\\"orig_ip_bytes\\":329,\\"resp_pkts\\":1,\\"resp_ip_bytes\\":328}\\n{\\"ts\\":1258531680.237254,\\"uid\\":\\"CMsxKW3uTZ3tSLsN0g\\",\\"id.orig_h\\":\\"192.168.1.103\\",\\"id.orig_p\\":137,\\"id.resp_h\\":\\"192.168.1.255\\",\\"id.resp_p\\":137,\\"proto\\":\\"udp\\",\\"service\\":\\"dns\\",\\"duration\\":3.780125141143799,\\"orig_bytes\\":350,\\"resp_bytes\\":0,\\"conn_state\\":\\"S0\\",\\"missed_bytes\\":0,\\"history\\":\\"D\\",\\"orig_pkts\\":7,\\"orig_ip_bytes\\":546,\\"resp_pkts\\":0,\\"resp_ip_bytes\\":0}\\n```\\n\\nAnd `http.log`:\\n\\n```json\\n{\\"ts\\":1258535653.087137,\\"uid\\":\\"CDsoEy4cHSHJRBvilg\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1191,\\"id.resp_h\\":\\"65.54.95.64\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"HEAD\\",\\"host\\":\\"download.windowsupdate.com\\",\\"uri\\":\\"/v9/windowsupdate/redir/muv4wuredir.cab?0911180916\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Windows-Update-Agent\\",\\"request_body_len\\":0,\\"response_body_len\\":0,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[]}\\n{\\"ts\\":1258535655.525107,\\"uid\\":\\"C8muAY3KSDGScVUrO4\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1192,\\"id.resp_h\\":\\"65.55.184.16\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"HEAD\\",\\"host\\":\\"www.update.microsoft.com\\",\\"uri\\":\\"/v9/windowsupdate/selfupdate/wuident.cab?0911180916\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Windows-Update-Agent\\",\\"request_body_len\\":0,\\"response_body_len\\":0,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[]}\\n```\\n\\nUse the regular [`json`](/next/formats/json) parser to get the data flowing:\\n\\n```bash\\ncat conn.log | tenzir \'read json --schema \\"zeek.conn\\" | head\'\\ncat http.log | tenzir \'read json --schema \\"zeek.http\\" | head\'\\n```\\n\\nThe option `--schema` of the `json` reader passes a name of a known schema that\\nbrings back the lost typing, e.g., the schema knows that the `duration` field in\\n`conn.log` is not a floating-point number, but a duration type, so that you can\\nfilter connections with `where duration < 4 mins`.\\n\\n### Streaming JSON\\n\\nThe above one-file-per-log format is not conducive to stream processing because\\na critical piece of information is missing: the type of the log (or *schema*),\\nwhich is only contained in the file name. So you can\'t just ship the data away\\nand infer the type later at ease. And passing the filename around through a side\\nchannel is cumbersome. Enter [JSON streaming\\nlogs](https://github.com/corelight/json-streaming-logs). This package adds two\\nnew fields: `_path` with the log type and `_write_ts` with the timestamp when\\nthe log was written. For example, `http.log` now gets an additional field\\n`{\\"_path\\": \\"http\\" , ...}`. This makes it a lot easier to consume, because you\\ncan now concatenate the entire output and multiplex it over a single stream.\\n\\nThis format doesn\'t come with stock Zeek. Use Zeek\'s package manager `zkg` to\\ninstall it:\\n\\n```bash\\nzkg install json-streaming-logs\\n```\\n\\nThen pass the package name to the list of scripts on the command line:\\n\\n```bash\\nzeek -r trace.pcap json-streaming-logs\\n```\\n\\nAnd now you get JSON logs in the current directory. Here\'s the same `conn.log`\\nand `http.log` example from above, this time with added `_path` and `_write_ts`\\nfields:\\n\\n```json title=\\"conn.log\\"\\n{\\"_path\\":\\"conn\\",\\"_write_ts\\":\\"2009-11-18T16:45:06.678526Z\\",\\"ts\\":\\"2009-11-18T16:43:56.223671Z\\",\\"uid\\":\\"CzFMRp2difzeGYponk\\",\\"id.orig_h\\":\\"192.168.1.104\\",\\"id.orig_p\\":1387,\\"id.resp_h\\":\\"74.125.164.85\\",\\"id.resp_p\\":80,\\"proto\\":\\"tcp\\",\\"service\\":\\"http\\",\\"duration\\":65.45066595077515,\\"orig_bytes\\":694,\\"resp_bytes\\":11708,\\"conn_state\\":\\"SF\\",\\"missed_bytes\\":0,\\"history\\":\\"ShADadfF\\",\\"orig_pkts\\":9,\\"orig_ip_bytes\\":1062,\\"resp_pkts\\":14,\\"resp_ip_bytes\\":12276}\\n```\\n\\n```json title=\\"http.log\\"\\n {\\"_path\\":\\"http\\",\\"_write_ts\\":\\"2009-11-18T17:00:51.888304Z\\",\\"ts\\":\\"2009-11-18T17:00:51.841527Z\\",\\"uid\\":\\"CgdQsm2eBBV8T8GjUk\\",\\"id.orig_h\\":\\"192.168.1.103\\",\\"id.orig_p\\":1399,\\"id.resp_h\\":\\"74.125.19.104\\",\\"id.resp_p\\":80,\\"trans_depth\\":1,\\"method\\":\\"GET\\",\\"host\\":\\"www.google.com\\",\\"uri\\":\\"/\\",\\"version\\":\\"1.1\\",\\"user_agent\\":\\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)\\",\\"request_body_len\\":0,\\"response_body_len\\":10205,\\"status_code\\":200,\\"status_msg\\":\\"OK\\",\\"tags\\":[],\\"resp_fuids\\":[\\"FI1gWL1b9SuIA8HAv3\\"],\\"resp_mime_types\\":[\\"text/html\\"]}\\n```\\n\\nMany tools have have logic to disambiguate based on a field like `_path`. That\\nsaid, JSON is always \\"dumbed down\\" compared to TSV, which contains additional\\ntype information, such as timestamps, durations, IP addresses, etc. This type\\ninformation is lost in the JSON output and up to the downstream tooling to bring\\nback.\\n\\nWith JSON Streaming logs, you can simply concatenate all logs Zeek generated and\\npass them to a tool of your choice. Tenzir has native support for these logs via\\nthe [`zeek-json`](/next/formats/zeek-json) parser:\\n\\n```bash\\ncat *.log | tenzir \'read zeek-json | taste 1\'\\n```\\n\\nIn fact, the `zeek-json` parser just an alias for `json --selector=zeek:_path`,\\nwhich extracts the schema name from the `_path` field to demultiplex the JSON\\nstream and assign the corresponding schema.\\n\\n### Writer Plugin\\n\\nIf the stock options of Zeek\'s logging framework do not work for you, you can\\nstill write a C++ *writer plugin* to produce any output of your choice.\\n\\nFor example, the [zeek-kafka](https://github.com/SeisoLLC/zeek-kafka) plugin\\nwrites incoming Zeek data to Kafka topics. For this use case, you can also\\nleverage Tenzir\'s [`kafka`](/next/connectors/kafka) connector and write:\\n\\n```bash\\ncat *.log | tenzir \'\\n  read zeek-tsv\\n  | extend _path=#schema\\n  | to kafka -t zeek write json\\n  \'\\n```\\n\\nThis pipeline starts by reading Zeek TSV, appends the `_path` field to emulate\\nStreaming JSON, and then writes the events to the Kafka topic `zeek`. The\\nexample is not equivalent to the Zeek Kafka plugin, because concatenate existing\\nfields and apply a (one-shot) pipeline, as opposed to continuously streaming to\\na Kafka topic. We\'ll elaborate on this in the next blog post, stay tuned.\\n\\n## Conclusion\\n\\nIn this blog, we presented the most common Zeek logging formats. We also\\nprovided examples how you can mobilize any of them in a Tenzir pipeline. If\\nyou\'re unsure when to use what Zeek logging format, here are our\\nrecommendations:\\n\\n:::tip Recommendation\\n- **Use TSV when you can.** If your downstream tooling can parse TSV, it is the\\n  best choice because it retains Zeek\'s rich type annotations as\\n  metadata\u2014without the need for downstream schema wrangling.\\n- **Use Streaming JSON for the easy button**. The single stream of NDJSON\\n  logs is most versatile, since most downstream tooling supports it well. Use it\\n  when you need to get in business quickly.\\n- **Use stock JSON when you must**. There\'s marginal utility in the\\n  one-JSON-file-per-log format. It requires extra effort in keeping track of\\n  filenames and mapping fields to their corresponding types.\\n- **Use plugins for everything else**. If none of these fit the bill or you\\n  need a tighter integration, leverage Zeek\'s writer plugins to create a custom\\n  logger.\\n:::\\n\\nIf you\'re a Zeek power user and need power tools for data processing, take a\\ncloser look at what we do at [Tenzir](https://tenzir.com). There\'s a lot more\\nyou can do!"},{"id":"/migrating-from-vast-to-tenzir","metadata":{"permalink":"/blog/migrating-from-vast-to-tenzir","source":"@site/blog/migrating-from-vast-to-tenzir/index.md","title":"Migrating from VAST to Tenzir","description":"VAST is now Tenzir. This blog post describes what changed when [we renamed the","date":"2023-06-26T00:00:00.000Z","formattedDate":"June 26, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"vast","permalink":"/blog/tags/vast"},{"label":"community","permalink":"/blog/tags/community"},{"label":"project","permalink":"/blog/tags/project"}],"readingTime":1.55,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"Migrating from VAST to Tenzir","authors":"dominiklohmann","date":"2023-06-26T00:00:00.000Z","tags":["tenzir","vast","community","project"]},"prevItem":{"title":"Mobilizing Zeek Logs","permalink":"/blog/mobilizing-zeek-logs"},"nextItem":{"title":"Visibility Across Space and Time is now Tenzir","permalink":"/blog/vast-to-tenzir"}},"content":"VAST is now Tenzir. This blog post describes what changed when [we renamed the\\nproject](/blog/vast-to-tenzir).\\n\\n![VAST to Tenzir](vast-to-tenzir.excalidraw.svg)\\n\\n\x3c!--truncate--\x3e\\n\\n## TL;DR\\n\\n- Use `tenzir-node` instead of `vast start`.\\n- Use `tenzir` instead of `vast exec`.\\n- Use `tenzir-ctl` for all other commands.\\n- Move your configuration from `<prefix>/etc/vast/vast.yaml` to\\n  `<prefix>/etc/tenzir/tenzir.yaml`.\\n- Move your configuration from `$XDG_CONFIG_HOME/vast/vast.yaml` to\\n  `$XDG_CONFIG_HOME/tenzir/tenzir.yaml`.\\n- In your configuration, replace `vast:` with `tenzir:`.\\n- Prefix environment variables with `TENZIR_` instead of `VAST_`.\\n\\nIn addition to that, the following things have changed.\\n\\n## Project\\n\\n- The repository moved from `tenzir/vast` to `tenzir/tenzir`.\\n- Our Discord server is now the *Tenzir Community*. Join us at\\n  <https://discord.tenzir.com>!\\n- The documentation moved from [vast.io](https://vast.io) to\\n  [docs.tenzir.com](https://docs.tenzir.com).\\n\\n## Usage\\n\\n- We\'re making the split between starting a node and starting a pipeline more\\n  obvious:\\n  - `tenzir` executes pipelines (previously `vast exec`).\\n  - `tenzir-node` starts a node (previously `vast start`).\\n  - Some commands have not yet been ported over to pipelines, and are accessible\\n    under `tenzir-ctl`; this will be phased out over time without deprecation\\n    notices as commands are moving into pipeline operators.\\n  - The `vast` executable exists for drop-in backwards compatibility and is\\n    equivalent to running `tenzir-ctl`.\\n- Configuration moved to use `tenzir` over `vast` where possible.\\n- Packages are now called *Tenzir* instead of *VAST*.\\n- The default install prefix of packages moved from `/opt/vast` to `/opt/tenzir`.\\n- The Docker image now includes the proprietary plugins\\n- There exit separate Docker images `tenzir/tenzir` and `tenzir/tenzir-node` to\\n  match the new binaries `tenzir` and `tenzir-node`, respectively.\\n- The PyVAST package is deprecated and now called Tenzir. We will bring it back\\n  with the Tenzir v4.0 release.\\n- The interop with Apache Arrow uses `tenzir.` prefixes for the extension type\\n  names now. We support reading the old files transparently, but tools\\n  interfacing will need to adapt to the new names `tenzir.ip`, `tenzir.subnet`,\\n  and `tenzir.enumeration`."},{"id":"/vast-to-tenzir","metadata":{"permalink":"/blog/vast-to-tenzir","source":"@site/blog/vast-to-tenzir/index.md","title":"Visibility Across Space and Time is now Tenzir","description":"Renaming our project from VAST to Tenzir","date":"2023-06-20T00:00:00.000Z","formattedDate":"June 20, 2023","tags":[{"label":"tenzir","permalink":"/blog/tags/tenzir"},{"label":"vast","permalink":"/blog/tags/vast"},{"label":"community","permalink":"/blog/tags/community"},{"label":"project","permalink":"/blog/tags/project"}],"readingTime":1.715,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Visibility Across Space and Time is now Tenzir","description":"Renaming our project from VAST to Tenzir","authors":"mavam","date":"2023-06-20T00:00:00.000Z","tags":["tenzir","vast","community","project"]},"prevItem":{"title":"Migrating from VAST to Tenzir","permalink":"/blog/migrating-from-vast-to-tenzir"},"nextItem":{"title":"VAST v3.1","permalink":"/blog/vast-v3.1"}},"content":"After 5 years of developing two identities, the VAST project and Tenzir\\nthe company, we decided to streamline our efforts and **rename VAST to Tenzir**.\\n\\n\x3c!--truncate--\x3e\\n\\nVAST is the open-source software project Tenzir\'s Founder and CEO, Matthias\\nVallentin, created originally during his Master\'s at Technical University Munich\\nin 2006, and then continued to work on throughout his PhD from 2008 to 2016 at\\nthe University of California, Berkeley. The name is an acronym for Visibility\\nAcross Space and Time and originates from the HotSec\'08 paper about [Principles\\nfor Developing Comprehensive Network\\nVisibility](https://www.icir.org/mallman/papers/awareness-hotsec08.pdf) by Mark\\nAllman, Christian Kreibich, Vern Paxson, Robin Sommer, and Nicholas Weaver.\\n\\nThe following reasons ultimately drove our decision to rename VAST:\\n\\n- **Building two brands is double the effort**: we found that building two\\n  brands\u2014one for the VAST project and one for Tenzir as a company\u2014is a challenge\\n  for an early-stage startup like ourselves.\\n- **Taking a broader view when it comes to open source**: we also don\'t see our\\n  enterprise and open-source users as separate and isolated groupings. Instead\\n  we see a continuum. So we want to put all of our energy into a single, unified\\n  community.\\n- **Avoiding confusion and conflicts**: over the past years the term VAST has\\n  become very popular and widely used for a number of different projects and in\\n  adjacent industries (VAST Data, VAST.ai, anyone?) to avoid existing or future\\n  confusion we decided that it was better to retire the name but keep it part of\\n  our legacy and our story.\\n\\nOur next release will be Tenzir v4.0, continuing from the VAST v3.x series. In\\nother respects as well, the transition is an evolution, not a revolution. All\\nproject-related infrastructure, including the SemVer versioning, Git history,\\nGitHub repository, and so forth will remain the same. Stay tuned for a follow-up\\nblog post where we discuss the technical changes in depth and provide migration\\ninstructions.\\n\\nOh, and if you have any nostalgic anecdotes or stories to share, we\'d love to\\nhear them! Chime in on our [Discord chat](/discord) if you have any questions or\\nfeedback."},{"id":"/vast-v3.1","metadata":{"permalink":"/blog/vast-v3.1","source":"@site/blog/vast-v3.1/index.md","title":"VAST v3.1","description":"VAST v3.1 is out. This is","date":"2023-05-12T00:00:00.000Z","formattedDate":"May 12, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"operators","permalink":"/blog/tags/operators"}],"readingTime":1.84,"hasTruncateMarker":true,"authors":[{"name":"Tobias Mayer","title":"Software Architect","url":"https://github.com/tobim","email":"tobias@tenzir.com","imageURL":"https://github.com/tobim.png","key":"tobim"}],"frontMatter":{"title":"VAST v3.1","authors":["tobim"],"image":"/img/blog/vast-v3.1.excalidraw.svg","date":"2023-05-12T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","tags":["release","pipelines","operators"]},"prevItem":{"title":"Visibility Across Space and Time is now Tenzir","permalink":"/blog/vast-to-tenzir"},"nextItem":{"title":"VAST v3.0","permalink":"/blog/vast-v3.0"}},"content":"[VAST v3.1](https://github.com/tenzir/vast/releases/tag/v3.1.0) is out. This is\\na small checkpointing release that brings a few new changes and fixes.\\n\\n\x3c!--truncate--\x3e\\n\\n## Pipelines Reloaded\\n\\nThe old pipeline execution engine is now gone and we updated VAST to use\\nthe new engine everywhere. Most notably this applies to the `export` command,\\nthe compaction engine, and the `query` REST interface.\\n\\nFor this release, we removed support for configuration level export and import\\npipelines. This feature will make a return in the next major release.\\n\\nWe also removed the deprecated YAML-based pipeline syntax to fully concentrate\\non the VAST Language.\\n\\n## Operator Updates\\n\\nWe introduced several new operators:\\n\\n- [`tail`](/next/operators/tail): limits the input to the last N events.\\n- [`unique`](/next/operators/unique): removes adjacent duplicates\\n- [`measure`](/next/operators/measure): replaces the input with incremental\\n  metrics describing the input.\\n- `version`: returns a single event displaying version information of VAST. (Now\\n  [`show`](/next/operators/show).)\\n- [`from`](/next/operators/from): produces events by combining a connector and a\\n  format.\\n- [`read`](/next/operators/read): a short form of `from` that allows for\\n  omitting the connector.\\n- [`to`](/next/operators/to): consumes events by combining a connector and\\n  format.\\n- [`write`](/next/operators/write): a short form of `to` that allows for\\n  omitting the connector.\\n\\nAdditionally, the `put`, `replace`, and `extend` operators have been updated to\\nwork with selectors and extractors. Check out the [growing list of\\noperators](/next/operators/).\\n\\n## Operator Aliases\\n\\nYou can now define aliases for operators in the configuration file. Use it to\\nassign a short and reusable name for operators that would otherwise require\\nseveral arguments. For example:\\n\\n```yaml\\nvast:\\n  operators:\\n    aggregate_flows: |\\n       summarize\\n         pkts_toserver=sum(flow.pkts_toserver),\\n         pkts_toclient=sum(flow.pkts_toclient),\\n         bytes_toserver=sum(flow.bytes_toserver),\\n         bytes_toclient=sum(flow.bytes_toclient),\\n         start=min(flow.start),\\n         end=max(flow.end)\\n       by\\n         timestamp,\\n         src_ip,\\n         dest_ip\\n       resolution\\n         10 mins\\n```\\n\\nNow use it like a regular operator in a pipeline:\\n\\n```\\nfrom file read suricata | aggregate_flows\\n```\\n\\n## Notable Fixes\\n\\n### Improved IPv6 Subnet Handling\\n\\nThe handling of subnets in the IPv6 space received multiple fixes:\\n\\n- The expression `:ip !in ::ffff:0:0/96` now finds all events that\\n  contain IPs that cannot be represented as IPv4 addresses.\\n- Subnets with a prefix above 32 are now correctly formatted with\\n  an IPv6 network part, even if the address is representable as IPv4.\\n\\n### A More Resilient Systemd Service\\n\\nThe systemd unit for VAST now automatically restarts the node in case the\\nprocess went down."},{"id":"/vast-v3.0","metadata":{"permalink":"/blog/vast-v3.0","source":"@site/blog/vast-v3.0/index.md","title":"VAST v3.0","description":"VAST Language Evolution \u2014 Dataflow Pipelines","date":"2023-03-14T00:00:00.000Z","formattedDate":"March 14, 2023","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"pipelines","permalink":"/blog/tags/pipelines"},{"label":"language","permalink":"/blog/tags/language"},{"label":"cef","permalink":"/blog/tags/cef"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"introspection","permalink":"/blog/tags/introspection"},{"label":"regex","permalink":"/blog/tags/regex"}],"readingTime":9.14,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"},{"name":"Daniel Kostuj","title":"Software Engineer","url":"https://github.com/dakostu","email":"daniel@tenzir.com","imageURL":"https://github.com/dakostu.png","key":"dakostu"}],"frontMatter":{"title":"VAST v3.0","description":"VAST Language Evolution \u2014 Dataflow Pipelines","authors":["dominiklohmann","dakostu"],"image":"/img/blog/building-blocks.excalidraw.svg","date":"2023-03-14T00:00:00.000Z","tags":["release","pipelines","language","cef","performance","introspection","regex"]},"prevItem":{"title":"VAST v3.1","permalink":"/blog/vast-v3.1"},"nextItem":{"title":"From Slack to Discord","permalink":"/blog/from-slack-to-discord"}},"content":"[VAST v3.0][github-vast-release] is out. This release brings some major updates\\nto the the VAST language, making it easy to write down dataflow pipelines that\\nfilter, reshape, aggregate, and enrich security event data. Think of VAST as\\nsecurity data pipelines plus open storage engine.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v3.0.4\\n\\n\x3c!--truncate--\x3e\\n\\n![Pipelines and Storage](/img/blog/building-blocks.excalidraw.svg)\\n\\n## The VAST Language: Dataflow Pipelines\\n\\nStarting with v3.0, VAST introduces a new way to write pipelines, with a syntax\\nsimilar to [splunk](https://splunk.com), [Kusto][kusto],\\n[PRQL](https://prql-lang.org/), and [Zed](https://zed.brimdata.io/). Previously,\\nVAST only supported a YAML-like definition of pipelines in configuration files\\nto deploy them statically during import, export, or use them during compaction.\\n\\n[kusto]: https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/\\n\\nThe new syntax resembles the well-known Unix paradigm of command chaining. The\\ndifference to Unix pipelines is that VAST exchanges structured data between\\noperators. The `vast export` and `vast import` commands now accept such a\\npipeline as a string argument. Refer to the pipelines\\ndocumentation for more details on how to use the new pipeline\\nsyntax.\\n\\n:::info Pipeline YAML Syntax Deprecation\\nThis release introduces a transitional period from YAML-style to textual\\npipelines. The old YAML syntax for pipelines will be deprecated and removed\\naltogether in a future version. The new pipeline operators `head` and\\n`taste` have no YAML equivalent.\\n:::\\n\\n## Language Upgrades\\n\\nWe\'ve made some breaking changes to the the VAST language that we\'ve wanted to\\ndo for a long time. Here\'s a summary:\\n\\n1. We removed the term VASTQL: The VAST Query Language is now simply\\n   the VAST language, and \\"VAST\\" will supersede the \\"VASTQL\\" abbreviation.\\n\\n2. Several built-in types have a new name:\\n\\n   - `int` \u2192 `int64`\\n   - `count` \u2192 `uint64`\\n   - `real` \u2192 `double`\\n   - `addr` \u2192 `ip`\\n\\n   The old names are still supported for the time being, but trigger a\\n   warning on startup. We will remove support for the old names in a future\\n   release.\\n\\n3. The match operator `~` and its negated form `!~` no longer exist. Use `==`\\n   and `!=` instead to perform searches with regular expressions, e.g., `url ==\\n   /^https?.*/`. Such queries now work for all string fields in addition to the\\n   previously supported `#type` meta extractor.\\n\\n4. We removed the `#field` meta extractor. That is, queries of the form `#field\\n   == \\"some.field.name\\"` no longer work. Use `some.field.name != null` or the\\n   new short form `some.field.name` to check for field existence moving forward.\\n\\n5. We renamed the boolean literal values `T` and `F` to `true` and `false`,\\n   respectively. For example the query `suricata.alert.alerted == T` is no\\n   longer valid; use `suricata.alert.alerted == true` instead.\\n\\n6. We renamed the non-value literal value `nil` to `null`. For example the\\n   query `x != nil` is no longer valid; use `x != null` instead.\\n\\n7. The `map` type no longer exists: Instead of `map<T, U>`, use the equivalent\\n   `list<record{ key: T, value: U }>`.\\n\\nOur goal is for these changes to make the query language feel more natural to\\nour users. We\'ve got [big plans][rfc-001] on how to extend it\u2014and this felt very\\nmuch necessary as a preparatory step to making the language more useful.\\n\\n[rfc-001]: https://github.com/tenzir/vast/blob/main/rfc/001-composable-pipelines/README.md\\n\\n## Regular Expression Evaluation\\n\\nVAST now supports searching with regular expressions. For example, let\'s say you\\nare looking for all events that contain a GUID surrounded by braces whose third\\nand fourth section are `5ec2-7015`:\\n\\n```json {0} title=\\"vast export -n 1 json \'/\\\\{[0-9a-f]{8}-[0-9a-f]\\\\{4}-5ec2-7015-[0-9a-f]\\\\{12}\\\\}/\'\\"\\n{\\n  \\"RuleName\\": \\"-\\",\\n  \\"UtcTime\\": \\"2020-05-18T09:42:40.443000\\",\\n  \\"ProcessGuid\\": \\"{8bcf3217-5890-5ec2-7015-00000000b000}\\",\\n  \\"ProcessId\\": 172,\\n  \\"Image\\": \\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\conhost.exe\\",\\n  \\"FileVersion\\": \\"10.0.17763.1075 (WinBuild.160101.0800)\\",\\n  \\"Description\\": \\"Console Window Host\\",\\n  \\"Product\\": \\"Microsoft\xae Windows\xae Operating System\\",\\n  \\"Company\\": \\"Microsoft Corporation\\",\\n  \\"OriginalFileName\\": \\"CONHOST.EXE\\",\\n  \\"CommandLine\\": \\"\\\\\\\\??\\\\\\\\C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\conhost.exe 0xffffffff -ForceV1\\",\\n  \\"CurrentDirectory\\": \\"C:\\\\\\\\Windows\\",\\n  \\"User\\": \\"NT AUTHORITY\\\\\\\\SYSTEM\\",\\n  \\"LogonGuid\\": \\"{8bcf3217-54f5-5ebe-e703-000000000000}\\",\\n  \\"LogonId\\": 999,\\n  \\"TerminalSessionId\\": 0,\\n  \\"IntegrityLevel\\": \\"System\\",\\n  \\"Hashes\\": \\"SHA1=74F28DD9B0DA310D85F1931DB2749A26A9A8AB02\\",\\n  \\"ParentProcessGuid\\": \\"{8bcf3217-5890-5ec2-6f15-00000000b000}\\",\\n  \\"ParentProcessId\\": 3440,\\n  \\"ParentImage\\": \\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\OpenSSH\\\\\\\\sshd.exe\\",\\n  \\"ParentCommandLine\\": \\"\\\\\\"C:\\\\\\\\Windows\\\\\\\\System32\\\\\\\\OpenSSH\\\\\\\\sshd.exe\\\\\\" \\\\\\"-R\\\\\\"\\"\\n}\\n```\\n\\n:::tip Case-Insensitive Patterns\\nIn addition to writing `/pattern/`, you can specify a regular expression that\\nignores the casing of characters via `/pattern/i`. The `/i` flag is currently\\nthe only support pattern modifier.\\n:::\\n\\n## Revamped Status for Event Distribution\\n\\nThe event distribution statistics moved within the output of `vast status`.\\n\\nThey were previously available under the `index.statistics` section when using\\nthe `--detailed` option:\\n\\n```json {0} title=\\"VAST v2.4.1 \u276f vast status --detailed | jq .index.statistics\\"\\n{\\n  \\"events\\": {\\n    \\"total\\": 42\\n  },\\n  \\"layouts\\": {\\n    \\"suricata.alert\\": {\\n      \\"count\\": 1,\\n      \\"percentage\\": 2.4\\n    },\\n    \\"suricata.flow\\": {\\n      \\"count\\": 41,\\n      \\"percentage\\": 97.6\\n    }\\n  }\\n}\\n```\\n\\nIt is now under the `catalog` section and shows some additional information:\\n\\n```json {0} title=\\"VAST v3.0 \u276f vast status | jq .catalog\\"\\n{\\n  \\"num-events\\": 42,\\n  \\"num-partitions\\": 3,\\n  \\"schemas\\": {\\n    \\"suricata.alert\\": {\\n      \\"import-time\\": {\\n        \\"max\\": \\"2023-01-13T22:51:23.730183\\",\\n        \\"min\\": \\"2023-01-13T22:51:23.730183\\"\\n      },\\n      \\"num-events\\": 1,\\n      \\"num-partitions\\": 1\\n    },\\n    \\"suricata.flow\\": {\\n      \\"import-time\\": {\\n        \\"max\\": \\"2023-01-13T22:51:24.127312\\",\\n        \\"min\\": \\"2023-01-13T23:13:01.991323\\"\\n      },\\n      \\"num-events\\": 41,\\n      \\"num-partitions\\": 2\\n    }\\n  }\\n}\\n```\\n\\n## Display Schema of Stored Events\\n\\nThe `vast show schemas` command makes it easy to see the structure of events in\\nthe database at a glance.\\n\\n```yaml {0} title=\\"vast show schemas --yaml suricata.flow\\"\\n- suricata.flow:\\n    record:\\n      - timestamp:\\n          timestamp: time\\n      - flow_id:\\n          type: uint64\\n          attributes:\\n            index: hash\\n      - pcap_cnt: uint64\\n      - vlan:\\n          list: uint64\\n      - in_iface: string\\n      - src_ip: ip\\n      - src_port:\\n          port: uint64\\n      - dest_ip: ip\\n      - dest_port:\\n          port: uint64\\n      - proto: string\\n      - event_type: string\\n      - community_id:\\n          type: string\\n          attributes:\\n            index: hash\\n      - flow:\\n          suricata.component.flow:\\n            record:\\n              - pkts_toserver: uint64\\n              - pkts_toclient: uint64\\n              - bytes_toserver: uint64\\n              - bytes_toclient: uint64\\n              - start: time\\n              - end: time\\n              - age: uint64\\n              - state: string\\n              - reason: string\\n              - alerted: bool\\n      - app_proto: string\\n```\\n\\n:::tip Filter Schemas\\nThe `vast show schemas` command supports filtering not just by the exact name of\\na schema, but also by the module name. E.g., `vast show schemas zeek` will print\\na list of all schemas in the Zeek module that the VAST server holds data for.\\n:::\\n\\n## Common Event Format (CEF) Parser\\n\\nThis release includes a new reader plugin for the [Common Event Format\\n(CEF)][cef], a text-based event format that originally stems from ArcSight. This\\nline-based format consists of up to 8 pipe-separated fields, with the last field\\nbeing an optional list of key-value pairs:\\n\\n[cef]: https://www.microfocus.com/documentation/arcsight/arcsight-smartconnectors/pdfdoc/common-event-format-v25/common-event-format-v25.pdf\\n\\n```\\nCEF:Version|Device Vendor|Device Product|Device Version|Device Event Class ID|Name|Severity|[Extension]\\n```\\n\\nHere\'s a real-world instance.\\n\\n```\\nCEF:0|Cynet|Cynet 360|4.5.4.22139|0|Memory Pattern - Cobalt Strike Beacon ReflectiveLoader|8| externalId=6 clientId=2251997 scanGroupId=3 scanGroupName=Manually Installed Agents sev=High duser=tikasrv01\\\\\\\\administrator cat=END-POINT Alert dhost=TikaSrv01 src=172.31.5.93 filePath=c:\\\\\\\\windows\\\\\\\\temp\\\\\\\\javac.exe fname=javac.exe rt=3/30/2022 10:55:34 AM fileHash=2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD rtUtc=Mar 30 2022 10:55:34.688 dtUtc=Mar 30 2022 10:55:32.458 hostLS=2022-03-30 10:55:34 GMT+00:00 osVer=Windows Server 2016 Datacenter x64 1607 epsVer=4.5.5.6845 confVer=637842168250000000 prUser=tikasrv01\\\\\\\\administrator pParams=\\"C:\\\\\\\\Windows\\\\\\\\Temp\\\\\\\\javac.exe\\" sign=Not signed pct=2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609 pFileHash=1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7 pprUser=tikasrv01\\\\\\\\administrator ppParams=C:\\\\\\\\Windows\\\\\\\\Explorer.EXE pssdeep=49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3 pSign=Signed and has certificate info gpFileHash=CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F gpprUser=tikasrv01\\\\\\\\administrator gpParams=C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\userinit.exe gpssdeep=384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu gpSign=Signed actRem=Kill, Rename\\n```\\n\\nVAST\'s CEF plugin supports parsing such lines using the `cef` format:\\n\\n```\\nvast import cef < cef.log\\n```\\n\\nVAST translates the `extension` field to a nested record, where the key-value\\npairs of the extensions map to record fields. Here is an example of the above\\nevent:\\n\\n```json {0} title=\\"vast export json \'172.31.5.93\' | jq\\"\\n{\\n  \\"cef_version\\": 0,\\n  \\"device_vendor\\": \\"Cynet\\",\\n  \\"device_product\\": \\"Cynet 360\\",\\n  \\"device_version\\": \\"4.5.4.22139\\",\\n  \\"signature_id\\": \\"0\\",\\n  \\"name\\": \\"Memory Pattern - Cobalt Strike Beacon ReflectiveLoader\\",\\n  \\"severity\\": \\"8\\",\\n  \\"extension\\": {\\n    \\"externalId\\": 6,\\n    \\"clientId\\": 2251997,\\n    \\"scanGroupId\\": 3,\\n    \\"scanGroupName\\": \\"Manually Installed Agents\\",\\n    \\"sev\\": \\"High\\",\\n    \\"duser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"cat\\": \\"END-POINT Alert\\",\\n    \\"dhost\\": \\"TikaSrv01\\",\\n    \\"src\\": \\"172.31.5.93\\",\\n    \\"filePath\\": \\"c:\\\\\\\\windows\\\\\\\\temp\\\\\\\\javac.exe\\",\\n    \\"fname\\": \\"javac.exe\\",\\n    \\"rt\\": \\"3/30/2022 10:55:34 AM\\",\\n    \\"fileHash\\": \\"2BD1650A7AC9A92FD227B2AB8782696F744DD177D94E8983A19491BF6C1389FD\\",\\n    \\"rtUtc\\": \\"Mar 30 2022 10:55:34.688\\",\\n    \\"dtUtc\\": \\"Mar 30 2022 10:55:32.458\\",\\n    \\"hostLS\\": \\"2022-03-30 10:55:34 GMT+00:00\\",\\n    \\"osVer\\": \\"Windows Server 2016 Datacenter x64 1607\\",\\n    \\"epsVer\\": \\"4.5.5.6845\\",\\n    \\"confVer\\": 637842168250000000,\\n    \\"prUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"pParams\\": \\"C:\\\\\\\\Windows\\\\\\\\Temp\\\\\\\\javac.exe\\",\\n    \\"sign\\": \\"Not signed\\",\\n    \\"pct\\": \\"2022-03-30 10:55:27.140, 2022-03-30 10:52:40.222, 2022-03-30 10:52:39.609\\",\\n    \\"pFileHash\\": \\"1F955612E7DB9BB037751A89DAE78DFAF03D7C1BCC62DF2EF019F6CFE6D1BBA7\\",\\n    \\"pprUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"ppParams\\": \\"C:\\\\\\\\Windows\\\\\\\\Explorer.EXE\\",\\n    \\"pssdeep\\": \\"49152:2nxldYuopV6ZhcUYehydN7A0Fnvf2+ecNyO8w0w8A7/eFwIAD8j3:Gxj/7hUgsww8a0OD8j3\\",\\n    \\"pSign\\": \\"Signed and has certificate info\\",\\n    \\"gpFileHash\\": \\"CFC6A18FC8FE7447ECD491345A32F0F10208F114B70A0E9D1CD72F6070D5B36F\\",\\n    \\"gpprUser\\": \\"tikasrv01\\\\\\\\administrator\\",\\n    \\"gpParams\\": \\"C:\\\\\\\\Windows\\\\\\\\system32\\\\\\\\userinit.exe\\",\\n    \\"gpssdeep\\": \\"384:YtOYTIcNkWE9GHAoGLcVB5QGaRW5SmgydKz3fvnJYunOTBbsMoMH3nxENoWlymW:YLTVNkzGgoG+5BSmUfvJMdsq3xYu\\",\\n    \\"gpSign\\": \\"Signed\\",\\n    \\"actRem\\": \\"Kill, Rename\\"\\n  }\\n}\\n```\\n\\n:::note Syslog Header\\nSometimes CEF is prefixed with a syslog header. VAST currently only supports the\\n\\"raw\\" form without the syslog header. We are working on support for composable\\n*generic* formats, e.g., syslog, where the message can basically be any other\\nexisting format.\\n:::\\n\\n## Tidbits\\n\\nThis VAST release contains a fair amount of other changes and interesting\\nimprovements. As always, the [changelog][changelog] contains a complete list of\\nuser-facing changes since the last release.\\n\\nHere are some entries that we want to highlight:\\n\\n[changelog]: https://vast.io/changelog#v303\\n\\n### Removing Empty Fields from JSON Output\\n\\nThe `vast export json` command gained new options in addition to the already\\nexisting `--omit-nulls`: Pass `--omit-empty-records`, `--omit-empty-lists`,\\nor `--omit-empty-maps` to cause VAST not to display empty records, lists, or\\nmaps respectively.\\n\\nThe flag `--omit-empty` empty combines the three new options and `--omit-nulls`,\\nessentially causing VAST not to render empty values at all. To set these options\\nglobally, add the following to your vast.yaml configuration file:\\n\\n```yaml\\nvast:\\n  export:\\n    json:\\n      # Always omit empty records and lists when using the JSON export format,\\n      # but keep empty lists and maps.\\n      omit-nulls: true\\n      omit-empty-records: true\\n      omit-empty-maps: false\\n      omit-empty-lists: false\\n```\\n\\n### Faster Shutdown\\n\\nVAST processes now shut down faster, which especially improves the performance\\nof the `vast import` and `vast export` commands for small amounts of data\\ningested or quickly finishing queries.\\n\\nTo quantify this, we\'ve created a database with nearly 300M Zeek events, and ran\\nan export of a single event with both VAST v2.4.1 and VAST v3.0 repeatedly.\\n\\n```text {0} title=\\"\u276f vast -qq count --estimate | numfmt --grouping\\"\\n299,759,532\\n```\\n\\n```text {0} title=\\"VAST v2.4.1 \u276f hyperfine --warmup=5 --min-runs=20 \'vast -qq --bare-mode export -n1 null\'\\"\\nBenchmark 1: vast -qq --bare-mode export -n1 null\\n  Time (mean \xb1 \u03c3):     975.5 ms \xb1   4.8 ms    [User: 111.2 ms, System: 51.9 ms]\\n  Range (min \u2026 max):   966.3 ms \u2026 985.3 ms    20 runs\\n```\\n\\n```text {0} title=\\"VAST v3.0 \u276f hyperfine --warmup=5 --min-runs=20 \'vast -qq export -n1 null\'\\"\\nBenchmark 1: vast -qq --bare-mode export -n1 null\\n  Time (mean \xb1 \u03c3):     210.8 ms \xb1   3.5 ms    [User: 99.8 ms, System: 42.5 ms]\\n  Range (min \u2026 max):   204.1 ms \u2026 217.1 ms    20 runs\\n```\\n\\n### Connection Stability\\n\\nVAST clients may now be started before the VAST server: Client processes now\\nattempt to connect to server processes repeatedly until the configured\\nconnection timeout expires.\\n\\nWe found this to generally improve reliability of services with multiple VAST\\nclients, for which we often encountered problems with VAST clients being unable\\nto connect to a VAST server when started before or immediately after the VAST\\nserver.\\n\\nAdditionally, we\'ve fixed a bug that caused VAST to crash when thousands of\\nclients attempted to connect at around the same time.\\n\\n### Slim Docker Image\\n\\nThe new `tenzir/vast-slim` Docker image is an alternative to the existing\\n`tenzir/vast` Docker image that comes in at just under 40 MB in size\u2014less than a\\nthird than the regular image, making it even quicker to get started with VAST.\\n\\n### Bundled Python Bindings\\n\\nVAST installations now include Python bindings to VAST as a site package. The\\npackage is called `vast` and also available [separately on PyPI][vast-pypi].\\n\\n[vast-pypi]: https://pypi.org/project/pyvast\\n\\n### Expression Short Forms\\n\\nExtractors can now be used where predicates are expected to test for the\\nexistance of a field or type. For example, `x` and `:T` expand to `x != null`\\nand `:T != null`, respectively. This pairs nicely with the already existing\\nshort forms for values, e.g., `\\"foo\\"` expands to `:string == \\"foo`."},{"id":"/from-slack-to-discord","metadata":{"permalink":"/blog/from-slack-to-discord","source":"@site/blog/from-slack-to-discord/index.md","title":"From Slack to Discord","description":"Moving our community chat from Slack to Discord","date":"2023-02-09T00:00:00.000Z","formattedDate":"February 9, 2023","tags":[{"label":"community","permalink":"/blog/tags/community"},{"label":"chat","permalink":"/blog/tags/chat"},{"label":"discord","permalink":"/blog/tags/discord"}],"readingTime":0.785,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"From Slack to Discord","description":"Moving our community chat from Slack to Discord","authors":"mavam","image":"/img/blog/slack-to-discord.excalidraw.svg","date":"2023-02-09T00:00:00.000Z","tags":["community","chat","discord"]},"prevItem":{"title":"VAST v3.0","permalink":"/blog/vast-v3.0"},"nextItem":{"title":"The New REST API","permalink":"/blog/the-new-rest-api"}},"content":"We are moving our community chat from Slack to Discord. Why? TL;DR: because\\nDiscord has better support for community building. VAST is not the first project\\nthat abandons Slack. [Numerous][meilisearch] [open-source][appwrite]\\n[projects][deepset] [have][sst] [done][qovery] [the][neo4j] [same][discord-oss].\\n\\n[meilisearch]: https://blog.meilisearch.com/from-slack-to-discord-our-migration/\\n[appwrite]: https://appwrite.io/\\n[deepset]: https://www.deepset.ai/blog/migration-to-discord\\n[sst]: https://sst.dev/blog/moving-to-discord.html\\n[qovery]: https://www.qovery.com/blog/feedback-from-slack-to-discord-13-months-later\\n[neo4j]: https://neo4j.com/blog/neo4j-community-is-migrating-from-slack-to-discord/\\n[discord-oss]: https://discord.com/open-source\\n\\n\x3c!--truncate--\x3e\\n\\n![Slack-to-Discord](/img/blog/slack-to-discord.excalidraw.svg)\\n\\n:::info Discord Invite Link\\nYou can join our Discord community chat via <https://discord.tenzir.com>.\\n:::\\n\\nHere are the top four reasons why we are switching:\\n\\n- **Retention**: Slack\'s free plan has only 90 days message retention. We prefer\\n  permanence of our community discussion.\\n\\n- **Moderation**: Discord has solid moderation tools that rely on role-based\\n  access, and makes it possible adhere to our [Code of\\n  Conduct](/next/contribute/code-of-conduct) upon joining.\\n\\n- **Invitation**: Unlimited invite links that do not expire.\\n\\n- **Inclusion**: Users can self-assign their preferred pronouns.\\n\\nWe hope that the majority of our Slack users understand these concerns and\\nwill join us over at Discord. See you there!"},{"id":"/the-new-rest-api","metadata":{"permalink":"/blog/the-new-rest-api","source":"@site/blog/the-new-rest-api/index.md","title":"The New REST API","description":"As of v2.4 VAST ships with a new web plugin that","date":"2023-01-26T00:00:00.000Z","formattedDate":"January 26, 2023","tags":[{"label":"frontend","permalink":"/blog/tags/frontend"},{"label":"rest","permalink":"/blog/tags/rest"},{"label":"api","permalink":"/blog/tags/api"},{"label":"architecture","permalink":"/blog/tags/architecture"}],"readingTime":6.89,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"The New REST API","authors":["lava","mavam"],"date":"2023-01-26T00:00:00.000Z","last_updated":"2023-12-12T00:00:00.000Z","image":"/img/blog/rest-api-deployment-single.excalidraw.svg","tags":["frontend","rest","api","architecture"]},"prevItem":{"title":"From Slack to Discord","permalink":"/blog/from-slack-to-discord"},"nextItem":{"title":"Parquet & Feather: Data Engineering Woes","permalink":"/blog/parquet-and-feather-data-engineering-woes"}},"content":"As of [v2.4](/blog/vast-v2.4) VAST ships with a new `web` plugin that\\nprovides a REST API. The [API documentation](/api) describes the\\navailable endpoints also provides an\\n[OpenAPI](https://spec.openapis.org/oas/latest.html) spec for download. This\\nblog post shows how we built the API and what you can do with it.\\n\\n\x3c!--truncate--\x3e\\n\\nWhy does VAST need a REST API? Two reasons:\\n\\n1. **Make it easy to integrate with VAST**. To date, the only interface to VAST\\n   is the command line. This is great for testing and ad-hoc use cases, but to\\n   make it easy for other tools to integrate with VAST, a REST API is the common\\n   expectation.\\n\\n2. **Develop our own web frontend**. We are in the middle of building a\\n   [Svelte](https://svelte.dev/) frontend that delivers a web-based experience\\n   of interacting with VAST through the browser. This frontend interacts with\\n   VAST through the REST API.\\n\\nTwo architectural features of VAST made it really smooth to design the REST API:\\nPlugins and Actors.\\n\\nFirst, VAST\'s plugin system offers a flexible extension mechanism to add\\nadditional functionality without bloating the core. Specifically, we chose\\n[RESTinio](https://github.com/Stiffstream/restinio) as C++ library that\\nimplements an asynchronous HTTP and WebSocket server. Along with it comes a\\ndependency on Boost ASIO. We deem it acceptable to have this dependency of the\\n`web` plugin, but would feel less comfortable with adding dependencies to the\\nVAST core, which we try to keep as lean as possible.\\n\\nSecond, the actor model architecture of VAST makes it easy to\\nintegrate new \\"microservices\\" into the system. The `web` plugin is a *component\\nplugin* that provides a new actor with a typed messaging interface. It neatly\\nfits into the existing architecture and thereby inherits the flexible\\ndistribution and scaling properties. Concretely, there exist two ways to run the\\nREST API actor: either as a separate process or embedded inside a VAST server\\nnode:\\n\\n![REST API - Single Deployment](rest-api-deployment-single.excalidraw.svg)\\n\\nRunning the REST API as dedicated process gives you more flexibility with\\nrespect to deployment, fault isolation, and scaling. An embedded setup offers\\nhigher throughput and lower latency between the REST API and the other VAST\\ncomponents.\\n\\nThe REST API is also a *command plugin* and exposes the\u2014you guessed it\u2014`web`\\ncommand. To run the REST API as dedicated process, spin up a VAST node as\\nfollows:\\n\\n```bash\\nvast web server --certfile=/path/to/server.certificate --keyfile=/path/to/private.key\\n```\\n\\nTo run the server within the main VAST process, use a `start` command:\\n\\n```bash\\nvast start --commands=\\"web server [...]\\"\\n```\\n\\nThe server will only accept TLS requests by default. To allow clients to connect\\nsuccessfully, you need to pass a valid certificate and corresponding private key\\nwith the `--certfile` and `--keyfile` arguments.\\n\\n## Authentication\\n\\nClients must authenticate all requests with a valid token. The token is a short\\nstring that clients put in the `X-VAST-Token` request header.\\n\\nYou can generate a valid token on the command line as follows:\\n\\n```bash\\nvast web generate-token\\n```\\n\\nFor local testing and development, generating suitable certificates and tokens\\ncan be a hassle. For this scenario, you can start the server in [developer\\nmode](#developer-mode) where it accepts plain HTTP connections and does not\\nperform token authentication.\\n\\n## TLS Modes\\n\\nThere exist four modes to start the REST API, each of which suits a slightly\\ndifferent use case.\\n\\n### Developer Mode\\n\\nThe developer mode bypasses encryption and authentication token verification.\\n\\n![REST API - Developer Mode](rest-api-mode-developer.excalidraw.svg)\\n\\nPass `--mode=dev` to start the REST API in developer mode:\\n\\n```bash\\nvast web server --mode=dev\\n```\\n\\n### Server Mode\\n\\nThe server mode reflects the \\"traditional\\" mode of operation where VAST binds to\\na network interface. This mode only accepts HTTPS connections and requires a\\nvalid authentication token for every request. This is the default mode of\\noperation.\\n\\n![REST API - Server Mode](rest-api-mode-server.excalidraw.svg)\\n\\nPass `--mode=server` to start the REST API in server mode:\\n\\n```bash\\nvast web server --mode=server\\n```\\n\\n### Upstream TLS Mode\\n\\nThe upstream TLS mode is suitable when VAST sits upstream of a separate\\nTLS terminator that is running on the same machine. This kind of setup\\nis commonly encountered when running nginx as a reverse proxy.\\n\\n![REST API - Developer Mode](rest-api-mode-developer.excalidraw.svg)\\n\\nVAST only listens on localhost addresses, accepts plain HTTP but still\\nchecks authentication tokens.\\n\\nPass `--mode=upstream` to start the REST API in server mode:\\n\\n```bash\\nvast web server --mode=upstream\\n```\\n\\n### Mutual TLS Mode\\n\\nThe mutual TLS mode is suitable when VAST sits upstream of a separate TLS\\nterminator that may be running on a different machine. In this scenario,\\nthe connection between the terminator and VAST must again be encrypted\\nto avoid leaking the authentication token to the network.\\n\\nRegular TLS requires only the server to present a certificate to prove his\\nidentity. In mutual TLS mode, the client additionally needs to provide a\\nvalid *client certificate* to the server. This ensures that the TLS terminator\\ncannot be impersonated or bypassed.\\n\\nTypically self-signed certificates are used for that purpose, since both ends of\\nthe connection are configured together and not exposed to the public internet.\\n\\n![REST API - mTLS Mode](rest-api-mode-mtls.excalidraw.svg)\\n\\nPass `--mode=mtls` to start the REST API in mutual TLS mode:\\n\\n```bash\\nvast web server --mode=mtls\\n```\\n\\n## Usage Examples\\n\\nNow that you know how we put the REST API together, let\'s look at some\\nend-to-end examples.\\n\\n### See what\'s inside VAST\\n\\nOne straightforward example is checking the number of records in VAST:\\n\\n```bash\\ncurl \\"https://vast.example.org:42001/api/v0/status?verbosity=detailed\\" \\\\\\n  | jq .index.statistics\\n```\\n\\n```json\\n{\\n  \\"events\\": {\\n    \\"total\\": 8462\\n  },\\n  \\"layouts\\": {\\n    \\"zeek.conn\\": {\\n      \\"count\\": 8462,\\n      \\"percentage\\": 100\\n    }\\n  }\\n}\\n```\\n\\n:::caution Status changes in v3.0\\nIn the upcoming v3.0 release, the statistics under the key `.index.statistics`\\nwill move to `.catalog`. This change is already merged into the master branch.\\nConsult the status key reference for details.\\n:::\\n\\n### Perform a HTTP health check\\n\\nThe `/status` endpoint can also be used as a HTTP health check in\\n`docker-compose`:\\n\\n```yaml\\nversion: \'3.4\'\\nservices:\\n  web:\\n    image: tenzir/vast\\n    environment:\\n      - \\"VAST_START__COMMANDS=web server --mode=dev\\"\\n    ports:\\n      - \\"42001:42001\\"\\n    healthcheck:\\n      test: curl --fail http://localhost:42001/status || exit 1\\n      interval: 60s\\n      retries: 5\\n      start_period: 20s\\n      timeout: 10s\\n```\\n\\n### Run a query\\n\\nThe other initial endpoints can be used to get data out of VAST. For example, to\\nget up to two `zeek.conn` events which connect to the subnet `192.168.0.0/16`, using\\nthe VAST query expression `net.src.ip in 192.168.0.0/16`:\\n\\n```bash\\ncurl \\"http://127.0.0.1:42001/api/v0/export?limit=2&expression=net.src.ip%20in%20192.168.0.0%2f16\\"\\n```\\n\\n```json\\n{\\n  \\"version\\": \\"v2.4.0-457-gb35c25d88a\\",\\n  \\"num_events\\": 2,\\n  \\"events\\": [\\n    {\\n      \\"ts\\": \\"2009-11-18T08:00:21.486539\\",\\n      \\"uid\\": \\"Pii6cUUq1v4\\",\\n      \\"id.orig_h\\": \\"192.168.1.102\\",\\n      \\"id.orig_p\\": 68,\\n      \\"id.resp_h\\": \\"192.168.1.1\\",\\n      \\"id.resp_p\\": 67,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": null,\\n      \\"duration\\": \\"163.82ms\\",\\n      \\"orig_bytes\\": 301,\\n      \\"resp_bytes\\": 300,\\n      \\"conn_state\\": \\"SF\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"Dd\\",\\n      \\"orig_pkts\\": 1,\\n      \\"orig_ip_bytes\\": 329,\\n      \\"resp_pkts\\": 1,\\n      \\"resp_ip_bytes\\": 328,\\n      \\"tunnel_parents\\": []\\n    },\\n    {\\n      \\"ts\\": \\"2009-11-18T08:08:00.237253\\",\\n      \\"uid\\": \\"nkCxlvNN8pi\\",\\n      \\"id.orig_h\\": \\"192.168.1.103\\",\\n      \\"id.orig_p\\": 137,\\n      \\"id.resp_h\\": \\"192.168.1.255\\",\\n      \\"id.resp_p\\": 137,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": \\"dns\\",\\n      \\"duration\\": \\"3.78s\\",\\n      \\"orig_bytes\\": 350,\\n      \\"resp_bytes\\": 0,\\n      \\"conn_state\\": \\"S0\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"D\\",\\n      \\"orig_pkts\\": 7,\\n      \\"orig_ip_bytes\\": 546,\\n      \\"resp_pkts\\": 0,\\n      \\"resp_ip_bytes\\": 0,\\n      \\"tunnel_parents\\": []\\n    }\\n  ]\\n}\\n```\\n\\nNote that when using `curl`, all request parameters need to be properly\\nurlencoded. This can be cumbersome for the `expression` and `pipeline`\\nparameters, so we also provide an `/export` POST endpoint that accepts\\nparameters in the JSON body. The next example shows how to use POST requests\\nfrom curl. It also uses the `/query` endpoint instead of `/export` to get\\nresults iteratively instead of a one-shot result. The cost for this is having to\\nmake two API calls instead of one:\\n\\n```bash\\ncurl -XPOST -H\\"Content-Type: application/json\\" -d\'{\\"expression\\": \\"udp\\"}\' http://127.0.0.1:42001/api/v0/query/new\\n```\\n\\n```json\\n{\\"id\\": \\"31cd0f6c-915f-448e-b64a-b5ab7aae2474\\"}\\n```\\n\\n```bash\\ncurl http://127.0.0.1:42001/api/v0/query/31cd0f6c-915f-448e-b64a-b5ab7aae2474/next?n=2 | jq\\n```\\n\\n```json\\n{\\n  \\"position\\": 0,\\n  \\"events\\": [\\n    {\\n      \\"ts\\": \\"2009-11-18T08:00:21.486539\\",\\n      \\"uid\\": \\"Pii6cUUq1v4\\",\\n      \\"id.orig_h\\": \\"192.168.1.102\\",\\n      \\"id.orig_p\\": 68,\\n      \\"id.resp_h\\": \\"192.168.1.1\\",\\n      \\"id.resp_p\\": 67,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": null,\\n      \\"duration\\": \\"163.82ms\\",\\n      \\"orig_bytes\\": 301,\\n      \\"resp_bytes\\": 300,\\n      \\"conn_state\\": \\"SF\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"Dd\\",\\n      \\"orig_pkts\\": 1,\\n      \\"orig_ip_bytes\\": 329,\\n      \\"resp_pkts\\": 1,\\n      \\"resp_ip_bytes\\": 328,\\n      \\"tunnel_parents\\": []\\n    },\\n    {\\n      \\"ts\\": \\"2009-11-18T08:08:00.237253\\",\\n      \\"uid\\": \\"nkCxlvNN8pi\\",\\n      \\"id.orig_h\\": \\"192.168.1.103\\",\\n      \\"id.orig_p\\": 137,\\n      \\"id.resp_h\\": \\"192.168.1.255\\",\\n      \\"id.resp_p\\": 137,\\n      \\"proto\\": \\"udp\\",\\n      \\"service\\": \\"dns\\",\\n      \\"duration\\": \\"3.78s\\",\\n      \\"orig_bytes\\": 350,\\n      \\"resp_bytes\\": 0,\\n      \\"conn_state\\": \\"S0\\",\\n      \\"local_orig\\": null,\\n      \\"missed_bytes\\": 0,\\n      \\"history\\": \\"D\\",\\n      \\"orig_pkts\\": 7,\\n      \\"orig_ip_bytes\\": 546,\\n      \\"resp_pkts\\": 0,\\n      \\"resp_ip_bytes\\": 0,\\n      \\"tunnel_parents\\": []\\n    }\\n  ]\\n}\\n```\\n\\n:::note Still Experimental\\nPlease note that we consider the API version `v0` experimental, and we make no\\nstability guarantees at the moment.\\n:::\\n\\nAs always, if you have any question on usage, swing by our [community\\nchat](/discord). Missing routes? Let us know so that we know\\nwhat to prioritize. Now happy curling! :curling_stone:"},{"id":"/parquet-and-feather-data-engineering-woes","metadata":{"permalink":"/blog/parquet-and-feather-data-engineering-woes","source":"@site/blog/parquet-and-feather-data-engineering-woes/index.md","title":"Parquet & Feather: Data Engineering Woes","description":"Apache Arrow and [Apache","date":"2023-01-10T00:00:00.000Z","formattedDate":"January 10, 2023","tags":[{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"}],"readingTime":7.09,"hasTruncateMarker":true,"authors":[{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Data Engineering Woes","authors":"dispanser","date":"2023-01-10T00:00:00.000Z","tags":["arrow","parquet","feather"]},"prevItem":{"title":"The New REST API","permalink":"/blog/the-new-rest-api"},"nextItem":{"title":"VAST v2.4.1","permalink":"/blog/vast-v2.4.1"}},"content":"[Apache Arrow](https://arrow.apache.org/) and [Apache\\nParquet](https://parquet.apache.org) have become the de-facto columnar formats\\nfor in-memory and on-disk representations when it comes to structured data.\\nBoth are strong together, as they provide data interoperability and foster a\\ndiverse ecosystem of data tools. But how well do they actually work together\\nfrom an engineering perspective?\\n\\n\x3c!--truncate--\x3e\\n\\nIn our previous posts, we introduced the formats and did a quantitative\\ncomparison of Parquet and Feather-on the write path. In this post, we look at\\nthe developer experience.\\n\\n:::info Parquet & Feather: 3/3\\nThis blog post is the last part of a 3-piece series on Parquet and Feather.\\n\\n1. [Enabling Open Investigations][parquet-and-feather-1]\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. This blog post\\n\\n[parquet-and-feather-1]: /blog/parquet-and-feather-enabling-open-investigations/\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n:::\\n\\nWhile our Feather implementation proved to be straight-forward, the Parquet\\nstore implementation turned out to be more difficult. Recall that VAST has its\\nown type system relying on [Arrow extension\\ntypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types) to\\nexpress domain-specific concepts like IP addresses, subnets, or enumerations. We\\nhit a few places where the Arrow C++ implementation does not support all VAST\\ntypes directly. It\'s trickier than we thought, as we see next.\\n\\n## Row Groups\\n\\nIn Apache Parquet, a [row group](https://parquet.apache.org/docs/concepts/) is a\\nsubset of a Parquet file that\'s itself written in a columnar fashion. Smaller\\nrow groups allow for higher granularity in reading parts of an individual file,\\nat the expense of a potentially increased file size due to less optimal\\nencoding. In VAST, we send around batches of data\\nthat are considerably smaller than what a recommended Parquet file size would\\nlook like. A typical Parquet file size recommendation is 1GB, which translates\\nto 5\u201310GB of data in memory when reading the entire file. To produce files sized\\nin this order of magnitude, we planned to use individual row groups, each of\\nwhich aligned with the size of our Arrow record batches that comprise\\n2<sup>16</sup> events occupying a few MBs.\\n\\nHowever, attempting to read a Parquet file that was split into multiple row\\ngroups doesn\'t work for some of our schemas, yielding:\\n\\n```\\nNotImplemented: Nested data conversions not implemented for chunked array outputs\\n```\\n\\nThis appears to be related to\\n[ARROW-5030](https://issues.apache.org/jira/browse/ARROW-5030). Our current\\nworkaround is to write a single row group, and split up the resulting Arrow\\nrecord batches into the desired size after reading. However, this increases\\nlatency to first result, an important metric for some interactive use cases we\\nenvision for VAST.\\n\\n## Arrow \u2192 Parquet \u2192 Arrow Roundtrip Schema Mismatch\\n\\nParquet is a separate project which precedes Arrow, and has its own data types,\\nwhich don\'t exactly align with what Arrow provides. While it\'s possible to\\ninstruct Arrow to also serialize [its own\\nschema](https://arrow.apache.org/docs/cpp/api/formats.html#_CPPv4N7parquet21ArrowWriterProperties7BuilderE)\\ninto the Parquet file metadata, this doesn\'t seem to play well in concert with\\nextension types. As a result, a record batch written to and then read from a\\nParquet file no longer adheres to the same schema!\\n\\nThis bit us in the following scenarios.\\n\\n### VAST Enumerations\\n\\nVAST comes with an enumeration type that represents a fixed mapping of strings\\nto numeric values, where the mapping is part of the type metadata. We represent\\nenums as extension types wrapping an Arrow dictionary of strings backed by\\nunsigned 8-bit integers. On read, Arrow turns these 8-bit index values into\\n32-bit values, which is not compatible with our extension type definition, so\\nthe extension type wrapping is lost. The diagram below illustrates this issue.\\n\\n![Arrow Schema Conversion](arrow-schema-conversion.excalidraw.svg)\\n\\n### Extension Types inside Maps\\n\\nBoth our address type and subnet type extensions are lost if they occur in\\nnested records. For example, a map from a VAST address to a VAST enumeration of\\nthe following Arrow type is not preserved:\\n\\n```\\nmap<extension<vast.address>, extension<vast.enumeration>>\\n```\\n\\nAfter reading it from a Parquet file, the resulting Arrow type is:\\n\\n```\\nmap<fixed_size_binary[16], string>.\\n```\\n\\nThe key, an address type, has been replaced by its physical representation,\\nwhich is 16 bytes (allowing room for an IPv6 address). Interestingly, the\\nenumeration is replaced by a string instead of a dictionary as observed in the\\nprevious paragraph. So the same type behaves differently depending on where in\\nthe schema it occurs.\\n\\nWe created an issue in the Apache JIRA to track this:\\n[ARROW-17839](https://issues.apache.org/jira/browse/ARROW-17839).\\n\\nTo fix these 3 issues, we\'re post-processing the data after reading it from\\nParquet. The workaround is a multi-step process:\\n\\n1. Side-load the Arrow schema from the Parquet metadata. This yields the actual\\n   schema, because it\'s in no way related to Parquet other than using its\\n   metadata capabilities to store it.\\n\\n1. Load the actual Arrow table. This table has its own schema, which is not the\\n   same schema as the one derived from the Parquet metadata directly.\\n\\n1. Finally, recursively walk the two schema trees with the associated data\\n   columns, and whenever there\'s a mismatch between the two, fix the data arrays\\n   by casting or transforming it, yielding a table that is aligned with the\\n   expected schema.\\n\\n   - In the first case (`dictionary` vs `vast.enumeration`) we cast the `int32`\\n     Arrow array of values into a `uint8` Arrow array, and manually create the\\n     wrapping extension type and extension array. This is relatively cheap, as\\n     casting is cheap and the wrapping is done at the array level, not the value\\n     level.\\n\\n   - In the second case (physical `binary[16]` instead of `vast.address`) we\\n     just wrap it in the appropriate extension type. Again, this is a cheap\\n     operation.\\n\\n   - The most expensive fix-up we perform is when the underlying type has been\\n     changed from an enumeration to a string: we have to create the entire array\\n     from scratch after building a lookup table that translates the string values\\n     into their corresponding numerical representation.\\n\\n## Apache Spark Support\\n\\nSo now VAST writes its data into a standardized, open format\u2014we integrate\\nseamlessly with the entire big data ecosystem, for free, right? I can read my\\nVAST database with Apache Spark and analyze security telemetry data on a\\n200-node cluster?\\n\\nNope. It\u2019s not *that* standardized. Yet. Not every tool or library supports\\nevery data type. In fact, as discussed above, writing a Parquet file and reading\\nit back *even with the same tool* doesn\'t always produce the data you started\\nwith.\\n\\nWe attempting to load a Parquet file with a single row, and a single field of\\ntype VAST\'s `count` (a 64-bit unsigned integer) into Apache Spark v3.2, we are\\ngreeted with:\\n\\n```\\norg.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false))\\n  at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:1284)\\n```\\n\\nApache Spark v3.2 refuses to read the `import_time` field (a metadata column\\nadded by VAST itself). It turns out that Spark v3.2 has a\\n[regression](https://issues.apache.org/jira/browse/SPARK-40819). Let\'s try with\\nversion v3.1 instead, which shouldn\u2019t have this problem:\\n\\n```\\norg.apache.spark.sql.AnalysisException: Parquet type not supported: INT64 (UINT_64)\\n```\\n\\nWe got past the timestamp issue, but it still doesn\'t work: Spark only supports\\nsigned integer types, and refuses to load our Parquet file with an unsigned 64\\nbit integer value. The [related Spark JIRA\\nissue](https://issues.apache.org/jira/browse/SPARK-10113) is marked as resolved,\\nbut unfortunately the resolution is \\"a better error message.\\" However, [this\\nstack overflow post](https://stackoverflow.com/q/64383029) has the solution: if\\nwe define an explicit schema, Spark happily converts our column into a signed\\ntype.\\n\\n```scala\\nval schema = StructType(\\n  Array(\\n    StructField(\\"event\\",\\n      StructType(\\n        Array(\\n          StructField(\\"c\\", LongType))))))\\n```\\n\\nFinally, it works!\\n\\n```\\nscala> spark.read.schema(schema).parquet(<file>).show()\\n+-----+\\n|event|\\n+-----+\\n| {13}|\\n+-----+\\n```\\n\\nWe were able to read VAST data in Spark, but it\'s not an easy and out-of-the-box\\nexperience we were hoping for. It turns out that different tools don\'t always\\nsupport all the data types, and additional effort is required to integrate with\\nthe big players in the Parquet ecosystem.\\n\\n## Conclusion\\n\\nWe love Apache Arrow\u2014it\'s a cornerstone of our system, and we\'d be in much\\nworse shape without it. We use it everywhere from the storage layer (using\\nFeather and Parquet) to the data plane (where we are passing around Arrow record\\nbatches).\\n\\nHowever, as VAST uses a few less common Arrow features we sometimes stumble over\\nsome of the rougher edges. We\'re looking forward to fixing some of these things\\nupstream, but sometimes you just need a quick solution to help our users.\\n\\nThe real reason why we wrote this blog post is to show how quickly the data\\nengineering can escalate. This is the long tail that nobody wants to talk about\\nwhen telling you to build your own security data lake. And it quickly adds up!\\nIt\'s also heavy-duty data wrangling, and not ideally something you want your\\nsecurity team working on when they would be more useful hunting threats. Even\\nmore reasons to use a purpose-built security data technology like VAST."},{"id":"/vast-v2.4.1","metadata":{"permalink":"/blog/vast-v2.4.1","source":"@site/blog/vast-v2.4.1/index.md","title":"VAST v2.4.1","description":"Faster Query Taste","date":"2022-12-19T00:00:00.000Z","formattedDate":"December 19, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":1.685,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.4.1","description":"Faster Query Taste","authors":"dominiklohmann","date":"2022-12-19T00:00:00.000Z","tags":["release","feather","performance"]},"prevItem":{"title":"Parquet & Feather: Data Engineering Woes","permalink":"/blog/parquet-and-feather-data-engineering-woes"},"nextItem":{"title":"VAST v2.4","permalink":"/blog/vast-v2.4"}},"content":"[VAST v2.4.1][github-vast-release] improves the performance of queries when VAST\\nis under high load, and significantly reduces the time to first result for\\nqueries with a low selectivity.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.4.1\\n\\n\x3c!--truncate--\x3e\\n\\n## Reading Feather Files Incrementally\\n\\nVAST\'s Feather store na\xefvely used the [Feather reader][feather-reader] from the\\nApache Arrow C++ library in its initial implementation. However, its API is\\nrather limited: It does not support reading record batches incrementally. We\'ve\\nswapped this out with a more efficient implementation that does.\\n\\n[feather-reader]: https://github.com/apache/arrow/blob/apache-arrow-10.0.1/cpp/src/arrow/ipc/feather.h#L57-L108\\n\\nThis is best explained visually:\\n\\n![Incremental Reads](incremental-reads.excalidraw.svg)\\n\\nWithin the scope of a single Feather store file, a single query takes the same\\namount of time overall, but there exist two distinct advantages of this\\napproach:\\n\\n1. The first result arrives much faster at the client.\\n2. Stores do less work for cancelled queries.\\n\\nOne additional benefit that is not immediately obvious comes into play when\\nqueries arrives at multiple stores in parallel: disk reads are more evenly\\nspread out now, making them less likely to overlap between stores. For\\ndeployments with slower I/O paths this can lead to a significant query\\nperformance improvement.\\n\\nTo verify and test this, we\'ve created a VAST database with 300M Zeek events\\n(33GB on disk) from a Corelight sensor. All tests were performed on a cold start\\nof VAST, i.e., we stopped and started VAST after every repetition of each test.\\n\\nWe performed three tests:\\n\\n1. Export a single event (20 times)\\n2. Export all events (20 times)\\n3. Rebuild the entire database (3 times)\\n\\nThe results are astonishingly good:\\n\\n|Test|Benchmark|v2.4.0|v2.4.1|Improvement|\\n|:-:|:-:|:-:|:-:|:-:|\\n|**(1)**|Avg. store load time|55.1ms|4.2ms|13.1x|\\n||Time to first result/Total time|19.8ms|14.5ms|1.4x|\\n|**(2)**|Avg. store load time|386.5ms|7.3ms|52.9x|\\n||Time to first result|69.2ms|25.4ms|2.7x|\\n||Total time|39.38s|33.30s|1.2x|\\n|**(3)**|Avg. store load time|480.3ms|9.1ms|52.7x|\\n||Total time|210.5s|198.0s|1.1x|\\n\\nIf you\'re using the Feather store backend (the default as of v2.4.0), you will\\nsee an immediate improvement with VAST v2.4.1. There are no other changes\\nbetween the two releases.\\n\\n:::info Parquet Stores\\nVAST also offers an experimental Parquet store backend, for which we plan to\\nmake a similar improvement in a coming release.\\n:::"},{"id":"/vast-v2.4","metadata":{"permalink":"/blog/vast-v2.4","source":"@site/blog/vast-v2.4/index.md","title":"VAST v2.4","description":"Open Storage","date":"2022-12-09T00:00:00.000Z","formattedDate":"December 9, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"frontend","permalink":"/blog/tags/frontend"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"docker","permalink":"/blog/tags/docker"},{"label":"python","permalink":"/blog/tags/python"},{"label":"arrow","permalink":"/blog/tags/arrow"}],"readingTime":4.195,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.4","description":"Open Storage","authors":"dominiklohmann","date":"2022-12-09T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","tags":["release","frontend","feather","parquet","docker","python","arrow"]},"prevItem":{"title":"VAST v2.4.1","permalink":"/blog/vast-v2.4.1"},"nextItem":{"title":"Parquet & Feather: Writing Security Telemetry","permalink":"/blog/parquet-and-feather-writing-security-telemetry"}},"content":"[VAST v2.4][github-vast-release] completes the switch to open storage formats,\\nand includes an early peek at three upcoming features for VAST: A web plugin\\nwith a REST API and an integrated frontend user interface, Docker Compose\\nconfiguration files for getting started with VAST faster and showing how to\\nintegrate VAST into your SOC, and new Python bindings that will make writing\\nintegrations easier and allow for using VAST with your data science libraries,\\nlike Pandas.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.4.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Preventing Vendor Lock-in with Open Storage\\n\\nVAST\'s Apache Feather (V2) and Apache Parquet storage backends are now\\nconsidered stable, and the default storage format is now Feather. This marks the\\nbeginning of a new era for VAST for all users: There is no more vendor lock-in\\nof your data!\\n\\nBoth as engineers and users of software we disdain vendor lock-in. Your data is\\nyours and no tool should hold it hostage. We want you to choose VAST because\\nit\'s the best engine when building a sustainable security data architecture. In\\nother words, *VAST decouples data acquisition from downstream security\\nanalytics*. To this end, we are not only committed to open source, but also to\\nopen standards\u2014for storage and processing.\\n\\nAs of this release, VAST no longer supports *writing* to its old proprietary\\nstorage format, but will still support *reading* from it until the next major\\nrelease. In the background, VAST transparently rebuilds old partitions to take\\nadvantage of the new format without any downtime. This may cause some additional\\nload when starting VAST first up after the update, but ensures that queries run\\nas fast as possible once all old partitions have been converted.\\n\\nIf you want to know more about Feather and Parquet, check out our in-depth blog\\npost series on them:\\n\\n1. [Enabling Open Investigations][parquet-and-feather-1]\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. [Data Engineering Woes][parquet-and-feather-3]\\n\\n[parquet-and-feather-1]: /blog/parquet-and-feather-enabling-open-investigations/\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n[parquet-and-feather-3]: /blog/parquet-and-feather-data-engineering-woes/\\n\\n## What\'s Next?\\n\\nVAST v2.4 contains a few new and experimental toys to play with. Here\'s an\\noverview of what they are, and how they all make it easier to integrate VAST\\nwith other security tools.\\n\\n### Docker Compose\\n\\nA new set of Docker Compose files makes it easier than ever to\\nget started with VAST. This is not designed for high-performance deployments of\\nVAST, but rather to make it easier to try VAST out\u2014all-batteries included,\\nbecause we want to use this to showcase and test the myriad of integrations\\nin a modern SOC.\\n\\nOur vision for this is to show how VAST as a modular platform can power modern\\nand sustainable approaches to composable security.\\n\\n### REST API and Frontend User Interface\\n\\nThe experimental `web` plugin adds a REST API to VAST, and also a\\nfrontend user interface we [built in Svelte][frontend-code].\\n\\nBoth the API and the frontend are still considered unstable and subject to\\nchange without notice. We plan to stabilize and version the API in the future.\\nFundamentally, the API serves two purposes:\\n\\n1. Make it easier to write integrations with VAST\\n2. Serve as a backend for VAST\'s bundled frontend\\n\\nThe frontend UI currently displays a status page for the installed VAST node.\\n\\n\x3c!--- this weird markup is to render a border around the image ---\x3e\\n![UI showing a status page](vast-ui-experimental.jpg)\\n\\nWe have some exciting features planned for both of these. Stay tuned!\\n\\n[frontend-code]: https://github.com/tenzir/vast/tree/v2.4.0/plugins/web/ui\\n\\n### Python Bindings\\n\\nWe want to make it as easy as possible to integrate VAST with other tools, so\\nwe\'re working on making that as easy as possible using VAST\'s Python bindings.\\nThe new bindings support analyzing data from VAST using industry-standard Python\\nlibraries, like Pandas.\\n\\nThis is all enabled by our commitment to open standards: VAST leverages Apache\\nArrow as its in-memory data representation. The Python bindings make it easy to\\nuse VAST\'s security-specific data types. For example, when running a query, IP\\naddresses, subnets, and patterns automatically convert to the Python-native\\ntypes, as opposed to remaining binary blobs or sheer strings.\\n\\n:::note Not yet on PyPI\\nVAST\'s new Python bindings are not yet on PyPI, as they are still heavily under\\ndevelopment. If you\'re too eager and cannot wait, go [check out the source\\ncode][python-code].\\n:::\\n\\n[python-code]: https://github.com/tenzir/vast/tree/v2.4.0/python\\n\\n## Other Noteworthy Changes\\n\\nA full list of changes to VAST since the last release is available in the\\n[changelog][changelog-2.4]. Here\'s a selection of changes that are particularly\\nnoteworthy:\\n\\n- VAST now loads all plugins by default. When asking new users for pitfalls they\\n  encountered, this ranked pretty high on the list of things we needed to\\n  change. To revert to the old behavior, set `vast.plugins: []` in your\\n  configuration file, or set `VAST_PLUGINS=` in your environment.\\n- The default endpoint changed from `localhost` to `127.0.0.1` to ensure a\\n  deterministic listening address.\\n- Exporting VAST\'s performance metrics via UDS no longer deadlocks VAST\'s\\n  metrics exporter when a listener is suspended.\\n- VAST\'s build process now natively supports building Debian packages. This\\n  makes upgrades for bare-metal deployments a breeze. As of this release, our\\n  CI/CD pipeline automatically attaches a Debian package in addition to the\\n  build archive to our releases.\\n\\n[changelog-2.4]: /changelog#v240"},{"id":"/parquet-and-feather-writing-security-telemetry","metadata":{"permalink":"/blog/parquet-and-feather-writing-security-telemetry","source":"@site/blog/parquet-and-feather-writing-security-telemetry/index.md","title":"Parquet & Feather: Writing Security Telemetry","description":"How does Apache Parquet compare to Feather for storing","date":"2022-10-24T00:00:00.000Z","formattedDate":"October 24, 2022","tags":[{"label":"benchmark","permalink":"/blog/tags/benchmark"},{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"},{"label":"quarto","permalink":"/blog/tags/quarto"},{"label":"r","permalink":"/blog/tags/r"}],"readingTime":26.585,"hasTruncateMarker":true,"authors":[{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"},{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Parquet & Feather: Writing Security Telemetry","authors":["dispanser","mavam"],"date":"2022-10-24T00:00:00.000Z","last_updated":"2023-02-08T00:00:00.000Z","tags":["benchmark","arrow","parquet","feather","quarto","r"]},"prevItem":{"title":"VAST v2.4","permalink":"/blog/vast-v2.4"},"nextItem":{"title":"VAST v2.3.1","permalink":"/blog/vast-v2.3.1"}},"content":"How does Apache [Parquet](https://parquet.apache.org/) compare to [Feather](https://arrow.apache.org/docs/python/feather.html) for storing\\nstructured security data? In this blog post, we answer this question.\\n\\n\x3c!--truncate--\x3e\\n\\n:::info Parquet & Feather: 2/3\\nThis is blog post is part of a 3-piece series on Parquet and Feather.\\n\\n1. [Enabling Open Investigations](/blog/parquet-and-feather-enabling-open-investigations/)\\n2. This blog post\\n3. [Data Engineering Woes](/blog/parquet-and-feather-data-engineering-woes/)\\n\\n:::\\n\\nIn the [previous blog](/blog/parquet-and-feather-enabling-open-investigations/), we explained why Parquet and\\nFeather are great building blocks for modern investigations. In this blog, we take\\na look at how they actually perform on the write path in two dimensions:\\n\\n- **Size**: how much space does typical security telemetry occupy?\\n- **Speed**: how fast can we write out to a store?\\n\\nParquet and Feather have different goals. While Parquet is an on-disk format\\nthat optimizes for size, Feather is a thin layer around the native Arrow\\nin-memory representation. This puts them at different points in the spectrum of\\nthroughput and latency.\\n\\nTo better understand this spectrum, we instrumented the write path of VAST,\\nwhich consists roughly of the following steps:\\n\\n1. Parse the input\\n2. Convert it into Arrow record batches\\n3. Ship Arrow record batches to a VAST server\\n4. Write Arrow record batches out into a Parquet or Feather store\\n5. Create an index from Arrow record batches\\n\\nSince steps (1\u20133) and (5) are the same for both stores, we ignore them in the\\nfollowing analysis and solely zoom in on (4).\\n\\n## Dataset\\n\\nFor our evaluation, we use a dataset that models a \u201cnormal day in a corporate\\nnetwork\u201d fused with data from for real-world attacks. While this approach might\\nnot be ideal for detection engineering, it provides enough diversity to analyze\\nstorage and processing behavior.\\n\\nSpecifically, we rely on a 3.77 GB PCAP trace of the [M57 case study](https://www.sciencedirect.com/science/article/pii/S1742287612000370). We\\nalso injected real-world attacks from\\n[malware-traffic-analysis.net](https://www.malware-traffic-analysis.net/index.html) into the PCAP trace. To\\nmake the timestamps look somewhat realistic, we shifted the timestamps of the\\nPCAPs to pretend that the corresponding activity happens on the same day. For\\nthis we used [`editcap`](https://www.wireshark.org/docs/wsug_html_chunked/AppToolseditcap.html) and then merged the resulting PCAPs into one\\nbig file using [`mergecap`](https://www.wireshark.org/docs/wsug_html_chunked/AppToolsmergecap.html).\\n\\nWe then ran [Zeek](https://zeek.org) and [Suricata](https://suricata.io) over\\nthe trace to produce structured logs. For full reproducibility, we host this\\ncustom data set in a [Google Drive folder](https://drive.google.com/drive/folders/1mPJYVGKTk86P2JU3KD-WFz8tUkTLK095?usp=sharing).\\n\\nVAST can ingest PCAP, Zeek, and Suricata natively. All three data sources are\\nhighly valuable for detection and investigation, which is why we use them in\\nthis analysis. They represent a good mix of nested and structured data (Zeek &\\nSuricata) vs.\xa0simple-but-bulky data (PCAP). To give you a flavor, here\u2019s an\\nexample Zeek log:\\n\\n    #separator \\\\x09\\n    #set_separator  ,\\n    #empty_field    (empty)\\n    #unset_field    -\\n    #path   http\\n    #open   2022-04-20-09-56-45\\n    #fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   trans_depth method  host    uri referrer    version user_agent  origin  request_body_len    response_body_len   status_code status_msg  info_code   info_msg    tags    username    password    proxied orig_fuids  orig_filenames  orig_mime_types resp_fuids  resp_filenames  resp_mime_types\\n    #types  time    string  addr    port    addr    port    count   string  string  string  string  string  string  string  count   count   count   string  count   string  set[enum]   string  string  set[string] vector[string]  vector[string]  vector[string]  vector[string]  vector[string]  vector[string]\\n    1637155963.249475   CrkwBA3xeEV9dzj1n   128.14.134.170  57468   198.71.247.91   80  1   GET 198.71.247.91   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36     -   0   51  200 OK  -   -   (empty) -   -   -   -   -   -   FhEFqzHx1hVpkhWci   -   text/html\\n    1637157241.722674   Csf8Re1mi6gYI3JC6f  87.251.64.137   64078   198.71.247.91   80  1   -   -   -   -   1.1 -   -   0   18  400 Bad Request -   -   (empty) -   -   -   -   -   -   FpKcQG2BmJjEU9FXwh  -   text/html\\n    1637157318.182504   C1q1Lz1gxAAyf4Wrzk  139.162.242.152 57268   198.71.247.91   80  1   GET 198.71.247.91   /   -   1.1 Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0  -   0   51  200 OK  -   -   (empty) -   -   -   -   -   -   FyTOLL1rVGzjXoNAb   -   text/html\\n    1637157331.507633   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  1   GET lifeisnetwork.com   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   51  200 OK  -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   Fnmp6k1xVFoqqIO5Ub  -   text/html\\n    1637157331.750342   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  2   GET lifeisnetwork.com   /   -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   51  200 OK  -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   F1uLr1giTpXx81dP4   -   text/html\\n    1637157331.915255   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  3   GET lifeisnetwork.com   /wp-includes/wlwmanifest.xml    -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   279 404 Not Found   -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   F9dg5w2y748yNX9ZCc  -   text/html\\n    1637157331.987527   C9FzNf12ebDETzvDLk  172.70.135.112  37220   198.71.247.91   80  4   GET lifeisnetwork.com   /xmlrpc.php?rsd -   1.1 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36 -   0   279 404 Not Found   -   -   (empty) -   -   X-FORWARDED-FOR -> 137.135.117.126  -   -   -   FxzLxklm7xyuzTF8h   -   text/html\\n\\nHere\u2019s a snippet of a Suricata log:\\n\\n``` json\\n{\\"timestamp\\":\\"2021-11-17T14:32:43.262184+0100\\",\\"flow_id\\":1129058930499898,\\"pcap_cnt\\":7,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"128.14.134.170\\",\\"src_port\\":57468,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:YXWfTYEyYLKVv5Ge4WqijUnKTrM=\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:32:43.237882+0100\\",\\"flow_id\\":675134617085815,\\"event_type\\":\\"flow\\",\\"src_ip\\":\\"54.176.143.72\\",\\"dest_ip\\":\\"198.71.247.91\\",\\"proto\\":\\"ICMP\\",\\"icmp_type\\":8,\\"icmp_code\\":0,\\"response_icmp_type\\":0,\\"response_icmp_code\\":0,\\"flow\\":{\\"pkts_toserver\\":1,\\"pkts_toclient\\":1,\\"bytes_toserver\\":50,\\"bytes_toclient\\":50,\\"start\\":\\"2021-11-17T14:43:34.649079+0100\\",\\"end\\":\\"2021-11-17T14:43:34.649210+0100\\",\\"age\\":0,\\"state\\":\\"established\\",\\"reason\\":\\"timeout\\",\\"alerted\\":false},\\"community_id\\":\\"1:WHH+8OuOygRPi50vrH45p9WwgA4=\\"}\\n{\\"timestamp\\":\\"2021-11-17T14:32:48.254950+0100\\",\\"flow_id\\":1129058930499898,\\"pcap_cnt\\":10,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"128.14.134.170\\",\\"dest_port\\":57468,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:18.327585+0100\\",\\"flow_id\\":652708491465446,\\"pcap_cnt\\":206,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"139.162.242.152\\",\\"src_port\\":57268,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:gEyyy4v7MJSsjLvl+3D17G/rOIY=\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:18.329669+0100\\",\\"flow_id\\":652708491465446,\\"pcap_cnt\\":208,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"139.162.242.152\\",\\"dest_port\\":57268,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"198.71.247.91\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:8.0) Gecko/20100101 Firefox/8.0\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.569634+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":224,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":0,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.750383+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":226,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":0}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.812254+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":228,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":1,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.915298+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":230,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":1}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.977269+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":232,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":2,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/wp-includes/wlwmanifest.xml\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:31.987556+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":234,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/wp-includes/wlwmanifest.xml\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/wp-includes/wlwmanifest.xml\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":279,\\"tx_id\\":2}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.049539+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":236,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":3,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/xmlrpc.php?rsd\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.057985+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":238,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/xmlrpc.php?rsd\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":404,\\"length\\":279},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/xmlrpc.php\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":279,\\"tx_id\\":3}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.119589+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":239,\\"event_type\\":\\"http\\",\\"src_ip\\":\\"172.70.135.112\\",\\"src_port\\":37220,\\"dest_ip\\":\\"198.71.247.91\\",\\"dest_port\\":80,\\"proto\\":\\"TCP\\",\\"tx_id\\":4,\\"community_id\\":\\"1:7YaniZQ3kx5r62SiXkvH9P6TINQ=\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51}}\\n{\\"timestamp\\":\\"2021-11-17T14:55:32.127935+0100\\",\\"flow_id\\":987097466129838,\\"pcap_cnt\\":241,\\"event_type\\":\\"fileinfo\\",\\"src_ip\\":\\"198.71.247.91\\",\\"src_port\\":80,\\"dest_ip\\":\\"172.70.135.112\\",\\"dest_port\\":37220,\\"proto\\":\\"TCP\\",\\"http\\":{\\"hostname\\":\\"lifeisnetwork.com\\",\\"url\\":\\"/\\",\\"http_user_agent\\":\\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\\",\\"xff\\":\\"137.135.117.126\\",\\"http_content_type\\":\\"text/html\\",\\"http_method\\":\\"GET\\",\\"protocol\\":\\"HTTP/1.1\\",\\"status\\":200,\\"length\\":51},\\"app_proto\\":\\"http\\",\\"fileinfo\\":{\\"filename\\":\\"/\\",\\"sid\\":[],\\"gaps\\":false,\\"state\\":\\"CLOSED\\",\\"stored\\":false,\\"size\\":51,\\"tx_id\\":4}}\\n```\\n\\nNote that Zeek\u2019s tab-separated value (TSV) format is already a structured table,\\nwhereas Suricata data needs to be demultiplexed first through the `event_type`\\nfield.\\n\\nThe PCAP packet type is currently hard-coded in VAST\u2019s PCAP plugin and looks\\nlike this:\\n\\n``` go\\ntype pcap.packet = record {\\n  time: timestamp,\\n  src: addr,\\n  dst: addr,\\n  sport: port,\\n  dport: port,\\n  vlan: record {\\n    outer: count,\\n    inner: count,\\n  },\\n  community_id: string #index=hash,\\n  payload: string #skip,\\n}\\n```\\n\\nNow that we\u2019ve looked at the structure of the dataset, let\u2019s take a look at our\\nmeasurement methodology.\\n\\n### Measurement\\n\\nOur objective is understanding the storage and runtime characteristics of\\nParquet and Feather on the provided input data. To this end, we instrumented\\nVAST to produce us with a measurement trace file that we then analyze with R for\\ngaining insights. The [corresponding patch](feather-parquet-zstd-experiments.diff) is not meant for further\\nproduction, so we kept it separate. But we did find an opportunity to improve\\nVAST and [made the Zstd compression level configurable](https://github.com/tenzir/vast/pull/2623). Our [benchmark\\nscript](benchmark.fish) is available for full reproducibility.\\n\\nOur instrumentation produced a [CSV file](data.csv) with the following features:\\n\\n- **Store**: the type of store plugin used in the measurement, i.e., `parquet`\\n  or `feather`.\\n- **Construction time**: the time it takes to convert Arrow record batches into\\n  Parquet or Feather. We fenced the corresponding code blocks and computed the\\n  difference in nanoseconds.\\n- **Input size**: the number of bytes that the to-be-converted record batches\\n  consume.\\n- **Output size**: the number of bytes that the store file takes up.\\n- **Number of events**: the total number of events in all input record batches\\n- **Number of record batches**: the number Arrow record batches per store\\n- **Schema**: the name of the schema; there exists one store file per schema\\n- **Zstd compression level**: the applied Zstd compression level\\n\\nEvery row corresponds to a single store file where we varied some of these\\nparameters. We used [hyperfine](https://github.com/sharkdp/hyperfine) as\\nbenchmark driver tool, configured with 8 runs. Let\u2019s take a look at the data.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(scales)\\nlibrary(stringr)\\nlibrary(tidyr)\\n\\n# For faceting, to show clearer boundaries.\\ntheme_bw_trans <- function(...) {\\n  theme_bw(...) +\\n  theme(panel.background = element_rect(fill = \\"transparent\\"),\\n        plot.background = element_rect(fill = \\"transparent\\"),\\n        legend.key = element_rect(fill = \\"transparent\\"),\\n        legend.background = element_rect(fill = \\"transparent\\"))\\n}\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read.csv(\\"data.csv\\") |>\\n  rename(store = store_type) |>\\n  mutate(duration = dnanoseconds(duration))\\n\\noriginal <- read.csv(\\"sizes.csv\\") |>\\n  mutate(store = \\"original\\", store_class = \\"original\\") |>\\n  select(store, store_class, schema, bytes)\\n\\n# Global view on number of events per schema.\\nschemas <- data |>\\n  # Pick one element from the run matrix.\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  group_by(schema) |>\\n  summarize(n = sum(num_events),\\n            bytes_memory = sum(bytes_memory))\\n\\n# Normalize store sizes by number of events/store.\\nnormalized <- data |>\\n  mutate(duration_normalized = duration / num_events,\\n         bytes_memory_normalized = bytes_memory / num_events,\\n         bytes_storage_normalized = bytes_in_storage / num_events,\\n         bytes_ratio = bytes_in_storage / bytes_memory)\\n\\n# Compute average over measurements.\\naggregated <- normalized |>\\n  group_by(store, schema, zstd.level) |>\\n  summarize(duration = mean(duration_normalized),\\n            memory = mean(bytes_memory_normalized),\\n            storage = mean(bytes_storage_normalized))\\n\\n# Treat in-memory measurements as just another storage type.\\nmemory <- aggregated |>\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  mutate(store = \\"memory\\", store_class = \\"memory\\") |>\\n  select(store, store_class, schema, bytes = memory)\\n\\n# Unite with rest of data.\\nunified <-\\n  aggregated |>\\n  select(-memory) |>\\n  mutate(zstd.level = factor(str_replace_na(zstd.level),\\n                             levels = c(\\"NA\\", \\"-5\\", \\"1\\", \\"9\\", \\"19\\"))) |>\\n  rename(bytes = storage, store_class = store) |>\\n  unite(\\"store\\", store_class, zstd.level, sep = \\"+\\", remove = FALSE)\\n\\nschemas_gt10k <- schemas |> filter(n > 10e3) |> pull(schema)\\nschemas_gt100k <- schemas |> filter(n > 100e3) |> pull(schema)\\n\\n# Only schemas with > 100k events.\\ncleaned <- unified |>\\n  filter(schema %in% schemas_gt100k)\\n\\n# Helper function to format numbers with SI unit suffixes.\\nfmt_short <- function(x) {\\n  scales::label_number(scale_cut = cut_short_scale(), accuracy = 0.1)(x)\\n}\\n```\\n\\n</details>\\n\\n### Schemas\\n\\nWe have a total of 42 unique schemas:\\n\\n     [1] \\"zeek.dce_rpc\\"       \\"zeek.dhcp\\"          \\"zeek.x509\\"         \\n     [4] \\"zeek.dpd\\"           \\"zeek.ftp\\"           \\"zeek.files\\"        \\n     [7] \\"zeek.ntlm\\"          \\"zeek.kerberos\\"      \\"zeek.ocsp\\"         \\n    [10] \\"zeek.ntp\\"           \\"zeek.dns\\"           \\"zeek.packet_filter\\"\\n    [13] \\"zeek.pe\\"            \\"zeek.radius\\"        \\"zeek.http\\"         \\n    [16] \\"zeek.reporter\\"      \\"zeek.weird\\"         \\"zeek.smb_files\\"    \\n    [19] \\"zeek.sip\\"           \\"zeek.smb_mapping\\"   \\"zeek.smtp\\"         \\n    [22] \\"zeek.conn\\"          \\"zeek.snmp\\"          \\"zeek.tunnel\\"       \\n    [25] \\"zeek.ssl\\"           \\"suricata.krb5\\"      \\"suricata.ikev2\\"    \\n    [28] \\"suricata.http\\"      \\"suricata.smb\\"       \\"suricata.ftp\\"      \\n    [31] \\"suricata.dns\\"       \\"suricata.fileinfo\\"  \\"suricata.tftp\\"     \\n    [34] \\"suricata.snmp\\"      \\"suricata.sip\\"       \\"suricata.anomaly\\"  \\n    [37] \\"suricata.smtp\\"      \\"suricata.dhcp\\"      \\"suricata.tls\\"      \\n    [40] \\"suricata.dcerpc\\"    \\"suricata.flow\\"      \\"pcap.packet\\"       \\n\\nThe schemas belong to three data *modules*: Zeek, Suricata, and PCAP. A module\\nis the prefix of a concrete type, e.g., for the schema `zeek.conn` the module is\\n`zeek` and the type is `conn`. This is only a distinction in terminology,\\ninternally VAST stores the full-qualified type as schema name.\\n\\nHow many events do we have per schema?\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nschemas <- normalized |>\\n  # Pick one element from the run matrix.\\n  filter(store == \\"feather\\" & zstd.level == 1) |>\\n  group_by(schema) |>\\n  summarize(n = sum(num_events),\\n            bytes_memory = sum(bytes_memory))\\n\\nschemas |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = reorder(schema, -n), y = n, fill = module)) +\\n    geom_bar(stat = \\"identity\\") +\\n    scale_y_log10(labels = scales::label_comma()) +\\n    labs(x = \\"Schema\\", y = \\"Number of Events\\", fill = \\"Module\\") +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/number-of-events-by-schema-1.svg\';\\n\\n<Svg1 />\\n\\nThe above plot (log-scaled y-axis) shows how many events we have per type.\\nBetween 1 and 100M events, we almost see everything.\\n\\nWhat\u2019s the typical event size?\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nschemas |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = reorder(schema, -n), y = bytes_memory / n, fill = module)) +\\n    geom_bar(stat = \\"identity\\") +\\n    guides(fill = \\"none\\") +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    labs(x = \\"Schema\\", y = \\"Bytes (in-memory)\\", color = \\"Module\\") +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/event-size-by-schema-1.svg\';\\n\\n<Svg2 />\\n\\nThe above plot keeps the x-axis from the previous plot, but exchanges the y-axis\\nto show normalized event size, in memory after parsing. Most events\\ntake up a few 100 bytes, with packet data consuming a bit more, and one 5x\\noutlier: `suricata.ftp`.\\n\\nSuch distributions are normal, even with these outliers. Some telemetry events\\nsimply have more string data that\u2019s a function of user input. For `suricata.ftp`\\nspecifically, it can grow linearly with the data transmitted. Here\u2019s a stripped\\ndown example of an event that is greater than 5 kB in its raw JSON:\\n\\n``` json\\n{\\n  \\"timestamp\\": \\"2021-11-19T05:08:50.885981+0100\\",\\n  \\"flow_id\\": 1339403323589433,\\n  \\"pcap_cnt\\": 5428538,\\n  \\"event_type\\": \\"ftp\\",\\n  \\"src_ip\\": \\"10.5.5.101\\",\\n  \\"src_port\\": 50479,\\n  \\"dest_ip\\": \\"62.24.128.228\\",\\n  \\"dest_port\\": 110,\\n  \\"proto\\": \\"TCP\\",\\n  \\"tx_id\\": 12,\\n  \\"community_id\\": \\"1:kUFeGEpYT1JO1VCwF8wZWUWn0J0=\\",\\n  \\"ftp\\": {\\n    \\"completion_code\\": [\\n      \\"155\\",\\n      ...\\n      <stripped 330 lines>\\n      ...\\n      \\"188\\",\\n      \\"188\\",\\n      \\"188\\"\\n    ],\\n    \\"reply\\": [\\n      \\" 41609\\",\\n      ...\\n      <stripped 330 lines>\\n      ...\\n      \\" 125448\\",\\n      \\" 126158\\",\\n      \\" 29639\\"\\n    ],\\n    \\"reply_received\\": \\"yes\\"\\n  }\\n}\\n```\\n\\nThis matches our mental model. A few hundred bytes per event with some outliers.\\n\\n### Batching\\n\\nOn the inside, a store is a concatenation of homogeneous Arrow record batches,\\nall having the same schema.\\n\\nThe Feather format is essentially the IPC wire format of record batches. Schemas\\nand dictionaries are only included when they change. For our stores, this means\\njust once in the beginning. In order to access a given row in a Feather file,\\nyou need to start at the beginning, iterate batch by batch until you arrive at\\nthe desired batch, and then materialize it before you can access the desired\\nrow via random access.\\n\\nParquet has *row groups* that are much like a record batch, except that they are\\ncreated at write time, so Parquet determines their size rather than the incoming\\ndata. Parquet offers random access over both the row groups and within an\\nindividual batch that is materialized from a row group. The on-disk layout of\\nParquet is still row-group by row-group, and in that column by column, so\\nthere\u2019s no big difference between Parquet and Feather in that regard. Parquet\\nencodes columns using different encoding techniques than Arrow\u2019s IPC format.\\n\\nMost stores only consist of a few record batches. PCAP is the only difference.\\nSmall stores are suboptimal because the catalog keeps in-memory state that is a\\nlinear function of the number of stores. (We are aware of this concern and are\\nexploring improvements, but this topic is out of scope for this post.) The issue\\nhere is catalog fragmentation.\\n\\nAs of [v2.3](/blog/vast-v2.3), VAST has automatic rebuilding in place, which\\nmerges underfull partitions to reduce pressure on the catalog. This doesn\u2019t fix\\nthe problem of linear state, but gives us much sufficient reach for real-world\\ndeployments.\\n\\n## Size\\n\\nTo better understand the difference between Parquet and Feather, we now take a\\nlook at them right next to each other. In addition to Feather and Parquet, we\\nuse two other types of \u201cstores\u201d for the analysis to facilitate comparison:\\n\\n1. **Original**: the size of the input prior it entered VAST, e.g., the raw JSON or\\n    a PCAP file.\\n\\n2. **Memory**: the size of the data in memory, measured as the sum of Arrow\\n    buffers that make up the table slice.\\n\\nLet\u2019s kick of the analysis by getting a better understanding at the size\\ndistribution.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  bind_rows(original, memory) |>\\n  ggplot(aes(x = reorder(store, -bytes, FUN = \\"median\\"),\\n             y = bytes, color = store_class)) +\\n  geom_boxplot() +\\n  scale_y_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n  labs(x = \\"Store\\", y = \\"Bytes/Event\\", color = \\"Store\\") +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/plot-schema-distribution-boxplot-1.svg\';\\n\\n<Svg3 />\\n\\nEvery boxplot corresponds to one store, with `original` and `memory` being also\\ntreated like stores. The suffix `-Z` indicates Zstd level `Z`, with `NA` meaning\\n\u201ccompression turned off\u201d entirely. Parquet stores on the right (in purple) have\\nthe smallest size, followed by Feather (red), and then their corresponding\\nin-memory (green) and original (turquoise) representation. The negative Zstd\\nlevel -5 makes Parquet actually worse than Feather.\\n\\n:::tip Analysis\\nWhat stands out is that disabling compression for Feather inflates the data\\nlarger than the original. This is not the case for Parquet. Why? Because Parquet\\nhas an orthogonal layer of compression using dictionaries. This absorbs\\ninefficiencies in heavy-tailed distributions, which are pretty standard in\\nmachine-generated data.\\n:::\\n\\nThe y-axis of above plot is log-scaled, which makes it hard for relative\\ncomparison. Let\u2019s focus on the medians (the bars in the box) only and bring the\\ny-axis to a linear scale:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nmedians <- unified |>\\n  bind_rows(original, memory) |>\\n  group_by(store, store_class) |>\\n  summarize(bytes = median(bytes))\\n\\nmedians |>\\n  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +\\n  geom_bar(stat = \\"identity\\") +\\n  scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n  labs(x = \\"Store\\", y = \\"Bytes/Event\\", fill = \\"Store\\") +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg4 from \'./index_files/figure-gfm/plot-schema-distribution-medians-1.svg\';\\n\\n<Svg4 />\\n\\nTo better understand the compression in numbers, we\u2019ll anchor the original size\\nat 100% and now show the *relative* gains of Parquet and Feather:\\n\\n| Store      | Class   | Bytes/Event | Size (%) | Compression Ratio |\\n|:-----------|:--------|------------:|---------:|------------------:|\\n| parquet+19 | parquet |        53.5 |     22.7 |               4.4 |\\n| parquet+9  | parquet |        54.4 |     23.1 |               4.3 |\\n| parquet+1  | parquet |        55.8 |     23.7 |               4.2 |\\n| feather+19 | feather |        57.8 |     24.6 |               4.1 |\\n| feather+9  | feather |        66.9 |     28.4 |               3.5 |\\n| feather+1  | feather |        68.9 |     29.3 |               3.4 |\\n| parquet+-5 | parquet |        72.9 |     31.0 |               3.2 |\\n| parquet+NA | parquet |        90.8 |     38.6 |               2.6 |\\n| feather+-5 | feather |        95.8 |     40.7 |               2.5 |\\n| feather+NA | feather |       255.1 |    108.3 |               0.9 |\\n\\n:::tip Analysis\\nParquet dominates Feather with respect to space savings, but not by much for\\nhigh Zstd levels. Zstd levels \\\\> 1 do not provide substantial space savings on\\naverage, where observe a compression ratio of **\\\\~4x** over the base data. Parquet\\nstill provides a **2.6** compression ratio in the absence of compression because\\nit applies dictionary encoding.\\n\\nFeather offers competitive compression with **\\\\~3x** ratio for equal Zstd levels.\\nHowever, without compression Feather expands beyond the original dataset size at\\na compression ratio of **\\\\~0.9**.\\n:::\\n\\nThe above analysis covered averages across schemas. If we juxtapose Parquet and\\nFeather per schema, we see the difference between the two formats more clearly:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\nparquet_vs_feather_size <- unified |>\\n  select(-store, -duration) |>\\n  pivot_wider(names_from = store_class,\\n              values_from = bytes,\\n              id_cols = c(schema, zstd.level))\\n\\nplot_parquet_vs_feather <- function(data) {\\n  data |>\\n    mutate(zstd.level = str_replace_na(zstd.level)) |>\\n    separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n    ggplot(aes(x = parquet, y = feather,\\n               shape = zstd.level, color = zstd.level)) +\\n      geom_abline(intercept = 0, slope = 1, color = \\"grey\\") +\\n      geom_point(alpha = 0.6, size = 3) +\\n      geom_text_repel(aes(label = schema),\\n                color = \\"grey\\",\\n                size = 1, # font size\\n                box.padding = 0.2,\\n                min.segment.length = 0, # draw all line segments\\n                max.overlaps = Inf,\\n                segment.size = 0.2,\\n                segment.color = \\"grey\\",\\n                segment.alpha = 0.3) +\\n      scale_size(range = c(0, 10)) +\\n      labs(x = \\"Bytes (Parquet)\\", y = \\"Bytes (Feather)\\",\\n           shape = \\"Zstd Level\\", color = \\"Zstd Level\\")\\n}\\n\\nparquet_vs_feather_size |>\\n  filter(schema %in% schemas_gt100k) |>\\n  plot_parquet_vs_feather() +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_bytes(units = \\"auto_si\\"))\\n```\\n\\n</details>\\n\\nimport Svg5 from \'./index_files/figure-gfm/plot-parquet-vs-feather-1.svg\';\\n\\n<Svg5 />\\n\\nIn the above log-log scatterplot, the straight line is the identity function.\\nEach point represents the median store size for a given schema. If a point is on\\nthe line, it means there is no difference between Feather and Parquet. We only\\nlook at schemas with more than 100k events to ensure that the constant factor\\ndoes not perturb the analysis. (Otherwise we end up with points *below* the\\nidentity line, which are completely dwarfed by the bulk in practice.) The color\\nand shape shows the different Zstd levels, with `NA` meaning no compression.\\nPoints clouds closer to the origin mean that the corresponding store class takes\\nup less space.\\n\\n:::tip Analysis\\nWe observe that **disabling compression hits Feather the hardest**.\\nUnexpectedly, a negative Zstd level of -5 does not compress well. The remaining\\nZstd levels are difficult to take apart visually, but it appears that the point\\nclouds form a parallel line, indicating stable compression gains. Notably,\\n**compressing PCAP packets is nearly identical with Feather and Parquet**,\\npresumably because of the low entropy and packet meta data where general-purpose\\ncompressors like Zstd shine.\\n:::\\n\\nZooming in to the bottom left area with average event size of less than 100B,\\nand removing the log scaling, we see the following:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nparquet_vs_feather_size |>\\n  filter(feather <= 100 & schema %in% schemas_gt100k) |>\\n  plot_parquet_vs_feather() +\\n    scale_x_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    coord_fixed()\\n```\\n\\n</details>\\n\\nimport Svg6 from \'./index_files/figure-gfm/plot-parquet-vs-feather-100-1.svg\';\\n\\n<Svg6 />\\n\\nThe respective point clouds form a parallel to the identity function, i.e., the\\ncompression ratio in this region pretty constant across schemas. There\u2019s also no\\nnoticeable difference between Zstd level 1, 9, and 19.\\n\\nIf we take pick a single point, e.g., `zeek.conn` with\\n4.7M events,\\nwe can confirm that the relative performance matches the results of our analysis\\nabove:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  filter(schema == \\"zeek.conn\\") |>\\n  ggplot(aes(x = reorder(store, -bytes), y = bytes, fill = store_class)) +\\n    geom_bar(stat = \\"identity\\") +\\n    guides(fill = \\"none\\") +\\n    labs(x = \\"Store\\", y = \\"Bytes/Event\\") +\\n    scale_y_continuous(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +\\n    facet_wrap(~ schema, scales = \\"free\\")\\n```\\n\\n</details>\\n\\nimport Svg7 from \'./index_files/figure-gfm/plot-zeek-suricata-1.svg\';\\n\\n<Svg7 />\\n\\nFinally, we look at the fraction of space Parquet takes compared to Feather on a\\nper schema basis, restricted to schemas with more than 10k events:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tibble)\\n\\nparquet_vs_feather_size |>\\n  filter(feather <= 100 & schema %in% schemas_gt10k) |>\\n  mutate(zstd.level = str_replace_na(zstd.level)) |>\\n  ggplot(aes(x = reorder(schema, -parquet / feather),\\n             y = parquet / feather,\\n             fill = zstd.level)) +\\n    geom_hline(yintercept = 1) +\\n    geom_bar(stat = \\"identity\\", position = \\"dodge\\") +\\n    labs(x = \\"Schema\\", y = \\"Parquet / Feather (%)\\", fill = \\"Zstd Level\\") +\\n    scale_y_continuous(breaks = 6:1 * 20 / 100, labels = scales::label_percent()) +\\n    theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0))\\n```\\n\\n</details>\\n\\nimport Svg8 from \'./index_files/figure-gfm/plot-parquet-divided-by-feather-1.svg\';\\n\\n<Svg8 />\\n\\nThe horizontal line is similar to the identity line in the scatterplot,\\nindicating that Feather and Parquet compress equally well. The bars represent\\nthat ratio of Parquet divided by Feather. The shorter the bars, the smaller the\\nsize, so the higher the gain over Feather.\\n\\n:::tip Analysis\\nWe see that Zstd level 19 brings Parquet and Feather close together. Even at\\nZstd level 1, the median ratio of Parquet stores is **78%**, and the 3rd\\nquartile **82%**. This shows that **Feather is remarkably competitive for typical\\nsecurity analytics workloads**.\\n:::\\n\\n## Speed\\n\\nNow that we have looked at the spatial properties of Parquet and Feather, we\\ntake a look at the runtime. With *speed*, we mean the time it takes to transform\\nArrow Record Batches into Parquet and Feather format. This analysis only\\nconsiders only CPU time; VAST writes the respective store in memory first and\\nthen flushes it one sequential write. Our mental model is that Feather is faster\\nthan Parquet. Is that the case when enabling compression for both?\\n\\nTo avoid distortion of small events, we also restrict the analysis to schemas\\nwith more than 100k events.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nunified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  ggplot(aes(x = reorder(store, -duration, FUN = \\"median\\"),\\n             y = duration, color = store_class)) +\\n  geom_boxplot() +\\n  scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n  theme(axis.text.x = element_text(angle = -90, size = 8, vjust = 0.5, hjust = 0)) +\\n  labs(x = \\"Store\\", y = \\"Speed (us)\\", color = \\"Store\\")\\n```\\n\\n</details>\\n\\nimport Svg9 from \'./index_files/figure-gfm/duration-distribution-1.svg\';\\n\\n<Svg9 />\\n\\nThe above boxplots show the time it takes to write a store for a given store and\\ncompression level combination. The log-scaled y-axis shows the normalized to number\\nof microseconds per event, across the distribution of all schemas. The sort order\\nis the median processing time, similar to the size discussion above.\\n\\n:::tip Analysis\\nAs expected, we roughly observe an ordering according to Zstd level: more\\ncompression means a longer runtime.\\n\\nUnexpectedly, for the same Zstd level, **Parquet store creation was always\\nfaster**. Our unconfirmed hunch is that Feather compression operates on more and\\nsmaller column buffers, whereas Parquet compression only runs over the\\nconcatenated Arrow buffers, yielding bigger strides.\\n:::\\n\\nWe don\u2019t have an explanation for why disabling compression for Parquet is\\n*slower* compared Zstd levels -5 and 1. In theory, strictly less cycles are\\nspent by disabling the compression code path. Perhaps compression results in\\ndifferent memory layout that is more cache-efficient. Unfortunately, we did not\\nhave the time to dig deeper into the analysis to figure out why disabling\\nParquet compression is slower. Please don\u2019t hesitate to reach out, e.g., via our\\n[community chat](/discord).\\n\\nLet\u2019s compare Parquet and Feather by compression level, per schema:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nparquet_vs_feather_duration <- unified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  select(-store, -bytes) |>\\n  pivot_wider(names_from = store_class,\\n              values_from = duration,\\n              id_cols = c(schema, zstd.level))\\n\\nparquet_vs_feather_duration |>\\n  mutate(zstd.level = str_replace_na(zstd.level)) |>\\n  separate(schema, c(\\"module\\", \\"type\\"), remove = FALSE) |>\\n  ggplot(aes(x = parquet, y = feather,\\n             shape = zstd.level, color = zstd.level)) +\\n    geom_abline(intercept = 0, slope = 1, color = \\"grey\\") +\\n    geom_point(alpha = 0.7, size = 3) +\\n    geom_text_repel(aes(label = schema),\\n              color = \\"grey\\",\\n              size = 1, # font size\\n              box.padding = 0.2,\\n              min.segment.length = 0, # draw all line segments\\n              max.overlaps = Inf,\\n              segment.size = 0.2,\\n              segment.color = \\"grey\\",\\n              segment.alpha = 0.3) +\\n    scale_size(range = c(0, 10)) +\\n    scale_x_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Speed (Parquet)\\", y = \\"Speed (Feather)\\",\\n         shape = \\"Zstd Level\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg10 from \'./index_files/figure-gfm/pairwise-runtime-comparion-1.svg\';\\n\\n<Svg10 />\\n\\nThe above scatterplot has an identity line. Points on this line indicates that\\nthere is no speed difference between Parquet and Feather. Feather is faster for\\npoints below the line, and Parquet is faster for points above the line.\\n\\n:::tip Analysis\\nIn addition to the above boxplot, this scatterplot makes it clearer to see the\\nimpact of the schemas.\\n\\nInterestingly, **there is no significant difference in Zstd levels -5 and 1,\\nwhile levels 9 and 19 stand apart further**. Disabling compression for Feather\\nhas a stronger effect on speed than for Parquet.\\n\\nOverall, we were surprised that **Feather and Parquet are not far apart in terms\\nof write performance once compression is enabled**. Only when compression is\\ndisabled, Parquet is substantially slower in our measurements.\\n:::\\n\\n## Space-Time Trade-off\\n\\nFinally, we combine the size and speed analysis into a single benchmark. Our\\ngoal is to find an optimal parameterization, i.e., one that strictly dominates\\nothers. To this end, we now plot size against speed:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned <- unified |>\\n  filter(schema %in% schemas_gt100k) |>\\n  mutate(zstd.level = factor(str_replace_na(zstd.level),\\n                             levels = c(\\"NA\\", \\"-5\\", \\"1\\", \\"9\\", \\"19\\"))) |>\\n  group_by(schema, store_class, zstd.level) |>\\n  summarize(bytes = median(bytes), duration = median(duration))\\n\\ncleaned |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = zstd.level)) +\\n    geom_point(size = 3, alpha = 0.7) +\\n    geom_text_repel(aes(label = schema),\\n              color = \\"grey\\",\\n              size = 1, # font size\\n              box.padding = 0.2,\\n              min.segment.length = 0, # draw all line segments\\n              max.overlaps = Inf,\\n              segment.size = 0.2,\\n              segment.color = \\"grey\\",\\n              segment.alpha = 0.3) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg11 from \'./index_files/figure-gfm/points-by-level-1.svg\';\\n\\n<Svg11 />\\n\\nEvery point in the above log-log scatterplot represents a store with a fixed\\nschema. Since we have multiple stores for a given schema, we took the median\\nsize and median speed. We then varied the run matrix by Zstd level (color) and\\nstore type (triangle/point shape). Points closer to the origin are \u201cbetter\u201d in\\nboth dimensions. So we\u2019re looking for the left-most and bottom-most ones.\\nDisabling compression puts points into the bottom-right area, and maximum\\ncompression into the top-left area.\\n\\nThe point closest to the origin has the schema `zeek.dce_rpc` for Zstd level 1,\\nboth for Feather and Parquet. Is there anything special about this log file?\\nHere\u2019s a sample:\\n\\n    #separator \\\\x09\\n    #set_separator  ,\\n    #empty_field    (empty)\\n    #unset_field    -\\n    #path   dce_rpc\\n    #open   2022-04-20-09-56-46\\n    #fields ts  uid id.orig_h   id.orig_p   id.resp_h   id.resp_p   rtt named_pipe  endpoint    operation\\n    #types  time    string  addr    port    addr    port    interval    string  string  string\\n    1637222709.134638   Cypdo7cTBbiS4Asad   10.2.9.133  49768   10.2.9.9    135 0.000254    135 epmapper    ept_map\\n    1637222709.140898   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000239    49671   drsuapi DRSBind\\n    1637222709.141520   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000311    49671   drsuapi DRSCrackNames\\n    1637222709.142068   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000137    49671   drsuapi DRSUnbind\\n    1637222709.143104   Cypdo7cTBbiS4Asad   10.2.9.133  49768   10.2.9.9    135 0.000228    135 epmapper    ept_map\\n    1637222709.143642   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000147    49671   drsuapi DRSBind\\n    1637222709.144040   CTDU3j3iAXfRITNiah  10.2.9.133  49769   10.2.9.9    49671   0.000296    49671   drsuapi DRSCrackNames\\n\\nIt appears to be rather normal: 10 columns, several different data types, unique\\nIDs, and some short strings. By looking at the data alone, there is no obvious\\nhint that explains the performance.\\n\\nWith dozens to hundreds of different schemas per data source (sometimes even\\nmore), there it will be difficult to single out individual schemas. But a point\\ncloud is unwieldy for relative comparison. To better represent the variance of\\nschemas for a given configuration, we can strip the \u201cinner\u201d points and only look\\nat their convex hull:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\n# Native convex hull does the job, no need to leverage ggforce.\\nconvex_hull <- cleaned |>\\n  group_by(store_class, zstd.level) |>\\n  slice(chull(x = bytes, y = duration))\\n\\nconvex_hull |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = zstd.level)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(fill = zstd.level, color = zstd.level),\\n                 alpha = 0.1,\\n                 show.legend = FALSE) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Zstd Level\\")\\n```\\n\\n</details>\\n\\nimport Svg12 from \'./index_files/figure-gfm/convex-hull-1.svg\';\\n\\n<Svg12 />\\n\\nIntuitively, the area of a given polygon captures its variance. A smaller area\\nis \u201cgood\u201d in that it offers more predictable behavior. The high amount of\\noverlap makes it still difficult to perform clearer comparisons. If we facet by\\nstore type, it becomes easier to compare the areas:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned |>\\n  group_by(store_class, zstd.level) |>\\n  # Native convex hull does the job, no need to leverage ggforce.\\n  slice(chull(x = bytes, y = duration)) |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = store_class, color = store_class)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(color = store_class, fill = store_class),\\n                 alpha = 0.3,\\n                 show.legend = FALSE) +\\n    scale_x_log10(n.breaks = 4, labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Store\\", color = \\"Store\\") +\\n    facet_grid(cols = vars(zstd.level)) +\\n    theme_bw_trans()\\n```\\n\\n</details>\\n\\nimport Svg13 from \'./index_files/figure-gfm/convex-hull-facet-by-level-1.svg\';\\n\\n<Svg13 />\\n\\nArranging the facets above row-wise makes it easier to compare the y-axis, i.e.,\\nspeed, where lower polygons are better. Arranging them column-wise makes it easier\\nto compare the x-axis, i.e., size, where the left-most polygons are better:\\n\\n<details><summary>Code</summary>\\n\\n``` r\\ncleaned |>\\n  group_by(store_class, zstd.level) |>\\n  slice(chull(x = bytes, y = duration)) |>\\n  ggplot(aes(x = bytes, y = duration,\\n             shape = zstd.level, color = zstd.level)) +\\n    geom_point(size = 3) +\\n    geom_polygon(aes(color = zstd.level, fill = zstd.level),\\n                 alpha = 0.3,\\n                 show.legend = FALSE) +\\n    scale_x_log10(labels = scales::label_bytes(units = \\"auto_si\\")) +\\n    scale_y_log10(labels = scales::label_number(scale = 1e6, suffix = \\"us\\")) +\\n    labs(x = \\"Size\\", y = \\"Speed\\", shape = \\"Zstd Level\\", color = \\"Zstd Level\\") +\\n    facet_grid(rows = vars(store_class)) +\\n    theme_bw_trans()\\n```\\n\\n</details>\\n\\nimport Svg14 from \'./index_files/figure-gfm/convex-hull-facet-by-store-1.svg\';\\n\\n<Svg14 />\\n\\n:::tip Analysis\\nAcross both dimensions, **Zstd level 1 shows the best average space-time\\ntrade-off for both Parquet and Feather**. In the above plots, we also observe our\\nfindings from the speed analysis: Parquet still dominates when compression is\\nenabled.\\n:::\\n\\n## Conclusion\\n\\nIn summary, we set out to better understand how Parquet and Feather behave on\\nthe write path of VAST, when acquiring security telemetry from high-volume data\\nsources. Our findings show that columnar Zstd compression offers great space\\nsavings for both Parquet and Feather. For certain schemas, Feather and Parquet\\nexhibit only a marginal differences. To our surprise, writing Parquet files is\\nstill faster than Feather for our workloads.\\n\\nThe pressing next question is obviously: what about the read path, i.e., query\\nlatency? This is a topic for future, stay tuned."},{"id":"/vast-v2.3.1","metadata":{"permalink":"/blog/vast-v2.3.1","source":"@site/blog/vast-v2.3.1/index.md","title":"VAST v2.3.1","description":"VAST v2.3.1 is now available. This small bugfix release","date":"2022-10-17T00:00:00.000Z","formattedDate":"October 17, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"rebuild","permalink":"/blog/tags/rebuild"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":0.215,"hasTruncateMarker":false,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.3.1","authors":"lava","date":"2022-10-17T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"Parquet & Feather: Writing Security Telemetry","permalink":"/blog/parquet-and-feather-writing-security-telemetry"},"nextItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/blog/parquet-and-feather-enabling-open-investigations"}},"content":"[VAST v2.3.1][github-vast-release] is now available. This small bugfix release\\naddresses an issue where compaction would hang if encountering\\ninvalid partitions that were produced by older versions of VAST when a large\\n`max-partition-size` was set in combination with badly compressible input data.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.1"},{"id":"/parquet-and-feather-enabling-open-investigations","metadata":{"permalink":"/blog/parquet-and-feather-enabling-open-investigations","source":"@site/blog/parquet-and-feather-enabling-open-investigations/index.md","title":"Parquet & Feather: Enabling Open Investigations","description":"Apache Parquet is the common denominator for structured data at rest.","date":"2022-10-07T00:00:00.000Z","formattedDate":"October 7, 2022","tags":[{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"parquet","permalink":"/blog/tags/parquet"},{"label":"feather","permalink":"/blog/tags/feather"}],"readingTime":5.16,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"},{"name":"Thomas Peiselt","title":"Data Engineer","url":"https://github.com/dispanser","email":"thomas@tenzir.com","imageURL":"https://github.com/dispanser.png","key":"dispanser"}],"frontMatter":{"title":"Parquet & Feather: Enabling Open Investigations","authors":["mavam","dispanser"],"date":"2022-10-07T00:00:00.000Z","last_updated":"2023-01-10T00:00:00.000Z","tags":["arrow","parquet","feather"]},"prevItem":{"title":"VAST v2.3.1","permalink":"/blog/vast-v2.3.1"},"nextItem":{"title":"A Git Retrospective","permalink":"/blog/a-git-retrospective"}},"content":"[Apache Parquet][parquet] is the common denominator for structured data at rest.\\nThe data science ecosystem has long appreciated this. But infosec? Why should\\nyou care about Parquet when building a threat detection and investigation\\nplatform? In this blog post series we share our opinionated view on this\\nquestion. In the next three blog posts, we\\n\\n1. describe how VAST uses Parquet and its little brother [Feather][feather]\\n2. benchmark the two formats against each other for typical workloads\\n3. share our experience with all the engineering gotchas we encountered along\\n   the way\\n\\n[parquet]: https://parquet.apache.org/\\n[feather]: https://arrow.apache.org/docs/python/feather.html\\n\\n\x3c!--truncate--\x3e\\n\\n:::info Parquet & Feather: 1/3\\nThis is blog post is part of a 3-piece series on Parquet and Feather.\\n\\n1. This blog post\\n2. [Writing Security Telemetry][parquet-and-feather-2]\\n3. [Data Engineering Woes][parquet-and-feather-3]\\n\\n[parquet-and-feather-2]: /blog/parquet-and-feather-writing-security-telemetry/\\n[parquet-and-feather-3]: /blog/parquet-and-feather-data-engineering-woes/\\n:::\\n\\n## Why Parquet and Feather?\\n\\nParquet is the de-facto standard for storing structured data in a format\\nconducive for analytics. Nearly all analytics engines support reading Parquet\\nfiles to load a dataset in memory for subsequent analysis.\\n\\nThe data science community has long built on this foundation, but the majority\\nof infosec tooling does not build on an open foundation. Too many\\nproducts hide their data behind silos, either wrapped behind a SaaS with a thin\\nAPI, or in a custom format that requires cumbersome ETL pipelines. Nearly all\\nadvanced use cases require full access to the data. Especially when\\nthe goal is developing realtime threat detection and response systems.\\n\\nSecurity is a data problem. But how should we represent that data? This is where\\nParquet enters the picture. As a vendor-agnostic storage format for structured\\nand nested data, it decouples storage from analytics. This is where SIEM\\nmonoliths fail: they offer a single black box that tightly couples data\\nacquisition and processing capabilities. Providing a thin \\"open\\" API is not really\\nopen, as it prevents high-bandwidth data access that is needed for advanced\\nanalytics workloads.\\n\\nOpen storage prevents vendor-lock-in. When any tool can work with the data, you\\nbuild a sustainable foundation for implementing future use cases. For example,\\nwith Parquet\'s column encryption, you can offload fine-grained compliance use\\ncases to a dedicated application. Want to try out a new analytics engine? Just\\npoint it to the Parquet files.\\n\\n## Parquet\'s Little Brother\\n\\n[Feather][feather] is Parquet\'s little brother. It emerged while building a\\nproof of concept for \\"fast, language-agnostic data frame storage for Python\\n(pandas) and R.\\" The format is a thin layer on top of [Arrow\\nIPC](https://arrow.apache.org/docs/python/ipc.html#ipc), making it conducive for\\nmemory mapping and zero-copy usage. On the spectrum of speed and\\nspace-efficiency, think of it this way:\\n\\n![Parquet vs. Feather](parquet-vs-feather.excalidraw.svg)\\n\\nBefore Feather existed, VAST had its own storage format that was 95% like\\nFeather, minus a thin framing. (We called it the *segment store*.)\\n\\nWait, but Feather is an in-memory format and Parquet an on-disk format. You\\ncannot compare them! Fair point, but don\'t forget transparent Zstd compression.\\nFor some schemas, we barely notice a difference (e.g., PCAP), whereas for\\nothers, Parquet stores boil down to a fraction of their Feather equivalent.\\n\\nThe [next blog post][parquet-and-feather-2] goes into these details. For now, we\\nwant to stress that Feather is in fact a reasonable format for data at rest,\\neven when looking at space utilization alone.\\n\\n## Parquet and Feather in VAST\\n\\nVAST can store event data as Parquet or Feather. The unit of storage scaling is\\na *partition*. In Arrow terms, a partition is a persisted form of an [Arrow\\nTable][arrow-table], i.e., a concatenation of [Record\\nBatches][arrow-record-batch]. A partition has thus a fixed schema. VAST\'s store\\nplugin determines how a partition writes its buffered record\\nbatches to disk. The diagram below illustrates the architecture:\\n\\n![Parquet Analytics](parquet-analytics.excalidraw.svg)\\n\\n[arrow-table]: https://arrow.apache.org/docs/python/data.html#tables\\n[arrow-record-batch]: https://arrow.apache.org/docs/python/data.html#record-batches\\n\\nThis architecture makes it easy to point an analytics application directly to\\nthe store files, without the need for ETLing it into a dedicated warehouse, such\\nas Spark or Hadoop.\\n\\nThe event data thrown at VAST has quite some variety of schemas. During\\ningestion, VAST first demultiplexes the heterogeneous stream of events into\\nmultiple homogeneous streams, each of which has a unique schema. VAST buffers\\nevents until the partition hits a pre-configured event limit (e.g., 1M) or until\\na timeout occurs (e.g., 60m). Thereafter, VAST writes the partition in one shot\\nand persists it.\\n\\nThe buffering provides optimal freshness of the data, as it enables queries run\\non not-yet-persisted data. But it also sets an upper bound on the partition\\nsize, given that it must fit in memory in its entirety. In the future, we plan\\nto make this freshness trade-off explicit, making it possible to write out\\nlarger-than-memory stores incrementally.\\n\\n## Imbuing Domain Semantics\\n\\nIn a [past blog][blog-arrow] we described how VAST uses Arrow\'s extensible\\ntype system to add richer semantics to the data. This is how the value of VAST\\ntranscends through the analytics stack. For example, VAST has native IP address\\ntypes that you can show up in Python as [ipaddress][ipaddress] instance. This\\navoids friction in the data exchange process. Nobody wants to spend time\\nconverting bytes or strings into the semantic objects that are ultimately need\\nfor the analysis.\\n\\n[blog-arrow]: /blog/apache-arrow-as-platform-for-security-data-engineering\\n[ipaddress]: https://docs.python.org/3/library/ipaddress.html\\n\\nHere\'s how VAST\'s type system looks like:\\n\\n![Type System - VAST](type-system-vast.excalidraw.svg)\\n\\nThere exist two major classes of types: *basic*, stateless types with a static\\nstructure and a-priori known representation, and *complex*, stateful types that\\ncarry additional runtime information. We map this type system without\\ninformation loss to Arrow:\\n\\n![Type System - Arrow](type-system-arrow.excalidraw.svg)\\n\\nVAST converts enum, address, and subnet types to\\n[extension-types][arrow-extension-types]. All types are self-describing and part\\nof the record batch meta data. Conversion is bi-directional. Both Parquet and\\nFeather support fully nested structures in this type system. In theory. Our\\nthird blog post in this series describes the hurdles we had to overcome to make\\nit work in practice.\\n\\n[arrow-extension-types]: https://arrow.apache.org/docs/format/Columnar.html#extension-types\\n\\nIn the next blog post, we perform a quantitative analysis of the two formats: how\\nwell do they compress the original data? How much space do they take up in\\nmemory? How much CPU time do I pay for how much space savings? In the meantime,\\nif you want to learn more about Parquet, take a look at the [blog post\\nseries][arrow-parquet-blog] from the Arrow team.\\n\\n[arrow-parquet-blog]: https://arrow.apache.org/blog/2022/10/05/arrow-parquet-encoding-part-1/"},{"id":"/a-git-retrospective","metadata":{"permalink":"/blog/a-git-retrospective","source":"@site/blog/a-git-retrospective/index.md","title":"A Git Retrospective","description":"The VAST project is roughly a decade old. But what happened over the last 10","date":"2022-09-15T00:00:00.000Z","formattedDate":"September 15, 2022","tags":[{"label":"git","permalink":"/blog/tags/git"},{"label":"r","permalink":"/blog/tags/r"},{"label":"quarto","permalink":"/blog/tags/quarto"},{"label":"notebooks","permalink":"/blog/tags/notebooks"},{"label":"engineering","permalink":"/blog/tags/engineering"},{"label":"open-source","permalink":"/blog/tags/open-source"}],"readingTime":4.54,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"A Git Retrospective","authors":"mavam","date":"2022-09-15T00:00:00.000Z","tags":["git","r","quarto","notebooks","engineering","open-source"]},"prevItem":{"title":"Parquet & Feather: Enabling Open Investigations","permalink":"/blog/parquet-and-feather-enabling-open-investigations"},"nextItem":{"title":"Public Roadmap and Open RFCs","permalink":"/blog/public-roadmap-and-open-rfcs"}},"content":"The VAST project is roughly a decade old. But what happened over the last 10\\nyears? This blog post looks back over time through the lens of the git *merge*\\ncommits.\\n\\nWhy merge commits? Because they represent a unit of completed contribution.\\nFeature work takes place in dedicated branches, with the merge to the main\\nbranch sealing the deal. Some feature branches have just one commit, whereas\\nothers dozens. The distribution is not uniform. As of `6f9c84198` on Sep 2,\\n2022, there are a total of 13,066 commits, with 2,334 being merges (17.9%).\\nWe\u2019ll take a deeper look at the merge commits.\\n\\n\x3c!--truncate--\x3e\\n\\n``` bash\\ncd /tmp\\ngit clone https://github.com/tenzir/vast.git\\ncd vast\\ngit checkout 6f9c841980b2333028b1ac19e2a21e99d96cbd36\\ngit log --merges --pretty=format:\\"%ad|%d\\" --date=iso-strict |\\n  sed -E \'s/(.+)\\\\|.*tag: ([^,)]+).*/\\\\1 \\\\2/\' |\\n  sed -E \'s/(.*)\\\\|.*/\\\\1 NA/\' \\\\\\n  > /tmp/vast-merge-commits.txt\\n```\\n\\nFor the statistics, we\u2019ll switch to R. In all subsequent figures, a single point\\ncorresponds to a merge commit. The reduced opacity alleviates the effects of\\noverplotting.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nlibrary(lubridate)\\nlibrary(readr)\\n\\ntheme_set(theme_minimal())\\n\\ndata <- read_table(\\"/tmp/vast-merge-commits.txt\\",\\n  col_names = c(\\"time\\", \\"tag\\"),\\n  col_types = \\"Tc\\"\\n) |>\\n  arrange(time) |>\\n  mutate(count = row_number())\\n\\nfirst_contribution <- \\\\(x) data |>\\n  filter(time >= x) |>\\n  pull(count) |>\\n  first()\\n\\nevents <- tribble(\\n  ~time, ~event,\\n  ymd(\\"2016-03-17\\"), \\"NSDI \'16\\\\npublication\\",\\n  ymd(\\"2017-08-31\\"), \\"Tenzir\\\\nincorporated\\",\\n  ymd(\\"2018-07-01\\"), \\"Tobias\\",\\n  ymd(\\"2019-09-15\\"), \\"Dominik\\",\\n  ymd(\\"2020-01-01\\"), \\"Benno\\",\\n  ymd(\\"2021-12-01\\"), \\"Thomas\\",\\n  ymd(\\"2022-07-01\\"), \\"Patryk\\",\\n) |>\\n  mutate(time = as.POSIXct(time), count = Vectorize(first_contribution)(time))\\n\\ndata |>\\n  ggplot(aes(x = time, y = count)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = events,\\n    aes(xend = time, yend = count + 200),\\n    color = \\"red\\"\\n  ) +\\n  geom_label(\\n    data = events,\\n    aes(y = count + 200, label = event),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  scale_x_datetime(date_breaks = \\"1 year\\", date_labels = \\"%Y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg1 from \'./index_files/figure-gfm/full-time-spectrum-1.svg\';\\n\\n<Svg1 />\\n\\nPrior to Tenzir taking ownership of the project and developing VAST, it was a\\ndissertation project evolving along during PhD work at the University of\\nCalifornia, Berkeley. We can see that the first pre-submission crunch started a\\nfew months before the [NSDI \u201916\\npaper](https://matthias.vallentin.net/papers/nsdi16.pdf).\\n\\nTenzir was born in fall 2017. Real-world contributions arrived as of 2018 when\\nthe small team set sails. Throughput increased as core contributors joined the\\nteam. Fast-forward to 2020 when we started doing public releases. The figure\\nbelow shows how this process matured.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(ggrepel)\\n\\ndata |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.1) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"red\\",\\n    segment.alpha = 0.2,\\n    box.padding = 0.2\\n  ) +\\n  scale_x_datetime(\\n    date_breaks = \\"1 year\\",\\n    limits = c(as.POSIXct(ymd(\\"2020-01-01\\")), max(data$time)),\\n    date_labels = \\"%Y\\"\\n  ) +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg2 from \'./index_files/figure-gfm/since-2020-1.svg\';\\n\\n<Svg2 />\\n\\nAs visible from the tag labels, we were at [CalVer](https://calver.org) for a\\nwhile, but ultimately switched to [SemVer](https://semver.org). Because we had\\nalready commercial users at the time, this helped us better communicate breaking\\nvs.\xa0non-breaking changes.\\n\\nLet\u2019s zoom in on all releases since v1.0. At this time, we had a solid\\nengineering and release process in place.\\n\\n<details><summary>Code</summary>\\n\\n``` r\\nlibrary(tidyr)\\nv1_0_0_rc1_time <- data |>\\n  filter(tag == \\"v1.0.0-rc1\\") |>\\n  pull(time)\\n\\nsince_v1_0_0_rc1 <- data |> filter(time >= v1_0_0_rc1_time)\\n\\nrc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(grepl(\\"rc\\", tag))\\n\\nnon_rc <- since_v1_0_0_rc1 |>\\n  drop_na() |>\\n  filter(!grepl(\\"rc\\", tag))\\n\\nsince_v1_0_0_rc1 |>\\n  ggplot(aes(x = time, y = count, label = tag)) +\\n  geom_point(size = 1, alpha = 0.2) +\\n  geom_segment(\\n    data = non_rc,\\n    aes(xend = time, yend = min(count)), color = \\"red\\"\\n  ) +\\n  geom_text_repel(\\n    size = 2,\\n    min.segment.length = 0,\\n    max.overlaps = Inf,\\n    segment.color = \\"grey\\",\\n    box.padding = 0.7\\n  ) +\\n  geom_point(\\n    data = rc, aes(x = time, y = count),\\n    color = \\"blue\\",\\n    size = 2\\n  ) +\\n  geom_point(\\n    data = non_rc, aes(x = time, y = count),\\n    color = \\"red\\",\\n    size = 2\\n  ) +\\n  geom_label(data = non_rc, aes(y = min(count)), size = 2, color = \\"red\\") +\\n  scale_x_datetime(date_breaks = \\"1 month\\", date_labels = \\"%b %y\\") +\\n  labs(x = \\"Time\\", y = \\"Merge Commits\\")\\n```\\n\\n</details>\\n\\nimport Svg3 from \'./index_files/figure-gfm/since-v1.0-1.svg\';\\n\\n<Svg3 />\\n\\nThe v2.0 release was a hard one for us, given the long distance to v1.1. We\\nmerged too much and testing took forever. Burnt by the time sunk in testing and\\nfixups, we decided to switch to an LPU model (\u201cleast publishable unit\u201d) to\\nreduce release cadence. We didn\u2019t manage to implement this model until after\\nv2.1 though, where the release cadence finally gets smaller. A monthly release\\nfeels about the right for our team size.\\n\\nThe key challenge is minimizing the feature freeze phase. The first release\\ncandidate (RC) kicks this phase off, and the final release lifts the\\nrestriction. In this period, features are not allowed to be merged.[^1] This is\\na delicate time window: too long and the fixups in the RC phase cause the\\npostponed pull requests to diverge, too short and we compromise on testing\\nrigor, causing a release that doesn\u2019t meet our Q&A requirements.\\n\\nThis is where we stand as of today. We\u2019re happy how far along we came, but\\nmany challenges still lay ahead of us. Increased automation and deeper testing\\nis the overarching theme, e.g., code coverage, fuzzing, GitOps. We\u2019re constantly\\nstriving to improve or processes. With a small team of passionate, senior\\nengineers, this is a lot of fun!\\n\\n[^1]: We enforced this with a `blocked` label. CI [doesn\u2019t allow\\n    merging](https://github.com/tenzir/vast/blob/6f9c841980b2333028b1ac19e2a21e99d96cbd36/.github/workflows/blocked.yaml) when this label is on a pull request."},{"id":"/public-roadmap-and-open-rfcs","metadata":{"permalink":"/blog/public-roadmap-and-open-rfcs","source":"@site/blog/public-roadmap-and-open-rfcs/index.md","title":"Public Roadmap and Open RFCs","description":"Open Source needs Open Governance","date":"2022-09-07T00:00:00.000Z","formattedDate":"September 7, 2022","tags":[{"label":"roadmap","permalink":"/blog/tags/roadmap"},{"label":"github","permalink":"/blog/tags/github"},{"label":"rfc","permalink":"/blog/tags/rfc"},{"label":"open-source","permalink":"/blog/tags/open-source"}],"readingTime":3.745,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Public Roadmap and Open RFCs","description":"Open Source needs Open Governance","authors":"mavam","date":"2022-09-07T00:00:00.000Z","last_updated":"2023-02-08T00:00:00.000Z","tags":["roadmap","github","rfc","open-source"]},"prevItem":{"title":"A Git Retrospective","permalink":"/blog/a-git-retrospective"},"nextItem":{"title":"VAST v2.3","permalink":"/blog/vast-v2.3"}},"content":"We are happy to announce that we have published [our engineering\\nroadmap][roadmap] along with an [RFC process][rfc] to actively participate in\\nshaping upcoming topics. This blog post explains why and how we did it.\\n\\n[roadmap]: https://vast.io/roadmap\\n[rfc]: https://github.com/tenzir/vast/tree/master/rfc\\n\\n\x3c!--truncate--\x3e\\n\\nAs a community-first open-source project, we constantly strive for increasing\\ntransparency. Our long-term goal is establishing a fully open governance model.\\nThis will allow for a clear delineation of the open-source project and unbiased\\ncommercial offerings that we, the engineering team behind VAST at\\n[Tenzir](https://tenzir.com), provide on top. Until we have bootstrapped\\nourselves and an active community, we aim for the right balance between open\\nand closed.\\n\\nOne step in the direction of open is publishing our [roadmap][roadmap] and\\nenabling the community to participate in the planning through an [Request For\\nComments (RFC)][rfc] process.\\n\\n## Public Roadmap\\n\\nIn the process of opening the roadmap, we had to answer several questions:\\n\\n1. **Audience**: should the content be for users only? What about developers?\\n   Should we only mention features or also refactorings?\\n\\n2. **Interaction**: should this just be a read-only page or something the\\n   community can directly interact with?\\n\\n3. **Tooling**: what is the right tool to encode the roadmap?\\n\\nLet\'s go through them one by one.\\n\\nRegarding audience, we want to avoid an overly narrow target group, as we are in\\nphase of growth where breadth instead of depth is more important. Moreover, we\\ngain more transparency if we can unveil all ongoing thrusts. Therefore, we want\\nto cover the full spectrum of personas, but make it possible for each individual\\ntype of persona to get a relevant view.\\n\\nRegarding interaction, we are actively looking for engagement. Throwing a\\nread-only version over the fence to the community is certainly informational,\\nbut we are looking for creating dialogue. Therefore, we want to allow everyone\\nto discuss the various roadmap items in the open.\\n\\nRegarding tooling, we are in need for something that integrates well with the\\nexisting environment. Our GitHub presence includes code, documentation, website\\ncontent, and third-party integrations. We also promote use of GitHub Discussions\\nto engage with us. This makes GitHub the focal point to engage with the content.\\nTherefore, we decided to encode the roadmap as GitHub issues; for clarity in a\\ndedicated repository at <https://github.com/tenzir/public-roadmap>.\\n\\nWe decided against dual-purposing the issue tracker of our main repository\\n<https://github.com/tenzir/vast> because it would add roadmap items as many\\nopen, long-running issues that scatter the attention and potentially confuse the\\ncommunity. That said, the primary value of the issue tracker is the layer on top\\nof issues: [GitHub Projects][github-projects], which allows for organizing\\nissues across multiple dimensions in a visually appealing way.\\n\\n[github-projects]: https://docs.github.com/en/issues/planning-and-tracking-with-projects\\n\\nThe quarterly board view make it easy to understand ongoing thrusts:\\n\\n[![Github Roadmap - Board](roadmap-board.jpg)][roadmap]\\n\\nThe milestones view provides a different perspective that focuses more on the\\nbigger-picture theme:\\n\\n[![Github Roadmap - Milestones](roadmap-milestones.jpg)][roadmap]\\n\\n## Open RFCs\\n\\nThe roadmap provides a lens into the short-term future. We don\'t want it to be\\njust read-only. Fundamentally, we want to build something that our users love.\\nWe also want to tap into the full potential of our enthusiasts by making it\\npossible to engage in technical depth with upcoming changes. Therefore, we are\\nestablishing a formal [Request for Comments (RFC) process][rfc].\\n\\nTo get an idea, how an RFC looks like, here\'s the [RFC template][rfc-template]:\\n\\n[rfc-template]: https://github.com/tenzir/vast/blob/main/rfc/000-template/README.md\\n\\nimport CodeBlock from \'@theme/CodeBlock\';\\nimport Template from \'!!raw-loader!@site/../rfc/000-template/README.md\';\\n\\n<CodeBlock language=\\"markdown\\">{Template}</CodeBlock>\\n\\n[RFC-001: Composable Pipelines](https://github.com/tenzir/vast/pull/2511) is an\\nexample instantiation of this template.\\n\\nThe RFC reviews take place 100% in the open. As of today, reviewers constitute\\nmembers from Tenzir\'s engineering team. Given our current resource constraints\\nand project state, we can only support a corporate-backed governance model. That\\nsaid, opening ourselves up is laying the foundation of trust and committment\\nthat we want to go beyond a walled garden. We understand that this is a long\\njourney and are excited about what\'s ahead of us.\\n\\nWhen an RFC gets accepted, it means that we put it on the roadmap, adjacent to\\nexisting items that compete for prioritization. In other words, even though we\\naccepted an RFC, there will be an indeterminate period of time until we can\\ndevote resources. We will always encourage community-led efforts and are\\nenthusiastic about supporting external projects that we can support within our\\ncapacities.\\n\\nThese are our \\"growing pains\\" that we can hopefully overcome together while\\nbuilding a thriving community. We still have our [community chat](/discord)\\nwhere we are looking forward to interact with everyone with questions or\\nfeedback. See you there!"},{"id":"/vast-v2.3","metadata":{"permalink":"/blog/vast-v2.3","source":"@site/blog/vast-v2.3/index.md","title":"VAST v2.3","description":"Automatic Rebuilds","date":"2022-09-01T00:00:00.000Z","formattedDate":"September 1, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"rebuild","permalink":"/blog/tags/rebuild"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.9,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.3","description":"Automatic Rebuilds","authors":"dominiklohmann","date":"2022-09-01T00:00:00.000Z","tags":["release","rebuild","performance"]},"prevItem":{"title":"Public Roadmap and Open RFCs","permalink":"/blog/public-roadmap-and-open-rfcs"},"nextItem":{"title":"Richer Typing in Sigma","permalink":"/blog/richer-typing-in-sigma"}},"content":"[VAST v2.3][github-vast-release] is now available, which introduces an automatic\\ndata defragmentation capability.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.3.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Automatic Rebuilds\\n\\nVAST server processes now continuously rebuild partitions in the background. The\\nfollowing diagram visualizes what happens under the hood:\\n\\n![Rebuild](rebuild.excalidraw.svg)\\n\\nRebuilding kicks in when a partition has the following properties:\\n\\n1. **Outdated**: if a partitions does not have the latest partition version, it\\n   may not enjoy the latest features and optimizations. It makes it also faster\\n   to adopt VAST versions that include breaking changes in the storage layout.\\n   Therefore, VAST rebuilds outdated partitions to bring them into the most\\n   recent state.\\n\\n2. **Undersized**: numerous small partitions can cause fragmentation in the\\n   catalog, causing higher memory consumption, larger database footprint, and\\n   slower queries. Rebuilding merges undersized partitions, thereby\\n   defragmenting the system. This reduces the resource footprint and makes\\n   queries faster.\\n\\nTo enable automatic rebuilding, set the new `vast.automatic-rebuild` option.\\n\\n```yaml\\nvast:\\n  # Control automatic rebuilding of partitions in the background for\\n  # optimization purposes. The given number controls how many rebuilds to run\\n  # concurrently, and thus directly controls the performance vs. memory and CPU\\n  # usage trade-off. Set to 0 to disable. Defaults to 1.\\n  automatic-rebuild: 1\\n```\\n\\nNow that we have an LSM-style merge operation of partitions, we reduced\\nthe partition cutoff timeout to 5 minutes from 1 hour by default (controlled\\nthrough the option `vast.active-partition-timeout`). This reduces the risk of\\ndata loss in case of a crash. This comes in handy in particular for low-volume\\ndata sources that never exhaust their capacity.\\n\\n## Optional Partition Indexes\\n\\nHistorically, VAST evolved from a special-purpose bitmap indexing system into a\\ngeneral-purpose telemetry engine for security data. Today, VAST has a two-tiered\\nindexing architecture with sparse sketch structures at the top, followed by a\\nsecond layer of dense indexes. As of this release, it is possible to disable\\nthis second layer.\\n\\nThe space savings can be substantial based on the size of your index. For\\nexample, if the first layer of indexing always yields highly selective results,\\nthen it the dense indexes do not provide a lot of value. One scenario would be\\nretro-matching: if you only do IoC-style point queries, they will be most likely\\ncovered well by the sketches. If you do not have selective queries, the dense\\nindex is not helping much anyway, since you need access the base data anyway. A\\nreally good use case for the indexes when your have a scatterd data access\\npatterns, i.e., highly selective results *within* a partition, but a result that\\nspans many disparate partitions.\\n\\nIn a simplified model, VAST performs three steps when executing a query:\\n\\n1. Send the query to the catalog, which maintains VAST\'s partitions, and ask it\\n   for a list of candidate partitions. The catalog maintains the first tier of\\n   sparse indexes, currently one per partition.\\n\\n2. Send the query to all candidate partitions in parallel, each of which\\n   contains dense indexes for all fields in the partition\'s schema. The index\\n   lookup yields a set of candidate records IDs within the partition.\\n\\n3. Send the query to all candidate partition\'s stores, provided the index lookup\\n   yielded record IDs. Then evaluating the query against the candidate events\\n   and return the result.\\n\\nHere\'s how you can configure a partition index to be disabled:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n        # Don\'t create partition indexes the suricata.http.http.url field.\\n      - targets:\\n          - suricata.http.http.url\\n        partition-index: false\\n        # Don\'t create partition indexes for fields of type addr.\\n      - targets:\\n          - :ip\\n        partition-index: false\\n```\\n\\n## Improved Responsiveness Under High Load\\n\\nTwo small changes improve VAST\'s behavior under exceptionally high load.\\n\\nFirst, the new `vast.connection-timeout` option allows for modifying the default\\nclient-to-server connection timeout of 10 seconds. Previously, if a VAST server\\nwas too busy to respond to a new client within 10 seconds, the client simply\\nexited with an unintelligable `request_timeout` error message. Here\'s how you\\ncan set a custom timeout:\\n\\n```yaml\\nvast:\\n  # The timeout for connecting to a VAST server. Set to 0 seconds to wait\\n  # indefinitely.\\n  connection-timeout: 10s\\n```\\n\\nThe option is additionally available under the environment variable\\n`VAST_CONNECTION_TIMEOUT` and the `--connection-timeout` command-line option.\\n\\nSecond, we improved the operability of VAST servers under high load from\\nautomated low-priority queries. We noticed that when spawning thousands of\\nautomated retro-match queries that compaction would stall and make little\\nvisible progress, risking the disk running full or no longer being compliant\\nwith GDPR-related policies enforced by compaction.\\n\\nTo ensure that compaction\'s internal and regular user-issued queries work as\\nexpected even in this scenario, VAST now considers queries issued with\\n`--low-priority`, with even less priority compared to regular queries (down from\\n33.3% to 4%) and internal high-priority queries used for rebuilding and\\ncompaction (down from 12.5% to 1%)."},{"id":"/richer-typing-in-sigma","metadata":{"permalink":"/blog/richer-typing-in-sigma","source":"@site/blog/richer-typing-in-sigma/index.md","title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","date":"2022-08-12T00:00:00.000Z","formattedDate":"August 12, 2022","tags":[{"label":"sigma","permalink":"/blog/tags/sigma"},{"label":"regex","permalink":"/blog/tags/regex"},{"label":"query-frontend","permalink":"/blog/tags/query-frontend"}],"readingTime":4.685,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"title":"Richer Typing in Sigma","description":"Towards Native Sigma Rule Execution","authors":"mavam","date":"2022-08-12T00:00:00.000Z","last_updated":"2023-02-12T00:00:00.000Z","tags":["sigma","regex","query-frontend"]},"prevItem":{"title":"VAST v2.3","permalink":"/blog/vast-v2.3"},"nextItem":{"title":"VAST v2.2","permalink":"/blog/vast-v2.2"}},"content":"VAST\'s Sigma frontend now supports more modifiers. In the Sigma language,\\nmodifiers transform predicates in various ways, e.g., to apply a function over a\\nvalue or to change the operator of a predicate. Modifiers are the customization\\npoint to enhance expressiveness of query operations.\\n\\nThe new [pySigma][pysigma] effort, which will eventually replace the\\nnow-considered-legacy [sigma][sigma] project, comes with new modifiers as well.\\nMost notably, `lt`, `lte`, `gt`, `gte` provide comparisons over value domains\\nwith a total ordering, e.g., numbers: `x >= 42`. In addition, the `cidr`\\nmodifier interprets a value as subnet, e.g., `10.0.0.0/8`. Richer typing!\\n\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[pysigma]: https://github.com/SigmaHQ/pySigma\\n\\n\x3c!--truncate--\x3e\\n\\nHow does the frontend work? Think of it as a parser that processes the YAML and\\ntranslates it into an expression tree, where the leaves are predicates with\\ntyped operands according to VAST\'s data model. Here\'s how it works:\\n\\n![Sigma Query Frontend](sigma-query-frontend.excalidraw.svg)\\n\\nLet\'s take a closer look at some Sigma rule modifiers:\\n\\n```yaml\\nselection:\\n  x|re: \'f(o+|u)\'\\n  x|lt: 42\\n  x|cidr: 192.168.0.0/23\\n  x|base64offset|contains: \'http://\'\\n```\\n\\nThe `|` symbol applies a modifier to a field. Let\'s walk through the above\\nexample:\\n\\n1. The `re` modifier changes the predicate operand from `x == \\"f(o+|u)\\"` to\\n   `x == /f(o+|u)/`, i.e., the type of the right-hand side changes from `string`\\n   to `pattern`.\\n\\n2. The `lt` modifier changes the predicate operator from `==` to `<`, i.e.,\\n   `x == 42` becomes `x < 42`.\\n\\n3. The `cidr` modifier changes the predicate operand to type subnet. In VAST,\\n   parsing the operand type into a subnet happens automatically, so the Sigma\\n   frontend only changes the operator to `in`. That is, `x == \\"192.168.0.0/23\\"`\\n   becomes `x in 192.168.0.0/23`. Since VAST supports top-k prefix search on\\n   subnets natively, nothing else needs to be changed.\\n\\n   Other backends expand this to:\\n\\n   ```c\\n   x == \\"192.168.0.*\\" || x == \\"192.168.1.*\\"\\n   ```\\n\\n   This expansion logic on strings doesn\'t scale very well: for a `/22`, you\\n   would have to double the number of predicates, and for a `/21` quadruple\\n   them. This is where rich and deep typing in the language pays off.\\n\\n4. `x`: there are two modifiers that operate in a chained fashion,\\n   transforming the predicate in two steps:\\n\\n   1. Initial: `x == \\"http://\\"`\\n   2. `base64offset`: `x == \\"aHR0cDovL\\" || x == \\"h0dHA6Ly\\" || x == \\"odHRwOi8v\\"`\\n   3. `contains`: `x in \\"aHR0cDovL\\" || x in \\"h0dHA6Ly\\" || x in \\"odHRwOi8v\\"`\\n\\n   First, `base64offset` always expands a value into a disjunction of 3\\n   predicates, each of which performs an equality comparison to a\\n   Base64-transformed value.[^1]\\n\\n   Thereafter, the `contains` modifier translates the respective predicate\\n   operator from `==` to `in`. Other Sigma backends that don\'t support substring\\n   search natively transform the value instead by wrapping it into `*`\\n   wildcards, e.g., translate `\\"foo\\"` into `\\"*foo*\\"`.\\n\\n[^1]: What happens under the hood is a padding a string with spaces. [Anton\\nKutepov\'s article][sigma-article] illustrates how this works.\\n\\n[sigma-article]: https://tech-en.netlify.app/articles/en513032/index.html\\n\\nOur ultimate goal is to support a fully function executional platform for Sigma\\nrules. The table below shows the current implementation status of modifiers,\\nwhere \u2705 means implemented, \ud83d\udea7 not yet implemented but possible, and \u274c not yet\\nsupported by VAST\'s execution engine:\\n\\n|Modifier|Use|sigmac|VAST|\\n|--------|---|:----:|:--:|\\n|`contains`|perform a substring search with the value|\u2705|\u2705|\\n|`startswith`|match the value as a prefix|\u2705|\u2705|\\n|`endswith`|match the value as a suffix|\u2705|\u2705|\\n|`base64`|encode the value with Base64|\u2705|\u2705\\n|`base64offset`|encode value as all three possible Base64 variants|\u2705|\u2705\\n|`utf16le`/`wide`|transform the value to UTF16 little endian|\u2705|\ud83d\udea7\\n|`utf16be`|transform the value to UTF16 big endian|\u2705|\ud83d\udea7\\n|`utf16`|transform the value to UTF16|\u2705|\ud83d\udea7\\n|`re`|interpret the value as regular expression|\u2705|\ud83d\udea7\\n|`cidr`|interpret the value as a IP CIDR|\u274c|\u2705\\n|`all`|changes the expression logic from OR to AND|\u2705|\u2705\\n|`lt`|compare less than (`<`) the value|\u274c|\u2705\\n|`lte`|compare less than or equal to (`<=`) the value|\u274c|\u2705\\n|`gt`|compare greater than (`>`) the value|\u274c|\u2705\\n|`gte`|compare greater than or equal to (`>=`) the value|\u274c|\u2705\\n|`expand`|expand value to placeholder strings, e.g., `%something%`|\u274c|\u274c\\n\\nAside from completing the implementation of the missing modifiers, there are\\nthree missing pieces for Sigma rule execution to become viable in VAST:\\n\\n1. **Regular expressions**: VAST currently has no efficient mechanism to execute\\n   regular expressions. A regex lookup requires a full scan of the data.\\n   Moreover, the regular expression execution speed is abysimal. But we are\\n   aware of it and are working on this soon. The good thing is that the\\n   complexity of regular expression execution over batches of data is\\n   manageable, given that we would call into the corresponding [Arrow Compute\\n   function][arrow-containment-tests] for the heavy lifting. The number one\\n   challenge will be reduing the data to scan, because the Bloom-filter-like\\n   sketch data structures in the catalog cannot handle pattern types. If the\\n   sketches cannot identify a candidate set, all data needs to be scanned,\\n\\n   To alleviate the effects of full scans, it\'s possible to winnow down the\\n   candidate set of partitions by executing rules periodically. When making the\\n   windows asymptotically small, this yields effectively streaming execution,\\n   which VAST already supports in the form of \\"live queries\\".\\n\\n2. **Case-insensitive strings**: All strings in Sigma rules are case-insensitive\\n   by default, but VAST\'s string search is case-sensitive. As a workaround, we\\n   could translate Sigma strings into regular expressions, e.g., `\\"Foo\\"` into\\n   `/Foo/i`. Unfortunately there is a big performance gap between string\\n   equality search and regular expression search. We will need to find a better\\n   solution for production-grade rule execution.\\n\\n3. **Field mappings**: while Sigma rules execute already syntactically, VAST\\n   currently doesn\'t touch the field names in the rules and interprets them as\\n   field extractors. In other words, VAST doesn\'t support\\n   the Sigma taxonomy yet. Until we provide the mappings, you can already write\\n   generic Sigma rules using concepts.\\n\\n[arrow-containment-tests]: https://arrow.apache.org/docs/cpp/compute.html#containment-tests\\n\\nPlease don\'t hesitate to swing by our [community chat](/discord)\\nand talk with us if you are passionate about Sigma and other topics around open\\ndetection and response."},{"id":"/vast-v2.2","metadata":{"permalink":"/blog/vast-v2.2","source":"@site/blog/vast-v2.2/index.md","title":"VAST v2.2","description":"Pipelines","date":"2022-08-05T00:00:00.000Z","formattedDate":"August 5, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"summarize","permalink":"/blog/tags/summarize"},{"label":"pipelines","permalink":"/blog/tags/pipelines"}],"readingTime":2.135,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v2.2","description":"Pipelines","authors":"lava","date":"2022-08-05T00:00:00.000Z","tags":["release","summarize","pipelines"]},"prevItem":{"title":"Richer Typing in Sigma","permalink":"/blog/richer-typing-in-sigma"},"nextItem":{"title":"VAST v2.1","permalink":"/blog/vast-v2.1"}},"content":"We released [VAST v2.2][github-vast-release] \ud83d\ude4c! Transforms now have a new name:\\n[pipelines](/blog/vast-v2.2#transforms-are-now-pipelines). The [summarize\\noperator](/blog/vast-v2.2#summarization-improvements) also underwent a facelift,\\nmaking aggregation functions pluggable and allowing for assigning names to\\noutput fields.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.2.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Transforms are now Pipelines\\n\\nAfter carefully reconsidering our naming decisions related to query execution\\nand data transformation, we came up with a naming convention that does a better\\njob in capturing the underlying concepts.\\n\\nMost notably, we renamed *transforms* to *pipelines*. A transform *step* is now a\\npipeline *operator*. This nomenclature is much more familiar to users coming\\nfrom dataflow and collection-based query engines. The implementation underneath\\nhasn\'t changed. As in the [Volcano model][volcano], data still flows through\\noperators, each of which consumes input from upstream operators and produces\\noutput for downstream operators. What we term a pipeline is the sequence of such\\nchained operators.\\n\\n[volcano]: https://paperhub.s3.amazonaws.com/dace52a42c07f7f8348b08dc2b186061.pdf\\n\\nWhile pipelines are not yet available at the query layer, they soon will be.\\nUntil then, you can deploy pipelines at load-time to transform data in motion\\nor data at rest.\\n\\nFrom a user perspective, the configuration keys associated with transforms have\\nchanged. Here\'s the updated example from our previous [VAST v1.0 release\\nblog](/blog/vast-v1.0).\\n\\n```yaml\\nvast:\\n  # Specify and name our pipelines, each of which are a list of configured\\n  # pipeline operators. Pipeline operators are plugins, enabling users to \\n  # write complex transformations in native code using C++ and Apache Arrow.\\n  pipelines:\\n     # Prevent events with certain strings to be exported, e.g., \\n     # \\"tenzir\\" or \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each pipeline at server- or client-side, on\\n  # `import` or `export`, and restrict them to a list of event types.\\n  pipeline-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - pipeline: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Summarization Improvements\\n\\nIn line with the above nomenclature changes, we\'ve improved the behavior of the\\n`summarize` operator. It is now possible to specify an explicit\\nname for the output fields. This is helpful when the downstream processing needs\\na predictable schema. Previously, VAST took simply the name of the input field.\\nThe syntax was as follows:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    min:\\n      - ts # implied name for aggregate field\\n```\\n\\nWe now switched the syntax such that the new field name is at the beginning:\\n\\n```yaml\\nsummarize:\\n  group-by:\\n    - ...\\n  aggregate:\\n    ts_min: # explicit name for aggregate field\\n      min: ts\\n```\\n\\nIn SQL, this would be the `AS` token: `SELECT min(ts) AS min_ts`."},{"id":"/vast-v2.1","metadata":{"permalink":"/blog/vast-v2.1","source":"@site/blog/vast-v2.1/index.md","title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","date":"2022-07-07T00:00:00.000Z","formattedDate":"July 7, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":3.935,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.1","description":"VAST v2.1 - Tune VAST Databases","authors":"dominiklohmann","date":"2022-07-07T00:00:00.000Z","tags":["release","performance"]},"prevItem":{"title":"VAST v2.2","permalink":"/blog/vast-v2.2"},"nextItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering"}},"content":"[VAST v2.1][github-vast-release] is out! This release comes with a particular\\nfocus on performance and reducing the size of VAST databases. It brings a new\\nutility for optimizing databases in production, allowing existing deployments to\\ntake full advantage of the improvements after upgrading.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## New Project Site\\n\\nVAST has new project site: [vast.io](https://vast.io). We ported all\\ndocumentation from `https://docs.tenzir.com`, added a lot of new content, and\\nrestructured the reading experience along the user journey.\\n\\nYou can find the Threat Bus documentation in Use VAST \u2192 Integrate \u2192 Threat\\nBus. Threat Bus is now officially in\\nmaintainance mode: we are only supporting existing features with bugfixes. That\\nsaid, Threat Bus will resurface in a new shape with its existing functionality\\nintegrated into VAST itself. Stay tuned.\\n\\n## Performance Improvements\\n\\nVAST now compresses data with [Zstd](http://www.zstd.net). The default\\nconfiguration achieves over 2x space savings. When transferring data between\\nclient and server processes, compression reduces the amount of transferred data\\nby up to 5x.\\n\\nAdditionally, VAST now compresses on-disk indexes with Zstd, resulting in a\\n50-80% size reduction depending on the type of indexes used.\\n\\nThis allowed us to increase the default partition size from 1,048,576 to\\n4,194,304 events[^1], and the default number of events in a single batch from 1,024\\nto 65,536, resulting in a massive performance increase at the cost of a ~20%\\nlarger memory footprint at peak loads. Use the option `vast.max-partition-size`\\nto tune this space-time tradeoff.\\n\\nTo benchmark this, we used [`speeve`][speeve] to generate 20 EVE JSON files\\ncontaining 8,388,608 events each[^2]. We spawned a VAST server process and ran\\n20 VAST client processes in parallel, with one process per file.\\n\\nWe observed a reduction of **up to 73%** of disk space utilization:\\n\\n![Database Size](storage-light.png#gh-light-mode-only)\\n![Database Size](storage-dark.png#gh-dark-mode-only)\\n\\nIn addition, we were able to scale the ingest rate by almost **6x** due to the\\nhigher batch size and the reduced memory usage per batch:\\n\\n![Ingest Rate](rate-light.png#gh-light-mode-only)\\n![Ingest Rate](rate-dark.png#gh-dark-mode-only)\\n\\nThe table below summaries the benchmarks:\\n\\n||VAST v2.0|VAST v2.1|Change|\\n|-:|:-|:-|:-|\\n|Ingest Duration|1,650 s|242 s|-85.3%|\\n|Ingest Rate|101,680 events/s|693,273 events/s|+581.8%|\\n|Index Size|14,791 MiB|5,721 MiB|-61.3%|\\n|Store Size|37,656 MiB|8,491 MiB|-77.5%|\\n|Database Size|52,446 MiB|14,212 MiB|-72.9%|\\n\\n:::note Compressed Filesystems\\nThe above benchmarks ran on filesystems without compression. We expect the gain\\nfrom compression to be smaller when using compressed filesystems like\\n[`btrfs`][btrfs].\\n:::\\n\\n[speeve]: https://github.com/satta/speeve\\n[btrfs]: https://btrfs.wiki.kernel.org/index.php/Main_Page\\n\\n[^1]: VAST v2.0 failed to write its partitions to disk with the defaults for\\n  v2.1 because the on-disk size exceeded the maximum possible size of a\\n  FlatBuffers table, which VAST internally uses to have an open standard for its\\n  persistent state.\\n[^2]: This resulted in 167,772,160 events, with a total of 200\'917\'930 unique\\n  values with a schema distribution of 80.74% `suricata.flow`, 7.85%\\n  `suricata.dns`, 5.35% `suricata.http`, 4.57% `suricata.fileinfo`, 1.04%\\n  `suricata.tls`, 0.41% `suricata.ftp`, and 0.04% `suricata.smtp`.\\n\\n## Rebuild VAST Databases\\n\\nThe new changes to VAST\'s internal data format only apply to newly ingested\\ndata. To retrofit changes, we introduce a new `rebuild` command with this\\nrelease. A rebuild effectively re-ingests events from existing partitions and\\natomically replaces them with partitions of the new format.\\n\\nThis makes it possible to upgrade persistent state to a newer version, or\\nrecreate persistent state after changing configuration parameters, e.g.,\\nswitching from the Feather to the Parquet store backend (that will land in\\nv2.2). Rebuilding partitions also recreates their sparse indexes that\\naccellerate query execution. The process takes place asynchronously in the\\nbackground.\\n\\nWe recommend running `vast rebuild` to upgrade your VAST v1.x partitions to VAST\\nv2.x partitions to take advantage of the new compression and an improved\\ninternal representation.\\n\\nThis is how you run it:\\n\\n```bash\\nvast rebuild [--all] [--undersized] [--parallel=<number>] [<expression>]\\n```\\n\\nA rebuild is not only useful when upgrading outdated partitions, but also when\\nchanging parameters of up-to-date partitions. Use the `--all` flag to extend a\\nrebuild operation to _all_ partitions. (Internally, VAST versions the partition\\nstate via FlatBuffers. An outdated partition is one whose version number is not\\nthe newest.)\\n\\nThe `--undersized` flag causes VAST to only rebuild partitions that are under\\nthe configured partition size limit `vast.max-partition-size`.\\n\\nThe `--parallel` options is a performance tuning knob. The parallelism level\\ncontrols how many sets of partitions to rebuild in parallel. This value defaults\\nto 1 to limit the CPU and memory requirements of the rebuilding process, which\\ngrow linearly with the selected parallelism level.\\n\\nAn optional expression allows for restricting the set of partitions to rebuild.\\nVAST performs a catalog lookup with the expression to identify the set of\\ncandidate partitions. This process may yield false positives, as with regular\\nqueries, which may cause unaffected partitions to undergo a rebuild. For\\nexample, to rebuild outdated partitions containing `suricata.flow` events\\nolder than 2 weeks, run the following command:\\n\\n```bash\\nvast rebuild \'#type == \\"suricata.flow\\" && #import_time < 2 weeks ago\'\\n```"},{"id":"/apache-arrow-as-platform-for-security-data-engineering","metadata":{"permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering","source":"@site/blog/apache-arrow-as-platform-for-security-data-engineering/index.md","title":"Apache Arrow as Platform for Security Data Engineering","description":"How VAST leverages Apache Arrow for Security Data Engineering","date":"2022-06-17T00:00:00.000Z","formattedDate":"June 17, 2022","tags":[{"label":"architecture","permalink":"/blog/tags/architecture"},{"label":"arrow","permalink":"/blog/tags/arrow"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":5.97,"hasTruncateMarker":true,"authors":[{"name":"Matthias Vallentin","title":"Founder & CEO","url":"https://github.com/mavam","email":"matthias@tenzir.com","imageURL":"https://github.com/mavam.png","key":"mavam"}],"frontMatter":{"description":"How VAST leverages Apache Arrow for Security Data Engineering","authors":"mavam","date":"2022-06-17T00:00:00.000Z","tags":["architecture","arrow","performance","query"]},"prevItem":{"title":"VAST v2.1","permalink":"/blog/vast-v2.1"},"nextItem":{"title":"VAST v2.0","permalink":"/blog/vast-v2.0"}},"content":"VAST bets on [Apache Arrow][arrow] as the open interface to structured data. By\\n\\"bet,\\" we mean that VAST does not work without Arrow. And we are not alone.\\nInflux\'s [IOx][iox], DataDog\'s [Husky][husky], Anyscale\'s [Ray][ray],\\n[TensorBase][tensorbase], and [others][arrow-projects] committed themselves to\\nmaking Arrow a corner stone of their system architecture. For us, Arrow was not\\nalways a required dependency. We shifted to a tighter integration over the years\\nas the Arrow ecosystem matured. In this blog post we explain our journey of\\nbecoming an Arrow-native engine.\\n\\n[arrow]: https://arrow.apache.org\\n[iox]: https://github.com/influxdata/influxdb_iox\\n[husky]: https://www.datadoghq.com/blog/engineering/introducing-husky/\\n[ray]: https://github.com/ray-project/ray\\n[tensorbase]: https://github.com/tensorbase/tensorbase\\n[arrow-projects]: https://arrow.apache.org/powered_by/\\n\\n\x3c!--truncate--\x3e\\n\\nToday, the need to bring advanced security analytics and data engineering\\ntogether is stronger than ever, but there is a huge gap between the two fields.\\nWe see Arrow as the vehicle to close this gap, allowing us developers to\\npractice *security data engineering* to make security analytics easy for users.\\nThat is, the experience should allow experts to interact with the data in the\\nsecurity domain, end-to-end without context switching. To achieve this, we began\\nour journey with VAST by developing a data model for structured security\\ntelemetry. Having worked for a decade with the [Zeek][zeek] (fka. Bro) network\\nsecurity monitor, we understood the value of having first-class support for\\ndomain-specific entities (e.g., native representation of IPv4 and IPv6\\naddresses) and type-specific operations (e.g., the ability to perform top-k\\nprefix search to answer subnet membership queries). In addition, the ability to\\nembed domain semantics with user-defined types (e.g., IP addresses, subnets, and\\nURLs) was central to expressing complex relationships to develop effective\\nanalytical models. It was clear that we needed the domain model deep in the core\\nof the system to successfully support security analytics.\\n\\nAfter having identified the data model requirements, the question of\\nrepresentation came next. At first, we unified the internal representation with\\na row-oriented representation using [MsgPack][msgpack], which comes with a\\nmechanism for adding custom types. The assumption was that a row-based data\\nrepresentation more closely matches typical event data (e.g., JSONL) and\\ntherefore allows for much higher processing rates. Moreover, early use cases of\\nVAST were limited to interactive, multi-dimensional search to extract a subset\\nof *entire* records, spread over a longitudinal archive of data. The\\nrow-oriented encoding worked well for this.\\n\\nBut as security operations were maturing, requirements extended to analytical\\nprocessing of structured data, making a columnar format increasingly beneficial.\\nAfter having witnessed first-hand the early commitment of [Ray][ray] to Arrow,\\nwe started using Arrow as optional dependency as additional column-oriented\\nencoding. We abstracted a batch of data encoding-independent behind a \\"table\\nslice\\":\\n\\n![MsgPack & Arrow](msgpack-arrow.excalidraw.svg)\\n\\nHiding the concrete encoding behind a cell-based access interface worked for\\nlow-volume use cases, but backfired as we scaled up and slowed us down\\nsubstantially in development. We needed to make a choice. This is where timing\\nwas right: our perception of the rapidly evolving Arrow ecosystem changed.\\nArrow-based runtimes were mushrooming all over the place. Nowadays it requires\\nonly a few lines of code to integrate Arrow data into the central logic of\\napplications. We realized that the primary value proposition of Arrow is to\\n*make data interoperability easy*.\\n\\nBut data interoperability is only a sufficient condition for enabling\\nsustainable security analytics. The differentiating value of a *security* data\\nplatform is support for the *security* domain. This is where Arrow\'s extension\\ntypes come into play. They add *semantics* to otherwise\\ngeneric types, e.g., by telling the user \\"this is a transport-layer port\\" and\\nnot just a 16-bit unsigned integer, or \\"this is a connection 4-tuple to\\nrepresent a network flow\\" instead of \\"this is a record with 4 fields of type\\nstring and unsigned integer\\". Extension types are composable and allow for\\ncreating a rich typing layer with meaningful domain objects on top of a\\nstandardized data representation. Since they are embedded in the data, they do\\nnot have to be made available out-of-band when crossing the boundaries of\\ndifferent tools. Now we have self-describing security data.\\n\\nInteroperability plus support for a domain-specific data model makes Arrow a\\nsolid *data plane*. It turns out that Arrow is much more than a standardized\\ndata representation. Arrow also comes with bag of tools for working with the\\nstandardized data. In the diagram below, we show the various Arrow pieces that\\npower the architecture of VAST:\\n\\n![Arrow Data Plane](arrow-data-plane.excalidraw.svg)\\n\\nIn the center we have the Arrow data plane that powers other parts of the\\nsystem. Green elements highlight Arrow building blocks that we use today, and\\norange pieces elements we plan to use in the future. There are several aspects\\nworth pointing out:\\n\\n1. **Unified Data Plane**: When users ingest data into VAST, the\\n   parsing process converts the native data into Arrow. Similarly, a\\n   conversation boundary exists when data leaves the system, e.g., when a user\\n   wants a query result shown in JSON, CSV, or some custom format. Source and\\n   sink data formats are exchangeable plugins.\\n\\n2. **Read/Write Path Separation**: one design goal of VAST is a strict\\n   separation of read and write path, in order to scale them independently. The\\n   write path follows a horizontally scalable architecture where builders (one per\\n   schema) turn the in-memory record batches into a persistent representation.\\n   VAST currently has support for Parquet and Feather.\\n\\n3. **Pluggable Query Engine**: VAST has live/continuous queries that simply run\\n   over the stream of incoming data, and historical queries that operate on\\n   persistent data. The harboring execution engine is something we are about to\\n   make pluggable. The reason is that VAST runs in extremely different\\n   environments, from cluster to edge. Query engines are usually optimized for a\\n   specific use case, so why not use the best engine for the job at hand? Arrow\\n   makes this possible. [DuckDB][duckdb] and [DataFusion][datafusion] are great\\n   example of embeddable query engines.\\n\\n4. **Unified Control Plane**: to realize a pluggable query engine, we also need\\n   a standardized control plane. This is where [Substrait][substrait] and\\n   Flight come into play. Flight for communication and Substrait as\\n   canonical query representation. We already experimented with Substrait,\\n   converting VAST queries into a logical query plan. In fact, VAST has a \\"query\\n   language\\" plugin to make it possible to translate security content. (For\\n   example, our Sigma plugin translates [Sigma rules][sigma]\\n   into VAST queries.) In short: Substrait is to the control plane what Arrow is\\n   to the data plane. Both are needed to modularize the concept of a query\\n   engine.\\n\\nMaking our own query engine more suitable for analytical workloads has\\nreceived less attention in the past, as we prioritized high-performance data\\nacquisition, low-latency search, in-stream matching using Compute,\\nand expressiveness of the underlying domain data model. We did so because VAST\\nmust run robustly in production on numerous appliances all over the world in a\\nsecurity service provider setting, with confined processing and storage where\\nefficiency is key.\\n\\nMoving forward, we are excited to bring more analytical horse power to the\\nsystem, while opening up the arena for third-party engines. With the bag of\\ntools from the Arrow ecosystem, plus all other embeddable Arrow engines that are\\nemerging, we have a modular architecture to can cover a very wide spectrum of\\nuse cases.\\n\\n[substrait]: https://substrait.io/\\n[datafusion]: https://arrow.apache.org/datafusion/\\n[msgpack]: https://msgpack.org/index.html\\n[duckdb]: https://duckdb.org/\\n[sigma]: https://github.com/SigmaHQ/sigma\\n[zeek]: https://zeek.org"},{"id":"/vast-v2.0","metadata":{"permalink":"/blog/vast-v2.0","source":"@site/blog/vast-v2.0/index.md","title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","date":"2022-05-16T00:00:00.000Z","formattedDate":"May 16, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"performance","permalink":"/blog/tags/performance"},{"label":"pcap","permalink":"/blog/tags/pcap"}],"readingTime":6.335,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v2.0","description":"VAST v2.0 - Smarter Query Scheduling & Tunable Filters","authors":"dominiklohmann","date":"2022-05-16T00:00:00.000Z","tags":["release","compaction","performance","pcap"]},"prevItem":{"title":"Apache Arrow as Platform for Security Data Engineering","permalink":"/blog/apache-arrow-as-platform-for-security-data-engineering"},"nextItem":{"title":"VAST v1.1.2","permalink":"/blog/vast-v1.1.2"}},"content":"Dear community, we are excited to announce [VAST v2.0][github-vast-release],\\nbringing faster execution of bulk-submitted queries, improved tunability of\\nindex structures, and new configurability through environment variables.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v2.0.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Scheduling\\n\\nVAST is now more intelligent in how it schedules queries.\\n\\nWhen a query arrives at the VAST server, VAST first goes to the catalog which\\nreturns a set of on-disk candidate partitions that the query may be applicable\\nto. Previous versions of VAST simply iterated through the available queries as\\nthey came in, loading partition by partition to extract events. Due to memory\\nconstraints, VAST is only able to keep some partitions in memory, which causes\\nfrequent loading and unloading of the same partitions for queries that access\\nthe same data. Now, VAST loads partitions depending on how many queries they are\\nrelevant for and evaluates all ongoing queries for one partition at a time.\\n\\nAdditionally, VAST now partitions the data for each schema separately, moving\\naway from partitions that contain events of multiple schemas. This helps with\\ncommon access patterns and speeds up queries restricted to a single schema.\\n\\nThe numbers speak for themselves:\\n\\n![Benchmarks](scheduler-light.png#gh-light-mode-only)\\n![Benchmarks](scheduler-dark.png#gh-dark-mode-only)\\n\\n## Updates to Aging, Compaction, and the Disk Monitor\\n\\nVAST v1.0 deprecated the experimental aging feature. Given popular demand we\'ve\\ndecided to un-deprecate it and to actually implement it on top of the same\\nbuilding blocks the new compaction mechanism uses, which means that it is now\\nfully working and no longer considered experimental.\\n\\nThe compaction plugin is now able to apply general time-based compactions that\\nare not restricted to a specific set of types. This makes it possible for\\noperators to implement rules like \\"delete all data after 1 week\\", without having\\nto list all possible data types that may occur.\\n\\nSome smaller interface changes improve the observability of the compactor for\\noperators: The  `vast compaction status` command prints the current compaction\\nstatus, and the `vast compaction list` command now lists all configured\\ncompaction rules of the VAST node.\\n\\nAdditionally, we\'ve improved overall stability and fault tolerance improvements\\nsurrounding the disk monitor and compaction features.\\n\\n## Fine-tuned Catalog Configuration\\n\\n:::note Advanced Users\\nThis section is for advanced users only.\\n:::\\n\\nThe catalog manages partition metadata and is responsible for deciding whether a\\npartition qualifies for a certain query. It does so by maintaining sketch data\\nstructures (e.g., Bloom filters, summary statistics) for each partition.\\nSketches are highly space-efficient at the cost of being probabilistic and\\nyielding false positives.\\n\\nDue to this characteristic, sketches can grow sublinear: doubling the number of\\nevents in a sketch does not lead to a doubling of the memory requirement.\\nBecause the catalog must be traversed in full for a given query it needs to be\\nmaintained in active memory to provide high responsiveness.\\n\\nA false positive can have substantial impact on the query latency by\\nmaterializing irrelevant partitions, which involves unnecessary I/O. Based on\\nthe cost of I/O, this penalty may be substantial. Conversely, reducing the false\\npositive rate increases the memory consumption, leading to a higher resident set\\nsize and larger RAM requirements.\\n\\nYou can control this space-time trade-off in the configuration section\\n`vast.index` by specifying index rules. Each rule corresponds to one sketch and\\nconsists of the following components:\\n\\n`targets`: a list of extractors to describe the set of fields whose values to\\nadd to the sketch. `fp-rate`: an optional value to control the false-positive\\nrate of the sketch.\\n\\nVAST does not create field-level sketches unless a dedicated rule with a\\nmatching target configuration exists. Here\'s an example:\\n\\n```yaml\\nvast:\\n  index:\\n    rules:\\n      - targets:\\n          # field synopses: need to specify fully qualified field name\\n          - suricata.http.http.url\\n        fp-rate: 0.005\\n      - targets:\\n          - :ip\\n        fp-rate: 0.1\\n```\\n\\nThis configuration includes two rules (= two sketches), where the first rule\\nincludes a field extractor and the second a type extractor. The first rule\\napplies to a single field, `suricata.http.http.url`, and has a false-positive\\nrate of 0.5%. The second rule creates one sketch for all fields of type `addr`\\nthat has a false-positive rate of 10%.\\n\\n## Configuring VAST with Environment Variables\\n\\nVAST now offers an additional configuration path besides editing YAML\\nconfiguration files and providing command line arguments: *setting environment\\nvariables*. This enables a convenient configuration experience when using\\ncontainer runtimes, such as Docker, where the other two configuration paths have\\na mediocre UX at best:\\n\\nThe container entry point is limited to adding command line arguments, where not\\nall options may be set. For Docker Compose and Kubernetes, it is often not\\ntrivially possible to even add command line arguments.\\n\\nProviding a manual configuration file is a heavy-weight action, because it\\nrequires (1) generating a potentially templated configuration file, and (2)\\nmounting that file into a location where VAST would read it.\\n\\nAn environment variable has the form `KEY=VALUE`. VAST processes only\\nenvironment variables having the form `VAST_{KEY}=VALUE`. For example,\\n`VAST_ENDPOINT=1.2.3.4` translates to the command line option\\n`--endpoint=1.2.3.4` and YAML configuration `vast.endpoint: 1.2.3.4`.\\n\\nRegarding precedence, environment variables override configuration file\\nsettings, and command line arguments override environment variables. Please\\nconsult the documentation for a more detailed explanation of how to specify keys\\nand values.\\n\\n## VLAN Tag Extraction and Better Packet Decapsulation\\n\\nVAST now extracts [802.1Q VLAN tags](https://en.wikipedia.org/wiki/IEEE_802.1Q)\\nfrom packets, making it possible to filter packets based on VLAN ID. The packet\\nschema includes a new nested record `vlan` with two fields: `outer` and `inner`\\nto represent the respective VLAN ID. For example, you can generate PCAP traces\\nof packets based on VLAN IDs as follows:\\n\\n```bash\\nvast export pcap \'vlan.outer > 0 || vlan.inner in [1, 2, 3]\' | tcpdump -r - -nl\\n```\\n\\nVLAN tags occur in many variations, and VAST extracts them in case of\\nsingle-tagging and  [QinQ\\ndouble-tagging](https://en.wikipedia.org/wiki/IEEE_802.1ad). Consult the PCAP\\ndocumentation for details on this feature.\\n\\nInternally, the packet decapsulation logic has been rewritten to follow a\\nlayered approach: frames, packets, and segments are the building blocks. The\\nplan is to reuse this architecture when switching to kernel-bypass packet\\nacquisition using DPDK. If you would like to see more work on the front of\\nhigh-performance packet recording, please reach out.\\n\\n## Breaking Changes\\n\\nThe `--verbosity` command-line option is now called `--console-verbosity`. The\\nshorthand options `-v`, `-vv`, `-vvv`, `-q`, `-qq`, and  `-qqq`  are unchanged.\\nThis aligns the command-line option with the configuration option\\n`vast.console-verbosity`, and disambiguates from the `vast.file-verbosity`\\noption.\\n\\nThe _Meta Index_ is now called the _Catalog_. This affects multiple status and\\nmetrics keys. We plan to extend the functionality of the Catalog in a future\\nrelease, turning it into a more powerful first instance for lookups.\\n\\nTransform steps that add or modify columns now add or modify the columns\\nin-place rather than at the end, preserving the nesting structure of the\\noriginal data.\\n\\n## Changes for Developers\\n\\nThe `vast get` command no longer exists. The command allowed for retrieving\\nevents by their internal unique ID, which we are looking to remove entirely in\\nthe future.\\n\\nChanges to the internal data representation of VAST require all transform step\\nplugins to be updated. The output format of the vast export arrow command\\nchanged for the address, subnet, pattern, and enumeration types, which are now\\nmodeled as [Arrow Extension\\nTypes](https://arrow.apache.org/docs/format/Columnar.html#extension-types). The\\nrecord type is no longer flattened. The mapping of VAST types to Apache Arrow\\ndata types  is now considered stable.\\n\\n## Smaller Things\\n\\n- VAST client commands now start much faster and use less memory.\\n- The `vast count --estimate \'<query>\'` feature no longer unnecessarily causes\\n  stores to load from disk, resulting in major speedups for larger databases and\\n  broad queries.\\n- The [tenzir/vast](https://github.com/tenzir/vast) repository now contains\\n  experimental Terraform scripts for deploying VAST to AWS Fargate and Lambda."},{"id":"/vast-v1.1.2","metadata":{"permalink":"/blog/vast-v1.1.2","source":"@site/blog/vast-v1.1.2/index.md","title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","date":"2022-03-29T00:00:00.000Z","formattedDate":"March 29, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":0.33,"hasTruncateMarker":true,"authors":[{"name":"Benno Evers","title":"Principal Engineer","url":"https://github.com/lava","email":"benno@tenzir.com","imageURL":"https://github.com/lava.png","key":"lava"}],"frontMatter":{"title":"VAST v1.1.2","description":"VAST v1.1.2 - Compaction & Query Language Frontends","authors":"lava","date":"2022-03-29T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v2.0","permalink":"/blog/vast-v2.0"},"nextItem":{"title":"VAST v1.1.1","permalink":"/blog/vast-v1.1.1"}},"content":"Dear community, we are happy to announce the release of [VAST\\nv1.1.2](https://github.com/tenzir/vast/releases/tag/v1.1.2), the latest release\\non the VAST v1.1 series. This release contains a fix for a race condition that\\ncould lead to VAST eventually becoming unresponsive to queries in large\\ndeployments.\\n\\n\x3c!--truncate--\x3e\\n\\nFixed a race condition that would cause queries to become stuck when an exporter\\nwould time out during the meta index lookup.\\n[#2165](https://github.com/tenzir/vast/pull/2165)"},{"id":"/vast-v1.1.1","metadata":{"permalink":"/blog/vast-v1.1.1","source":"@site/blog/vast-v1.1.1/index.md","title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","date":"2022-03-25T00:00:00.000Z","formattedDate":"March 25, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":0.635,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1.1","description":"VAST v1.1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-25T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.2","permalink":"/blog/vast-v1.1.2"},"nextItem":{"title":"VAST v1.1","permalink":"/blog/vast-v1.1"}},"content":"Dear community, we are excited to announce [VAST\\nv1.1.1][github-vast-release-new].\\n\\nThis release contains some important bug fixes on top of everything included in\\nthe [VAST v1.1][github-vast-release-old] release.\\n\\n[github-vast-release-new]: https://github.com/tenzir/vast/releases/tag/v1.1.1\\n[github-vast-release-old]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n- The disk monitor now correctly continues deleting until below the low water\\n  mark after a partition failed to delete.\\n- We fixed a rarely occurring race condition that caused query workers to become\\n  stuck after delivering all results until the corresponding client process\\n  terminated.\\n- Queries that timed out or were externally terminated while in the query\\n  backlog that had more unhandled candidate than taste partitions no longer\\n  permanently get stuck. This critical bug caused VAST to idle permanently on\\n  the export path once all workers were stuck.\\n\\nThanks to [@norg](https://github.com/norg) for reporting the issues."},{"id":"/vast-v1.1","metadata":{"permalink":"/blog/vast-v1.1","source":"@site/blog/vast-v1.1/index.md","title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","date":"2022-03-03T00:00:00.000Z","formattedDate":"March 3, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"compaction","permalink":"/blog/tags/compaction"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":5.975,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.1","description":"VAST v1.1 - Compaction & Query Language Frontends","authors":"dominiklohmann","date":"2022-03-03T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","compaction","query"]},"prevItem":{"title":"VAST v1.1.1","permalink":"/blog/vast-v1.1.1"},"nextItem":{"title":"VAST v1.0","permalink":"/blog/vast-v1.0"}},"content":"Dear community, we are excited to announce [VAST v1.1][github-vast-release],\\nwhich ships with exciting new features: *query language plugins* to exchange the\\nquery expression frontend, and *compaction* as a mechanism for expressing\\nfine-grained data retention policies and gradually aging out data instead of\\nsimply deleting it.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.1.0\\n\\n\x3c!--truncate--\x3e\\n\\n## Query Language Plugins\\n\\nVAST features a new query language plugin type\\nthat makes it possible to exchange the querying frontend, that is, replace the\\nlanguage in which the user writes queries. This makes it easier to integrate\\nVAST into specific domains without compromising the policy-neutral system core.\\n\\nThe first instance of the query language plugin is the [`sigma`\\nplugin](https://github.com/tenzir/vast/tree/master/plugins/sigma), which make it\\npossible to pass Sigma rules as\\ninput instead of a standard VAST query expression. Prior to this plugin, VAST\\nattempted to parse a query as Sigma rule first, and if that failed, tried to\\nparse it as a VAST expression. The behavior changed in that VAST now always\\ntries to interpret user input as VAST expression, and if that fails, goes\\nthrough all other loaded query language plugins.\\n\\nMoving forward, we will make it easier for integrators to BYO query language and\\nleverage VAST as an execution engine. We have already\\n[experimented](https://github.com/tenzir/vast/pull/2075) with\\n[Substrait](https://substrait.io), a cross-language protobuf spec for query\\nplans. The vision is that users can easily connect *any* query language that\\ncompiles into Substrait, and VAST takes the query plan as binary substrait blob.\\nSubstrait is still a very young project, but if the Arrow integration starts to\\nmature, it has the potential to enable very powerful types of queries without\\nmuch heavy lifting on our end. We already use the Arrow Compute API to implement\\ngeneric grouping and aggregation during compaction, which allows us to avoid\\nhand-roll and optimize compute kernels for standard functions.\\n\\n## Compaction Plugin\\n\\nCompaction is a feature to perform fine-grained transformation of historical\\ndata to manage a fixed storage budget. This gives operators full control over\\nshrinking data gradually\u2014both from a temporal and spatial angle:\\n\\n**Spatial**: Traditionally, reaching a storage budget triggers deletion of the\\noldest (or least-recently-used) data. This is a binary decision to throw away a\\nsubset of events. It does not differentiate the utility of data within an event.\\nWhat if you could only throw away the irrelevant parts and keep the information\\nthat might still be useful for longitudinal investigations? What if you could\\naggregate multiple events into a single one that captures valuable information?\\nImagine, for example, halving the space utilization of events with network flow\\ninformation and keeping them 6 months longer; or imagine you could roll up a set\\nof flows into a traffic matrix that only captures who communicated with whom in\\na given timeframe.\\n\\nBy incrementally elevating data into more space-efficient representations,\\ncompaction gives you a much more powerful mechanism to achieve long retention\\nperiods while working with high-volume telemetry.\\n\\n**Temporal**: data residency regulations often come with compliance policies\\nwith maximum retention periods, e.g., data containing personal data. For\\nexample, a policy may dictate a maximum retention of 1 week for events\\ncontaining URIs and 3 months for events containing IP addresses related to\\nnetwork connections. However, these retention windows could be broadened when\\npseudonomyzing or anonymizing the relevant fields.\\n\\nCompaction has a policy-based approach to specify these temporal constraints in\\na clear, declarative fashion.\\n\\nCompaction supersedes both the disk monitor and aging, being able to cover the\\nentire functionality of their behaviors in a more configurable way. The disk\\nmonitor remains unchanged and the experimental aging feature is deprecated (see\\nbelow).\\n\\n## Updates to Transform Steps\\n\\n### Aggregate Step\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/blog/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Additionally, we renamed the `aggregate` operator to\\n`summarize`. Please keep this in mind when reading the example\\nbelow and consult the documentation for the up-to-date syntax.\\n:::\\n\\nThe new `aggregate` transform step plugin allows for reducing data with an\\naggregation operation over a group of columns.\\n\\nAggregation is a two-step process of first bucketing data in groups of values,\\nand then executing an aggregation function that computes a single value over the\\nbucket. The functionality is in line with what standard execution engines offer\\nvia \\"group-by\\" and \\"aggregate\\".\\n\\nBased on how the transformation is invoked in VAST, the boundary for determining\\nwhat goes into a grouping can be a table slice (e.g., during import/export) or\\nan entire partition (during compaction).\\n\\nHow this works is best shown on example data. Consider the following events\\nrepresenting flow data that contain a source IP address, a start and end\\ntimestamp, the number of bytes per flow, a boolean flag whether there is an\\nassociated alert, and a unique identifier.\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 87122, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": false, \\"unique_id\\": 1}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2022-02-22T10:36:43\\", \\"end\\": \\"2022-02-22T10:36:48\\", \\"alerted\\": false, \\"unique_id\\": 2}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 640, \\"start\\": \\"2022-02-22T10:36:46\\", \\"end\\": \\"2022-02-22T10:36:47\\", \\"alerted\\": true, \\"unique_id\\": 3}\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 2162, \\"start\\": \\"2022-02-22T10:36:49\\", \\"end\\": \\"2022-02-22T10:36:51\\", \\"alerted\\": false, \\"unique_id\\": 4}\\n```\\n\\nWe can now configure a transformation that groups the events by their source IP\\naddress, takes the sum of the number of bytes, the minimum of the start\\ntimestamp, the maximum of the end timestamp, and the disjunction of the alerted\\nflag. Since the unique identifier cannot be aggregated in a meaningful manner,\\nit  is discarded.\\n\\n```yaml\\nvast:\\n  transforms:\\n    example-aggregation:\\n      - aggregate:\\n          group-by:\\n            - source_ip\\n          sum:\\n            - num_bytes\\n          min:\\n            - start\\n          max:\\n            - end\\n          any:\\n            - alerted\\n```\\n\\nAfter applying the transform, the resulting events will look like this:\\n\\n```json\\n{\\"source_ip\\": \\"10.0.0.1\\", \\"num_bytes\\": 89924, \\"start\\": \\"2022-02-22T10:36:40\\", \\"end\\": \\"2022-02-02T10:36:51\\", \\"alerted\\": true}\\n{\\"source_ip\\": \\"10.0.0.2\\", \\"num_bytes\\": 62335, \\"start\\": \\"2020-11-06T10:36:43\\", \\"end\\": \\"2020-02-22T10:36:48\\", \\"alerted\\": false}\\n```\\n\\nUnlike the built-in transform steps, `aggregate` is a separate open-source\\nplugin that needs to be manually enabled in your `vast.yaml` configuration to be\\nusable:\\n\\n```yaml\\nvast:\\n  plugins:\\n    - aggregate\\n```\\n\\n### Rename Step\\n\\nThe new `rename` transform step is a built-in that allows for changing the name\\nof the schema of data. This is particularly useful when a transformation changes\\nthe shape of the data. E.g., an aggregated `suricata.flow` should likely be\\nrenamed because it is of a different layout.\\n\\nThis is how you configure the transform step:\\n\\n```yaml\\nrename:\\n  layout-names:\\n    - from: suricata.flow\\n      to: suricata.aggregated_flow\\n```\\n\\n### Project and Select Steps\\n\\nThe built-in `project` and `select` transform steps now drop table slices where\\nno columns and rows match the configuration respectively instead of leaving the\\ndata untouched.\\n\\n## Deprecations\\n\\nThe `msgpack` encoding no longer exists. As we integrate deeper with Apache\\nArrow, the `arrow` encoding is now the only option. Configuration options for\\n`msgpack` will be removed in an upcoming major release. On startup, VAST now\\nwarns if any of the deprecated options are in use.\\n\\nVAST\u2019s *aging* feature never made it out of the experimental stage: it only\\nerased data without updating the index correctly, leading to unnecessary lookups\\ndue to overly large candidate sets and miscounts in the statistics. Because\\ntime-based compaction is a superset of the aging functionality (that also\\nupdates the index correctly), we will remove aging in a future release. VAST now\\nwarns on startup if it\u2019s configured to run aging."},{"id":"/vast-v1.0","metadata":{"permalink":"/blog/vast-v1.0","source":"@site/blog/vast-v1.0/index.md","title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","date":"2022-01-27T00:00:00.000Z","formattedDate":"January 27, 2022","tags":[{"label":"release","permalink":"/blog/tags/release"},{"label":"transforms","permalink":"/blog/tags/transforms"},{"label":"query","permalink":"/blog/tags/query"}],"readingTime":3.175,"hasTruncateMarker":true,"authors":[{"name":"Dominik Lohmann","title":"Engineering Manager","url":"https://github.com/dominiklohmann","email":"dominik@tenzir.com","imageURL":"https://github.com/dominiklohmann.png","key":"dominiklohmann"}],"frontMatter":{"title":"VAST v1.0","description":"VAST v1.0 \u2013 New Year, New Versioning Scheme","authors":"dominiklohmann","date":"2022-01-27T00:00:00.000Z","last_updated":"2022-07-15T00:00:00.000Z","tags":["release","transforms","query"]},"prevItem":{"title":"VAST v1.1","permalink":"/blog/vast-v1.1"}},"content":"We are happy to announce [VAST v1.0][github-vast-release]!\\n\\nThis release brings a new approach to software versioning for Tenzir. We laid\\nout the semantics in detail in a new [VERSIONING][github-versioning-md]\\ndocument.\\n\\n[github-vast-release]: https://github.com/tenzir/vast/releases/tag/v1.0.0\\n[github-versioning-md]: https://github.com/tenzir/vast/blob/v1.0.0/VERSIONING.md\\n\\n\x3c!--truncate--\x3e\\n\\n## Query events based on their import time\\n\\nThe new `#import_time` extractor allows for exporting\\nevents based on the time they arrived at VAST. Most of the time, this timestamp\\nis not far away from the timestamp of when the event occurred, but in certain\\ncases the two may deviate substantially, e.g., when ingesting historical events\\nfrom several years ago.\\n\\nFor example, to export all Suricata alerts that arrived at VAST on New Years Eve\\nas JSON, run this command:\\n\\n```bash\\nvast export json \'#type == \\"suricata.alert\\" && #import_time >= 2021-12-31 && #import_time < 2022-01-01\'\\n```\\n\\nThis differs from the `:timestamp` type extractor that\\nqueries all events that contain a type `timestamp`, which is an alias for the\\n`time` type.  By convention, the `timestamp` type represents the event time\\nembedded in the data itself. However, the import time  is not part of the event\\ndata itself, but rather part of metadata of every batch of events that VAST\\ncreates.\\n\\n## Omit `null` fields in the JSON export\\n\\nVAST renders all fields defined in the schema when exporting events as JSON. A\\ncommon option for many tools that handle JSON is to skip rendering `null`\\nfields, and the new `--omit-nulls` option to the JSON export does exactly that.\\n\\nTo use it on a case-by-case basis, add this flag to any JSON export.\\n\\n```bash\\nvast export json --omit-nulls \'<query>\'\\n\\n# This also works when attaching to a matcher.\\nvast matcher attach json --omit-nulls <matcher>\\n```\\n\\nTo always enable it, add this to your `vast.yaml` configuration file:\\n\\n```yaml\\nvast:\\n  import:\\n    omit-nulls: true\\n```\\n\\n## Selection and Projection Transform Steps\\n\\n:::info Transforms \u2192 Pipelines\\nIn [VAST v2.2](/blog/vast-v2.2), we renamed *transforms* to *pipelines*, and\\n*transform steps* to *pipeline operators*. This caused several configuration key\\nchanges. Please keep this in mind when reading the example below and consult the\\ndocumentation for the up-to-date syntax.\\n:::\\n\\nReshaping data during import and export is a common use case that VAST now\\nsupports. The two new built-in transform steps allow for filtering columns and\\nrows. Filtering columns (*projection*) takes a list of column names as input,\\nand filtering rows (*selection*)  works with an arbitrary query expression.\\n\\nHere\u2019s a usage example that sanitizes data leaving VAST during a query. If any\\nstring field in an event contains the value `tenzir` or `secret-username`, VAST\\nwill not include the event in the result set. The example below applies this\\nsanitization only to the events  `suricata.dns` and `suricata.http`, as defined\\nin the section `transform-triggers`.\\n\\n```yaml\\nvast:\\n  # Specify and name our transforms, each of which are a list of configured\\n  # transform steps. Transform steps are plugins, enabling users to write more\\n  # complex transformations in native code using C++ and Apache Arrow.\\n  transforms:\\n     # Prevent events with certain strings to be exported, e.g., \\"tenzir\\" or\\n     # \\"secret-username\\".\\n     remove-events-with-secrets:\\n       - select:\\n           expression: \':string !in [\\"tenzir\\", \\"secret-username\\"]\'\\n\\n  # Specify whether to trigger each transform at server- or client-side, on\\n  # import or export, and restrict them to a list of event types.\\n  transform-triggers:\\n    export:\\n      # Apply the remove-events-with-secrets transformation server-side on\\n      # export to the suricata.dns and suricata.http event types.\\n      - transform: remove-events-with-secrets\\n        location: server\\n        events:\\n          - suricata.dns\\n          - suricata.http\\n```\\n\\n## Threat Bus 2022.01.27\\n\\nThanks to a contribution from Sascha Steinbiss\\n([@satta](https://github.com/satta)), Threat Bus only reports failure when\\ntransforming a sighting context if the return code of the transforming program\\nindicates failure.\\n\\nA small peek behind the curtain: We\u2019re building the next generation of Threat\\nBus as part of VAST. We will continue to develop and maintain Threat Bus and its\\napps for the time being.\\n\\nThreat Bus 2022.01.27 is available [\ud83d\udc49\\nhere](https://github.com/tenzir/threatbus/releases/tag/2022.01.27)."}]}')}}]);